{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-01-Working-with-the-Data.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_01_Working_with_the_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdBTVdyx6XLO",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This beginner's project is based on many ideas found in *Machine Learning for Absolute Beginners: A Plain English Introduction* by Oliver Theobald. It has been heavily augmented with references found around the web (linked when appropriate).\n",
        "\n",
        "---\n",
        "\n",
        "![Machine Learning for Absolute Beginners](https://images-na.ssl-images-amazon.com/images/I/413%2BI3pEaXL.jpg)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This book is an excellent introduction for those just beginning their machine learning journey. Though this class introduces a number of principles found in that text, I highly recommend buying the book yourself and proceed through it after your experience here. He walks the reader through a number of important concepts that are too extensive for this course, but his writing is clear and he does a spectacular job of explaining difficult topics to ensure understanding. Additionally, he provides a number examples which the reader can tackle after getting down the basics. \n",
        "\n",
        "The book can be found here if you are interested: [Machine Learning for Absolute Beginners](https://www.amazon.com/Machine-Learning-Absolute-Beginners-Introduction/dp/1549617214/ref=sr_1_1?crid=1AF1PFSE85G4F&keywords=machine+learning+for+absolute+beginners+a+plain+english+introduction&qid=1563399014&s=gateway&sprefix=machine+learning+for+absolute+beginners+%2Caps%2C326&sr=8-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSOFzt3G6vkv",
        "colab_type": "text"
      },
      "source": [
        "# Download and Work with the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HupfNor71lI",
        "colab_type": "text"
      },
      "source": [
        "## A. Get the dataset\n",
        "1. Press \"Connect\" in the upper right corner of this page (on Colab).\n",
        "\n",
        "2. The dataset is available through Kaggle.com which requires a login to access. I have also made the dataset available on my Github page, but you should be able to access and import it using the commands in our first code cell. \n",
        "\n",
        "\n",
        "- If the following command fails, please go to [Github page](https://github.com/ccwilliamsut/machine_learning/tree/master/absolute_beginners/data_files/modified), download \"CaliforniaHousingDataModified.csv\" to your downloads folder and uncomment the alternative commands.\n",
        "\n",
        "- If the above does not work and you are still having trouble, consult this [link](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92) to learn how to work with data files in Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br8B_N6pRQnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "# --------------------------------------------------- ACQUIRE DATA ---------------------------------------------------\n",
        "# Set the URL for the data file\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "# Import the datafile from the provided url and run the cell\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# ------- ALTERNATIVE COMMANDS (if above commands do not work-------\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXeyg7UMXReV",
        "colab_type": "text"
      },
      "source": [
        "## B. Explore the Data\n",
        "As you begin to look over your data, it is **important** to consider the following **key concepts**:\n",
        "1. Does my data have **labels**? (dependent variables; determines whether we will use supervised or unsupervised algorithms)\n",
        "  - This will determine the type of model that you are able to create (regression or classification)\n",
        "2. Are there major **issues** with the data\n",
        "  - This includes things like **null values, few samples, misspellings, derived data, unknown scales or values, etc.**\n",
        "3. What is my **goal**? (For this course, our goal is to create a model that can predict housing prices based upon the given set of features.)\n",
        "    - Will I be able to accomplish that goal with this dataset?\n",
        "    - If not, can I employ feature engineering to create the data I need?\n",
        "    - If so, which features will contribute to that goal and which are unnecessary?\n",
        "4. Can I see any **relationships** in the data that might serve as a good foundation for my model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb-nqdN4X4o0",
        "colab_type": "text"
      },
      "source": [
        "### i. Basic Code Functions for Analyzing Datasets\n",
        "If we want to see the names of our **features** (i.e. column headings), we simply issue the first of the following commands. We can also neatly list them out with the second function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KUCFFiZZH_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the first 5 rows of the file\n",
        "print('The \"head\" of our data:')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5IrKqXA5y3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a random sample of the data\n",
        "print('\\n\\nRandom sample of the data:')\n",
        "df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5dpBjJNHGGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a statistical description of the data\n",
        "print('\\n\\nStatistical description of the data:')\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9XX2OfADona",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List the column headers to see what we are working with\n",
        "print(\"\\n\\nHere are all of our columns:\\n\")\n",
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCq3jZPJDssa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List all the columns in the dataset in an ordered way\n",
        "print('\\n\\nAll of our columns in a list form (much easier to read):\\n')\n",
        "list(df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNIjsKX_Dy6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the \"shape\" of the dataset\n",
        "print('\\n\\nLook at the shape a dataset (rows / columns):\\n')\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGeJj3qkEYfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at some information about the dataset (feature name, total non-null record count,\n",
        "#   wheter the feature is empty, datatype for each feature)\n",
        "print('\\n\\nLook at the info about a dataset (feature name, record count, records exist, datatype):\\n')\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqwz_zJlbGzG",
        "colab_type": "text"
      },
      "source": [
        "### ii. Look for potential problems\n",
        "\n",
        "**Finding problems**\n",
        "- Look at the above data closely. **Talk** about it with your **group/partner** or think about it on your own.\n",
        "- Can you spot the potential issues that we might have with this dataset?\n",
        "\n",
        ">**NOTE:** There are **at least 3 problems** that we can identify using the above information. \n",
        "- Can you find them all? \n",
        "- Can you see any more?\n",
        "- Do you think that the above data will help our model to be more accurate?\n",
        "- Should we keep all, some or just a few of these features?\n",
        "\n",
        "We will not any changes to our dataset just yet, but it is worth noting the potential problems that exist.\n",
        "\n",
        "Let's next explore the data through **visualization** to get a better idea of any additional problems that might be present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJwZFAZhFV8X",
        "colab_type": "text"
      },
      "source": [
        "## C. Visualization of Data\n",
        "Using a variety of graphs is a great way to explore data. \n",
        "\n",
        "> Websites referenced:\n",
        "- [Overview of graph types and purposes](https://towardsdatascience.com/a-step-by-step-guide-for-creating-advanced-python-data-visualizations-with-seaborn-matplotlib-1579d6a1a7d0)\n",
        "- [Analyze the data through data visualization using Seaborn (Toward Data Science)](https://towardsdatascience.com/analyze-the-data-through-data-visualization-using-seaborn-255e1cd3948e)\n",
        "- [Tutorial on visualizing distributions](https://www.machinelearningplus.com/plots/matplotlib-histogram-python-examples/)\n",
        "- [Creating multi-dimensional subplots](https://matplotlib.org/3.1.1/gallery/subplots_axes_and_figures/subplots_demo.html)\n",
        "- [Great demo of the differences between histograms and KDE](https://mglerner.github.io/posts/histograms-and-kernel-density-estimation-kde-2.html)\n",
        "- [Explanation of KDE](http://www.mvstat.net/tduong/research/seminars/seminar-2001-05/)\n",
        "- [Analyze the data through data visualization using Seaborn (Toward Data Science)](https://towardsdatascience.com/analyze-the-data-through-data-visualization-using-seaborn-255e1cd3948e)\n",
        "- [Stackoverflow Help](https://stackoverflow.com/questions/25212986/how-to-set-some-xlim-and-ylim-in-seaborn-lmplot-facetgrid)\n",
        "- [Seaborn Documentation: Pairplots](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
        "- [Short Tutorial: Pairplots](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n",
        "- [Pairplot Code Examples](https://jovianlin.io/data-visualization-seaborn-part-2/)\n",
        "- [Stackoverflow: Limiting x- and y-axis values](https://stackoverflow.com/questions/54951362/seaborn-jointplot-with-defined-axes-limits)\n",
        "- [Seaborn Documentation: Heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
        "- [Excellent tuturial on modifying heatmaps](https://heartbeat.fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations-f1c49c816f07)\n",
        "- [Short Tutorial about correlation and heatmaps](https://jovianlin.io/data-visualization-seaborn-part-2/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzNiaD_YCxNw",
        "colab_type": "text"
      },
      "source": [
        "### i. Histograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mKHFErB6Wm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use histograms to explore features in the dataset\n",
        "\n",
        "# Create a \"figure\" that can hold multiple plots\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(nrows = 1,              # create a grid with a specified number of rows\n",
        "                                    ncols = 3,              # specify the number of columns\n",
        "                                    figsize = (15, 4),      # specify the size of each subplot\n",
        "                                    sharey = False          # specify if all plots will share the y-axis values\n",
        "                                    )\n",
        "\n",
        "# ------------------ Histograms ---------------------\n",
        "# Histogram 1 (in slot 'ax1', the first container)\n",
        "sns.distplot(df['housing_median_age'], \n",
        "             kde = False,\n",
        "             bins = 20, \n",
        "             color = 'magenta',\n",
        "             ax = ax0\n",
        "             )\n",
        "\n",
        "# Histogram 2 (in slot 'ax2', the second container)\n",
        "sns.distplot(df['t_rooms'], \n",
        "             kde = False,\n",
        "             bins = 30, \n",
        "             color = 'green', \n",
        "             ax = ax1\n",
        "             )\n",
        "\n",
        "# Histogram 3 (in slot 'ax3', the third container)\n",
        "sns.distplot(df['median_house_value'], \n",
        "             kde = False,\n",
        "             bins = 50, \n",
        "             color = 'blue',\n",
        "             ax = ax2\n",
        "             )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGqfEpbGBf5p",
        "colab_type": "text"
      },
      "source": [
        "### ii. **Scatterplots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNg9N88HMV8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use scatterplots to compare features in the dataset\n",
        "\n",
        "# Create a \"figure\" that can hold multiple plots (in this case, 1 row and 3 columns)\n",
        "fig, ((ax1, ax2, ax3)) = plt.subplots(nrows = 1,                      # create a grid with a specified number of rows\n",
        "                                      ncols = 3,                      # specify the number of columns\n",
        "                                      figsize = (20, 5),              # specify the size of each subplot\n",
        "                                      sharey = False,                 # specify if all plots will share the y-axis values\n",
        "                                      sharex = False                  # specify if all plots will share the x-axis values\n",
        "                                      )\n",
        "\n",
        "# Plot scatterplots on the top row and lineplots on the bottom row\n",
        "# Scatterplots 1 to 3\n",
        "ax1 = sns.regplot(x = df['median_income'],\n",
        "                  y = df['median_house_value'],\n",
        "                  data = df,\n",
        "                  ax = ax1, \n",
        "                  dropna = True,\n",
        "                  order = 1,                                          # Note here that we are using polynomial regression\n",
        "                  line_kws = {'color': 'darkorange'},\n",
        "                  fit_reg = True\n",
        "                  )\n",
        "\n",
        "ax2 = sns.regplot(x = df['total_bedrooms'],\n",
        "                  y = df['median_house_value'],\n",
        "                  data = df,\n",
        "                  ax = ax2, \n",
        "                  dropna = True,\n",
        "                  order = 1,\n",
        "                  line_kws = {'color': 'purple'},\n",
        "                  fit_reg = True\n",
        "                  )\n",
        "\n",
        "ax3 = sns.regplot(x = df['population'],\n",
        "                  y = df['median_house_value'],\n",
        "                  data = df,\n",
        "                  ax = ax3, \n",
        "                  dropna = True,\n",
        "                  order = 1,\n",
        "                  line_kws = {'color': 'blue'},\n",
        "                  fit_reg = True\n",
        "                  )\n",
        "\n",
        "\n",
        "# Change the range of a few axes variables to make the graphs more useful\n",
        "#ax1.set(ylim = (0, 600000))\n",
        "#ax3.set(ylim = (0, 600000))\n",
        "#ax3.set(xlim = (0, 15000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pDV2UHWbFl",
        "colab_type": "text"
      },
      "source": [
        "### iii. Countplots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFZvVOrXWnOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a countplot with a categorical variable\n",
        "sns.countplot(x = df['ocean_proximity'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSFdnoVJE26l",
        "colab_type": "text"
      },
      "source": [
        "### iv. Categorical Histograms (using Facetgrid)\n",
        "**Facetgrid plots** allow one to break out data by category while analyzing either univariate or bivariate data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5FqNq4XkYD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze graphs side-by-side (using seaborn Facetgrid)\n",
        "ghist = sns.FacetGrid(df, \n",
        "                      col='ocean_proximity', \n",
        "                      hue = 'ocean_proximity', \n",
        "                      dropna = True, \n",
        "                      legend_out=True, \n",
        "                      despine=True\n",
        "                      )\n",
        "ghist.map(plt.hist,\n",
        "          'median_house_value',\n",
        "          alpha=1,\n",
        "          bins = 20\n",
        "          )\n",
        "\n",
        "# Create another facetgrid with scatterplots\n",
        "gscat = sns.FacetGrid(df,\n",
        "                      col='ocean_proximity',\n",
        "                      hue = 'ocean_proximity',\n",
        "                      dropna = True,\n",
        "                      despine=True\n",
        "                      )\n",
        "gscat.map(plt.scatter,\n",
        "          'median_income',\n",
        "          'median_house_value',\n",
        "          alpha=.3\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5FdXd6poWg9",
        "colab_type": "text"
      },
      "source": [
        "### v. Pairplots\n",
        "With this simple function, we can quickly analyze most of our features against one another to:\n",
        "- Seek out possible relationships\n",
        "- Identify possible issues with data\n",
        "```\n",
        "```\n",
        "\n",
        "***Websites referenced:***\n",
        "- [Seaborn Documentation: Pairplots](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
        "- [Short Tutorial](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n",
        "- [Pairplot Code Examples](https://jovianlin.io/data-visualization-seaborn-part-2/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0iJFEpmNrDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaboarn.pairplot() to quickly look at possible relationships and visually analyze the data\n",
        "\n",
        "# Use this to analyze each feature against the others while also segmenting out the proximity to the ocean\n",
        "#sns.pairplot(data = df,\n",
        "#              hue='ocean_proximity', \n",
        "#              dropna=True, \n",
        "#             )\n",
        "\n",
        "# Use this function to analyze each feature against the others with an added regression line to help identify relationships\n",
        "sns.pairplot(data = df, \n",
        "             dropna = True\n",
        "             )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlBQQmjtbPja",
        "colab_type": "text"
      },
      "source": [
        "### vi. Correlation Heatmaps\n",
        "We can also look at how well data is correlated via the **```seaboarn.heatmap()```** function. This will help to give us an idea of which features are well or poorly correlated to each other.\n",
        "\n",
        "> *Websites referenced:*\n",
        "- [Seaborn Documentation: Heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
        "- [Excellent tuturial on modifying heatmaps](https://heartbeat.fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations-f1c49c816f07)\n",
        "- [Short Tutorial about correlation and heatmaps](https://jovianlin.io/data-visualization-seaborn-part-2/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIPm62PDDUPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- Analyze the data with a heatmap --\n",
        "# 1. Only keep the columns and data that we want to see correlated\n",
        "cols = df.drop(['lattitude', 'longitude', 'ocean_proximity'],\n",
        "               axis=1\n",
        "               )\n",
        "\n",
        "# 2. Fill in missing values with the median value of the feature\n",
        "#     - You can also use the mean (or even the mode if dealing with categorical data)\n",
        "cols.fillna(cols.median(),\n",
        "            inplace = True\n",
        "            )\n",
        "\n",
        "# 2. Calculate the correlations\n",
        "corr = cols.corr()\n",
        "\n",
        "# 3. Set the size you want\n",
        "plt.figure(figsize=(9 ,9))\n",
        "\n",
        "# 4. Display the heatmap\n",
        "sns.heatmap(corr,\n",
        "            annot=True,\n",
        "            vmin = -1,\n",
        "            vmax = 1,\n",
        "            center = 0\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uA-3Yt61eL7",
        "colab_type": "text"
      },
      "source": [
        "## D. Check for Missing Data\n",
        "One thing that we need to do before moving on is to see if there is any missing data. If a value is null when the data is imported to Pandas, then the value \"NaN\" is assigned. Some of our data might have null (```NaN```) values that will cause problems with the performance of our model. \n",
        "\n",
        "Missing data can cause a number of problems:\n",
        "- Further analysis with statistical measures and graphs can **distort our understanding**\n",
        "- Missing data can **distort our model** if those values affect predictive or clustering calculations\n",
        "- Some **functions can fail** if they encouter null values\n",
        "\n",
        "Let's quickly look over our dataset for any missing values.\n",
        "\n",
        ">Websites Referenced:\n",
        "- [Kaggle.com tutorial on the usage of 'msno'](https://www.kaggle.com/residentmario/using-missingno-to-diagnose-data-sparsity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viAG3Pwj1NON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the MissingNo library to visualize where any missing data is\n",
        "msno.matrix(df, figsize = (10, 5), sparkline = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SedV62RY4msA",
        "colab_type": "text"
      },
      "source": [
        "It appears that our **```total_bedrooms```** feature has some missing data. We can investigate further to count the number of records with missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLCtiWsNAt99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a count of \"NaN\" or missing values by feature\n",
        "print('Count of missing values by feature:\\n')\n",
        "display(df.isnull().sum(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFrBAnnWfQFk",
        "colab_type": "text"
      },
      "source": [
        "## E. Summarizing Problems\n",
        "The primary problems at this point are:\n",
        "- The spelling of **```lattitude```**\n",
        "- The meaning of **```t_rooms```**\n",
        "- The scale used for **```median_income```**\n",
        "- The scale and distribution used for **```proximity_to_store```**\n",
        "- **Missing data** in the **```total_bedrooms```** feature\n",
        "- **Outlier data** in a number of features\n",
        "- The value limit of $500,000 for **```median_housing_value```**\n",
        "\n",
        "\n",
        "#### Analyzing the problems\n",
        "- **Spelling** might not seem important at first (after all, it doesn't directly affect our model's performance), but the spelling is actually a fairly big issue. This is not because of any immediate coding problems, as we could easily write our code and accommodate the misspelling. But when our model moves downstream in other workflows, we will have to make special note of the misspelling to others, lest we make their debugging more problematic and time-comsuming. Worse yet, it might not get caught at all and be shown to leadership as is, possibly leaving a bad impression. **If we think that we will possibly use this feature, then it is better to fix it now than wait for later**. \n",
        "\n",
        "- **```t_rooms```** presents another issue, as we cannot be sure exactly what it means at this point. We can guess based on the other data that it likely means \"total rooms\", but we need to be sure. To get the answer, we (fictionally, in this case) contact the team that collected the data and confirm that **```t_rooms```** should be **```total_rooms```**. Again, we want to think about if we actually need this feature before making the change. It seems likely that the total rooms in a house will have some kind of correlation to the overall price, so let's keep this and change it.\n",
        "\n",
        "- **```median_income```** and **```proximity_to_store```** present different problems. We do not know the scale being used in either case, and they appear to be different from one another. Additionally, the distribution for proximity to store appears to be uniform, meaning that it might not be adding anything to our model.\n",
        "\n",
        "- **Missing Data** can cause a number of problems in machine learning, but we will learn how to fix it in the following section (**Scrubbing Data**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YVOZGp6Lqrt",
        "colab_type": "text"
      },
      "source": [
        "# Takeaways\n",
        "In this lesson, we have learned how to:\n",
        "- Setup our environment\n",
        "- Import data\n",
        "- Explore data for problems and viability\n",
        "- Visualize data to identify additional issues and relationships\n",
        "- Check for missing data\n",
        "\n",
        "As you explore your dataset, it is important to keep specific things in mind:\n",
        "- \n",
        "\n",
        "\n",
        "### Moving Forward:\n",
        "We will work through all of the problems listed here (and more!) in our next lesson: **Scrubbing Data**. "
      ]
    }
  ]
}