{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLAB-04-Decision-Tree-Regression.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yAkWiEDA5QUv","colab_type":"text"},"source":["# Build a Decision Tree Model (Regression)\n","\n","> Websites Referenced:\n","- [Scikit-Learn Documentation: Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n","- [Excellent explanation of Decision Tree Regression with sample code](https://gdcoder.com/decision-tree-regressor-explained-in-depth/)"]},{"cell_type":"markdown","metadata":{"id":"B9VVGiXJ7TWW","colab_type":"text"},"source":["## A. Setup Environment"]},{"cell_type":"code","metadata":{"id":"nEy-VYZn7lpr","colab_type":"code","colab":{}},"source":["# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# Import libraries\n","import pandas as pd\n","import seaborn as sns; sns.set()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","import missingno as msno\n","from IPython.display import display\n","from sklearn.model_selection import train_test_split \n","from sklearn import ensemble\n","from scipy import stats\n","from sklearn import linear_model\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import learning_curve,GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn import preprocessing\n","from collections import Counter\n","from sklearn.externals import joblib\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n","\n","df = pd.read_csv(url)\n","#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n","\n","\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n","df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n","\n","\n","# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n","df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n","df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n","df.drop(['ocean_proximity'], axis = 1, inplace = True)\n","df = pd.concat([df, df_dummies], axis=1)\n","\n","\n","# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n","df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n","\n","\n","# ------------------------------------- FIX MISSING DATA -------------------------------------\n","tb_med = df['total_bedrooms'].median(axis=0)\n","df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n","df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n","df.name = 'df'\n","\n","# ------------------------------------- Z-SCORE -------------------------------------\n","z = np.abs(stats.zscore(df))\n","dfz = df[(z < 3).all(axis = 1)]\n","dfz.name = 'dfz'\n","\n","# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n","q1 = df.quantile(0.25)\n","q3 = df.quantile(0.75)\n","iqr = q3 - q1\n","lower = q1 - (1.5 * iqr)\n","upper = q3 + (1.5 * iqr)\n","dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n","dfi.name = 'dfi'\n","#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n","\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","def make_heatmap(df = 'df'):\n","  corr = df.corr()\n","  plt.figure(figsize=(15 ,9))\n","  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n","  plt.show()\n","  plt.close()\n","\n","\n","def make_heading(heading = 'heading'):\n","  print('{0}:\\n'.format(heading), '-' * 30)\n","\n","\n","def show_coef(df_in, model):\n","  coef_df = pd.DataFrame(model.coef_, df_in.columns, columns=['Coefficient'])\n","  make_heading('\\n\\nCoefficients')\n","  display(coef_df)\n","  make_heading('\\n\\nIntercept')\n","  display(model.intercept_)\n","\n","\n","def drop_features(df_in):\n","  df_in = df_in.drop([#'median_house_value',\n","                      'longitude',\n","                      'latitude',\n","                      #'housing_median_age',\n","                      'total_rooms',\n","                      'total_bedrooms',\n","                      #'median_income',\n","                      'INLAND',\n","                      #'NEAR BAY',\n","                      #'NEAR OCEAN'\n","                      ],\n","                      axis = 1\n","                      )\n","  return features_dfx\n","\n","\n","def plot_test_predictions(y_test, y_pred):\n","  make_heading('Prediction Performance')  # Make a heading to separate output\n","  fig, ax = plt.subplots()\n","  ax = plt.subplot()\n","  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n","  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n","  ax.set_xlabel('Actual')\n","  ax.set_ylabel('Predicted')\n","  ax.set_title(\"Ground Truth vs Predicted\")\n","  plt.show()\n","  plt.close()\n","\n","\n","def plot_feature_importance(feature_importance):\n","  # Make importances relative to max importance\n","  feature_importance = 100.0 * (feature_importance / feature_importance.max())\n","  sorted_idx = np.argsort(feature_importance)\n","  pos = np.arange(sorted_idx.shape[0]) + .5\n","  make_heading('Feature Importance')  # Make a heading to separate output\n","  plt.subplot(1, 2, 2)\n","  plt.barh(pos, feature_importance[sorted_idx], align='center')\n","  plt.yticks(pos, features_dfx)\n","  plt.xlabel('Relative Importance')\n","  plt.title('Variable Importance')\n","  plt.show()\n","  plt.close()\n","  \n","\n","def show_metrics(X_train, X_test, y_train, y_test, y_pred):\n","  # Display the shape of each array:\n","  #make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n","  #print('X_train shape: ', X_train.shape)\n","  #print('X_test shape:  ', X_test.shape)\n","  #print('y_train shape: ', y_train.shape)\n","  #print('y_test shape:  ', y_test.shape)\n","  #print('y_pred shape:  ', y_pred.shape)\n","\n","  # Display training / testing set metrics\n","  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n","  print(\"Training Accuracy (score)                 (X_train, y_train):  {:.2f}\".format(model_train_score))\n","  print(\"Test Accuracy (score)                     (X_test, y_test):    {:.2f}\".format(model_test_score))\n","  print(\"Predictive Accuracy (R^2 score)           (y_test, y_pred):    {:.2f}\".format(predictive_accuracy))\n","  print(\"Explained Variance (1 is best) (loss)     (y_test, y_pred):    {:.2f}\".format(ev))\n","  print(\"Mean Absolute Error on Test Set (loss)    (y_test, y_pred):    {:.2f}\".format(mean_ae))\n","  print(\"Median Absolute Error on Test Set (loss)  (y_test, y_pred):    {:.2f}\".format(median_ae))\n","  print(\"RMSE on Test Set (loss)                   (y_test, y_pred):    {:.2f}\".format(rmse))\n","  print(\"Model Depth                            (model.get_depth()):    {:d}\".format(model_depth))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6NoTOrvD-rq","colab_type":"text"},"source":["## B. Choose a dataset to use\n","- Three datasets have been created:\n","  1. **df** -- the **original dataset** ('NaN' values have been replaced with median, so there are no null values)\n","  2. **dfz** -- a dataset that has used the **z-score method** for removing outlier data\n","  3. **dfi** -- a dataset that has used the **IQR method** for removing outlier data\n","\n","```\n","\n","```\n","\n","**NOTE:** When choosing your dataset, you will need to change two commands to reflect the dataset that you want to use. In the following example, the \n","\n","**Original code:** --------------------------------------->  **New code** (For example, to change **from** 'dfz' **to** 'dfi' dataset):\n","```\n","dfx = dfz.copy()      -->   dfx = dfi.copy()\n","dfx.name = dfz.name   -->   dfx.name = dfi.name\n","```"]},{"cell_type":"code","metadata":{"id":"1mCB2AeT7cPc","colab_type":"code","colab":{}},"source":["# *************************** DETERMINE DATA FOR USE ***************************\n","# Choose which dataframe you would like to use (dfi: IQR, dfz: z-score, df: original (with replaced NaN values))\n","dfx = dfi.copy()\n","dfx.name = dfi.name\n","\n","# Show a heatmap for the given dataframe you want to use\n","make_heatmap(dfx)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77jpQAgEHoDf","colab_type":"text"},"source":["## C. Step by Step: Building a Decision Tree Model\n","\n","- In this section, we walk through building a **Decision Tree Regression model** using the dataframe chosen in the previous step.\n","\n","### The basic steps are:\n","  1. Define the **features** and the **target variable**.\n","  2. **Split** the dataset into **training** and **testing** partitions\n","  3. **Define the model:**\n","    - **Select Hyperparameters**\n","    - **Train the model** - run the model against our training set\n","    - Use the trained model to **predict against the test set**\n","  4. **Capture metrics** for the model (done during training, testing and prediction)\n","    - Training accuracy\n","    - Testing accuracy\n","    - Prediction error\n","    - Error values (Mean Absolute Error, Median Absolute Error, RMSE, etc.)\n","  5. **Analyze metrics**\n","    - Look at the varirous accuracy and error scores to **determine how well the model fits** (overfit, underfit, good, bad, etc.)\n","  6. **Plot graphs** to visualize the various accuracy / error metrics\n","  7. **Predict new values** to see if the model performs as expected (given the accuracy and coefficient values)\n","\n","---\n","### Setting Hyperparameters\n","Please see this link for more information: [Scikit-Learn Documentation: Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n","- **```criterion```** : string, optional (default=”mse”)\n","- The function to measure the quality of a split. Supported criteria are:\n","  - ***mse*** for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node\n","  - ***friedman_mse***, which uses mean squared error with Friedman’s improvement score for potential splits\n","  - ***mae*** for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.\n","- **```min_samples_leaf```** : int, float, optional (default=1)\n","  - It is the minimum number of samples for a terminal node.\n","  - In other words, a final node must have this many samples in order to be established. Any less, and it will not be created.\n","\n","- **```min_samples_split```** : int, float, optional (default=2)\n","  - It is the minimum samples for a node split that we discuss above.\n","  - In other words, a node cannot be split unless it has at least this many samples. Any less, and it will become a terminal node.\n","\n","- **```max_features```** : int, float, string or None, optional (default=”auto”)\n","  - The number of features to consider when looking for the best split\n","  - In other words, this determines the number (precisely defined or given as a percentage) of features that the model will contemplate when deriving the best means by which to split a node.\n","\n","- **```max_depth```** : integer or None, optional (default=None)\n","  - The maximum depth of the tree. \n","  - In other words, what is the maximum \"depth\" that the tree should grow?"]},{"cell_type":"code","metadata":{"id":"fw2dwazM5ey2","colab_type":"code","colab":{}},"source":["# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> STEP BY STEP DECISION TREE CONSTRUCTION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","\n","# ---------------------------- Identify Target Variable ----------------------------\n","# Create a copy of the dataframe with only the desired features (dropping the target feature)\n","features_dfx = dfx.drop(['median_house_value'], axis = 1)\n","\n","# Determine additional features to keep/drop (# means that we want to keep that variable)\n","features_dfx = features_dfx.drop([#'longitude',\n","                                  #'latitude',\n","                                  #'housing_median_age',\n","                                  #'total_rooms',\n","                                  #'total_bedrooms',\n","                                  #'median_income',\n","                                  #'INLAND',\n","                                  'NEAR BAY',\n","                                  'NEAR OCEAN'\n","                                  ],\n","                                 axis = 1\n","                                 )\n","\n","\n","errvals = np.array([])              # Create an array to hold our error values during training (used for plotting results)\n","rs = 20                             # Set our \"Random State\" variable (ensuring that we always start at the same point in our dataset)\n","\n","\n","# ---------------------------- SPLIT THE DATASET ----------------------------\n","# We extract the values of our two datasets into \"X\" and \"y\" variables for use in later functions\n","# The X variables are the \"explanatory\" numbers (or images). They are the numbers that we use to try and predict\n","#   what the \"y\" variable is.\n","\n","# \"X\" are the independent variables, the \"featues\" that we will use to try and predict \"y\"\n","X = features_dfx.values\n","\n","# \"y\" is the dependent variable (the \"label\"); the actual value that we are trying to learn to predict. \n","#   Each time we make a prediction, it is compared to the actual values, and the \"loss\" (the difference between\n","#   the prediction and real number) is added to previous \"loss\". \n","y = dfx['median_house_value'].values\n","\n","# Split the dataset into training and testing arrays\n","#   We take our X and y variables that we have just created and now split each one into a training set and testing\n","#   set: X_train, X_test, y_train, y_test)\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.3,    # reserve this percentage of our data for testing, use the rest for training\n","                                                    shuffle = True,\n","                                                    random_state = rs\n","                                                    )\n","\n","\n","# ---------------------------- CREATE THE MODEL  ----------------------------\n","# Define the desired algorithm and congigure the hyperparameters\n","\"\"\"model = DecisionTreeRegressor(criterion='mae',  # can also be 'mse' or 'friedman_mse'\n","                              max_depth = 500, \n","                              min_samples_split = 4,\n","                              min_samples_leaf = 4,\n","                              max_features = 0.6,\n","                              random_state = rs, \n","                              )\n","\"\"\"\n","\n","model = DecisionTreeRegressor(criterion='friedman_mse',  # can also be 'mse' or other values\n","                              max_depth = None, \n","                              min_samples_split = 64,\n","                              min_samples_leaf = 32,\n","                              max_features = 0.9,\n","                              random_state = rs, \n","                              )\n","\n","model_name = type(model).__name__   # Get the name of the model (for use in our display functions)\n","\n","\n","# ---------------------------- Fit / TRAIN THE MODEL ----------------------------\n","# Train the model using our training sets (X_train, y_train)\n","model.fit(X_train, y_train)\n","\n","# Here we want to input our X_test array to make predictions based upon our trained model\n","y_pred = model.predict(X_test)\n","\n","# ---------------------------- GATHER METRICS ----------------------------\n","model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n","model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n","mean_ae = metrics.mean_absolute_error(y_test, y_pred)  # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n","median_ae = metrics.median_absolute_error(y_test, y_pred)  # Median absolute error\n","rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n","ev = metrics.explained_variance_score(y_test, y_pred) # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n","predictive_accuracy = metrics.r2_score(y_test, y_pred)  # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n","model_depth = model.get_depth()\n","\n","\n","# ---------------------------- ANALYSIS / VISUALIZATION ----------------------------\n","show_metrics(X_train, X_test, y_train, y_test, y_pred)  # Dispay the scores and loss for the model\n","plot_test_predictions(y_test, y_pred)                   # Create a scatterplot of real values against predicted ones for the test set\n","feature_importance = model.feature_importances_         # Gather feature importance values\n","plot_feature_importance(feature_importance)             # Plot feature importance\n","joblib.dump(model, 'ca_housing_dt_model.pkl')           # Save the model so that we can use it later\n"],"execution_count":0,"outputs":[]}]}