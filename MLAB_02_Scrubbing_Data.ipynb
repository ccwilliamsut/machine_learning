{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-02-Scrubbing-Data.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_02_Scrubbing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCtOprwGT15o",
        "colab_type": "text"
      },
      "source": [
        "# Scrubbing Data\n",
        "\n",
        "Most of a data scientist's time is spent on working with data. This includes performing all of the tasks that we have done previously as well as the ones which we will now cover:\n",
        "- One-hot encoding\n",
        "- Deleting (and sometimes creating new) features\n",
        "- Fixing missing data\n",
        "- Dealing with outlier data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZyC_LVrT6bY",
        "colab_type": "text"
      },
      "source": [
        "## A. Setup Environment\n",
        "\n",
        "To begin with, we will re-import all of our libraries and perform all of the small changes that we have made thus far. The reason I re-introduce this code here is to make it easier and faster for you to make changes to your code and see the results. It keeps you from having to wait for all of the above code to execute before seeing any of your changes.\n",
        "\n",
        "> **NOTE:** Because we will be looking at a heatmap a couple of times, I have created a function call that will make it much easier. By calling this function and supplying it with a version of the dataframe (optional), we do not have to re-create the code each time. To call it from here on out, we simply issue the command **```make_heatmap(df)```**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlLjpjDBNE0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(dataframe):\n",
        "  corr = dataframe.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, \n",
        "              annot=True, \n",
        "              vmin = -1, \n",
        "              vmax = 1, \n",
        "              center = 0, \n",
        "              fmt = '.1g', \n",
        "              cmap = 'coolwarm'\n",
        "              )\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVaKt6wcUQ8N",
        "colab_type": "text"
      },
      "source": [
        "## B. One-Hot Encoding\n",
        "One of our features, **```ocean_proximity```**, is *categorical*. Because we believe this feature to be useful in helping us predict housing prices, we want to keep it, but **we must change the categorical values into numerical ones** for modeling purposes. \n",
        "\n",
        "To do this, we employ a procedure known as **one-hot encoding** which creates a new feature for each of the known values and then assigns a **1 or 0** depending upon whether or not each row contains that value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiTFl1cTWj74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a copy of our dataset to make things easier below\n",
        "dfx = df.copy()\n",
        "\n",
        "# Perform one-hot encoding on 'ocean_proximity'\n",
        "# Show the initial state \n",
        "print('The original state of our dataset:')\n",
        "display(dfx.sample(5))\n",
        "\n",
        "# Change our target feature into a categorical data type\n",
        "dfx['ocean_proximity'] = pd.Categorical(dfx['ocean_proximity'])\n",
        "\n",
        "# Create a temp dataframe to hold our new \"dummy values\"\n",
        "dfx_dummies = pd.get_dummies(dfx['ocean_proximity'], \n",
        "                            drop_first = False\n",
        "                            )\n",
        "\n",
        "# Drop the target feature from the original dataframe\n",
        "dfx.drop(['ocean_proximity'], \n",
        "        axis = 1, \n",
        "        inplace = True\n",
        "        )\n",
        "\n",
        "# Join our temp dataframe the our original to create a single dataframe (concatenate dataframes)\n",
        "dfx = pd.concat([dfx, dfx_dummies], \n",
        "               axis=1\n",
        "               )\n",
        "\n",
        "# Show the new state after one-hot encoding\n",
        "print('\\n\\nThe state of our dataset after one-hot encoding:')\n",
        "display(dfx.sample(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5an4RoLEN0P",
        "colab_type": "text"
      },
      "source": [
        "## C. Removing Unwanted Features\n",
        "When working with many datasets, there are **likely to be features that are not useful for the goal at hand**. In our case, we are trying to predict the median house value, so we must look at the features we have and decide if there are any that will not help us in that goal.\n",
        "\n",
        "Let's first take a look at the correlations between our target variable and the other variables to get an idea of which features will be useful in predicting a price. We will use our brand new **```make_heatmap```** function to build it.\n",
        "\n",
        ">**A note on multicollinearity**:\n",
        "> This term refers to the instance of two independent variables (features) being perfectly (or nearly so) correlated with one another. For example, as light increases, darkness decreases and so these two things are correlated closely.\n",
        "This can create problems in **linear regression** models (see this [link](https://www.quora.com/Is-multicollinearity-a-problem-with-gradient-boosted-trees) and this [link](https://medium.com/future-vision/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168) for more information). Therefore, we want to **keep only one feature if it exhibits multicollinearity with one or more others if we are using linear regression**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-jbjuqDvfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform all the previous actions\n",
        "dfx = df.copy()\n",
        "dfx['ocean_proximity'] = pd.Categorical(dfx['ocean_proximity'])\n",
        "dfx_dummies = pd.get_dummies(dfx['ocean_proximity'], drop_first = True)\n",
        "dfx.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "dfx = pd.concat([dfx, dfx_dummies], axis=1)\n",
        "\n",
        "# Call on your new function to create a heatmap of the current dataframe\n",
        "make_heatmap(dfx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwZ7ZaDIE57x",
        "colab_type": "text"
      },
      "source": [
        "Based upon our goal, the following features can probably be dropped:\n",
        "- **```households / total_bedrooms```** - Exhibiting multicollinearity\n",
        "- **```total_rooms / population```** - Exhibiting multicollinearity\n",
        "- **```proximity_to_store```** - Remember from our graphical analysis that this appears to be uniformly distributed and, thus, of little use for predictive purposes\n",
        "- **```ISLAND```** - This features appears to have almost no correlation with anything across the board. Investigate this further, but it looks like we might be able to drop it as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sOjtAWqWD-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare possible multicollinearity candidates\n",
        "dfx_pair = dfx[['households', 'total_bedrooms', 'total_rooms', 'population']].copy()\n",
        "sns.pairplot(data = dfx_pair, x_vars = dfx_pair.columns, y_vars = dfx_pair.columns, dropna = True); plt.show(); plt.close()\n",
        "\n",
        "# Look at the distribution of questionable features\n",
        "sns.distplot(dfx['proximity_to_store'], bins = 20); plt.show(); plt.close()\n",
        "icount = dfx['ISLAND']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EElKx_EIg2Df",
        "colab_type": "text"
      },
      "source": [
        "Having double-checked the data, it appears safe to eliminate:\n",
        "- **```total_rooms```**\n",
        "- **```households```**\n",
        "- **```proximity_to_store```**\n",
        "\n",
        "After eliminating them, we can re-check to see if we have any further problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQjmkP0EhP53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop unwanted / unnecessary features\n",
        "dfx = dfx.drop(['households', \n",
        "                'proximity_to_store'], \n",
        "               axis = 1\n",
        "               )\n",
        "\n",
        "# Also drop this feature from our temp dataframe for one last comparison\n",
        "dfx_pair = dfx_pair.drop(['households'], axis = 1)\n",
        "\n",
        "# Verify that we do not need to eliminate any more features\n",
        "make_heatmap(dfx)\n",
        "sns.pairplot(data = dfx, x_vars = dfx_pair.columns, y_vars = dfx_pair.columns, dropna = True); plt.show(); plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGyM7bnmmzwE",
        "colab_type": "text"
      },
      "source": [
        "## D. Working with Missing Data\n",
        "When first analyzing our data, we discovered that there were some \"NaN\" values (null data) in the **```total_bedrooms```** feature. Before proceeding to build our model, we must first figure out what to do with the records that contain those missing values.\n",
        "\n",
        "Let's quickly count the missing values again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C14Kman52-GA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a count of \"NaN\" or missing values by feature\n",
        "print('Count of missing values by feature:\\n')\n",
        "display(dfx.isnull().sum(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnl_YzCr5tvc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now that we have located missing data, we have *at least* four choices:\n",
        "1. Fill in missing values with the **mode**\n",
        "2. Fill in missing values with the **median**\n",
        "3. **Delete** samples with missing values\n",
        "4. **Delete** the feature(s) containing the missing data\n",
        "\n",
        "**Options 1 & 2** will depend upon the type of data being manipulated. \n",
        "- For **categorical data**, it is generally okay to use the **mode** as this represents the most frequently encountered value.\n",
        "- For **continuous data**, it is generally okay to use the **median** so as to avoid the influence of any outlier data (using the mean will allow this influence)\n",
        "\n",
        "**Option 3** (deleting samples) is considered the **last resort** as it will cause us to lose valuable data that can help our model perform better. If the number of samples to be deleted are insignificant compared to the overall size of the dataset, then this might not be a problem. Often, however, our data is limited and every sample counts, so we will want to keep every bit of it that we can.\n",
        "\n",
        "**Option 4** (deleting features with missing data) is also worth considering **if and only if the feature is not consequential for the model**. In other words, if we do not feel that the data in this feature will help us to train a more accurate prediction model, then we can just delete the feature altogether and be done with it.\n",
        "\n",
        "> **NOTE:** There are actually a host of different methods for dealing with missing data, and they range greatly in their complixity. **The method one chooses will ultimately depend upon the type of data being used and the goal(s) of the model.** For the purposes of this lesson, however, we have covered 4 possibilities simply to demonstrate that there are easy ways to manage datasets with missing values. For more information on dealing with missing data, please see the following links:\n",
        "- [Towards Data Science: Compensating for missing values](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n",
        "- [Geeks for Geeks: Excellent yet simple tutorial for working with missing data](https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/)\n",
        "- [Pandas Documentation: Working with missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
        "- [Machine Learning Mastery: Detailed options for dealing with missing data](https://machinelearningmastery.com/handle-missing-data-python/)\n",
        "\n",
        "```\n",
        "```\n",
        "In this case, we have 207 values missing out of 20,640 which represents about 1% of our data. For the purposes of this exercise, we will elect to keep that data and fill those missing values.\n",
        "\n",
        "> **Question:** Which method should we use to fill in the missing data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsLMUaAB6Yvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's look at the current state of our data prior to changes\n",
        "print('-------------------- BEFORE CHANGES --------------------')\n",
        "print('\\nCount of null values per feature:')\n",
        "print(dfx.isnull().sum(axis=0))\n",
        "\n",
        "\n",
        "# ---------------------------- APPLY CHANGES ----------------------------\n",
        "# First, create a variable to hold the median value\n",
        "tb_med = dfx['total_bedrooms'].median(axis=0)\n",
        "\n",
        "# Next, we want to fill the \"NaN\" values with the median value of the column (with current 'NaN' columns skipped in the calculation)\n",
        "dfx['total_bedrooms'] = dfx['total_bedrooms'].fillna(value = tb_med)\n",
        "\n",
        "# Change the datatype of 'total_bedrooms' to 'int' (since we cannot really have partial bedrooms)\n",
        "dfx['total_bedrooms'] = dfx['total_bedrooms'].astype(int)\n",
        "\n",
        "\n",
        "# ---------------------------- VERIFY CHANGES ----------------------------\n",
        "# Next, we want to verify that our changes have been correctly applied\n",
        "print('-------------------- AFTER CHANGES --------------------')\n",
        "print('\\nCount of null values per feature:')\n",
        "print(dfx.isnull().sum(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFuzjpwUaCme",
        "colab_type": "text"
      },
      "source": [
        "Next, we want to inspect **```ISLAND```** to count the instances of 0 and 1. We will also compare the categorical values against one another to get an idea of relevancy in terms of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztAeYWpSeG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze our categorical values for any multicollinearity or other issues\n",
        "cols = dfx[['ISLAND', 'INLAND', 'NEAR BAY', 'NEAR OCEAN']].copy()\n",
        "make_heatmap(cols)\n",
        "\n",
        "# Count only the records that have '1'\n",
        "print('\\n\\nValue Counts for Categoricals:\\n')\n",
        "print(dfx['ISLAND'].value_counts(), '\\n')\n",
        "print(dfx['INLAND'].value_counts(), '\\n')\n",
        "print(dfx['NEAR BAY'].value_counts(), '\\n')\n",
        "print(dfx['NEAR OCEAN'].value_counts(), '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdckGZjHY_I5",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the graph and counts above, there are only **5 instances** in which the **```ISLAND```** feature is relevant. We will need to keep an eye on this as we move through the next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxD9xTIGDNft",
        "colab_type": "text"
      },
      "source": [
        "## E. Working with Outlier Data\n",
        "\n",
        "Now that we have filled in our missing data, let's take a look at our remaining features' distributions. This will help us to get an idea of other changes that we might need to make.\n",
        "\n",
        "First, let's look at some graphs (histograms and boxplots).\n",
        "\n",
        ">Websites Referenced:\n",
        "- [Towards Data Science: Working with Outlier Data](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
        "- [Medium.com: Standardize and Normalize Data](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir9wN-IaJQTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at distributions to identify possible outlier data\n",
        "fig, axes = plt.subplots(nrows=3, \n",
        "                         ncols=4, \n",
        "                         figsize = (20, 10)\n",
        "                         )\n",
        "# Flatten the axes to make them easier to reference\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each graph that you would like to see\n",
        "bins = 20\n",
        "dfx.hist('longitude', ax = axes[0], bins = bins)\n",
        "dfx.hist('latitude', ax = axes[1], bins = bins)\n",
        "dfx.hist('population', ax = axes[2], bins = bins)\n",
        "dfx.hist('housing_median_age', ax = axes[3], bins = bins)\n",
        "dfx.hist('total_rooms', ax = axes[4], bins = bins)\n",
        "dfx.hist('total_bedrooms', ax = axes[5], bins = bins)\n",
        "dfx.hist('median_income', ax = axes[6], bins = bins)\n",
        "dfx.hist('median_house_value', ax = axes[7], bins = bins)\n",
        "sns.countplot(x = dfx['INLAND'], ax = axes[8])\n",
        "sns.countplot(x = dfx['NEAR BAY'], ax = axes[9])\n",
        "sns.countplot(x = dfx['NEAR OCEAN'], ax = axes[10])\n",
        "sns.countplot(x = dfx['ISLAND'], ax = axes[11]); plt.tight_layout(); plt.show(); plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUVhsR9fcFKz",
        "colab_type": "text"
      },
      "source": [
        "We can also use **boxplots** and **logistic plots** to get a better idea about outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWEa7wnd25k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at distributions to identify possible outlier data\n",
        "fig, axes = plt.subplots(nrows=2, \n",
        "                         ncols=4, \n",
        "                         figsize = (16, 8)\n",
        "                         )\n",
        "# Flatten the axes to make them easier to reference\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each graph that you would like to see\n",
        "bins = 20\n",
        "sns.boxplot(x=df['longitude'], ax = axes[0])\n",
        "sns.boxplot(x=df['latitude'], ax = axes[1])\n",
        "sns.boxplot(x=df['housing_median_age'], ax = axes[2])\n",
        "sns.boxplot(x=df['total_rooms'], ax = axes[3])\n",
        "sns.boxplot(x=df['total_bedrooms'], ax = axes[4])\n",
        "sns.boxplot(x=df['population'], ax = axes[5])\n",
        "sns.boxplot(x=df['median_income'], ax = axes[6])\n",
        "sns.boxplot(x=df['median_house_value'], ax = axes[7])\n",
        "sns.lmplot(x = 'median_house_value', y = 'INLAND', logistic = True, data = dfx, ci = None)\n",
        "sns.lmplot(x = 'median_house_value', y = 'NEAR BAY', logistic = True, data = dfx, ci = None)\n",
        "sns.lmplot(x = 'median_house_value', y = 'NEAR OCEAN', logistic = True, data = dfx, ci = None)\n",
        "sns.lmplot(x = 'median_house_value', y = 'ISLAND', logistic = True, data = dfx, ci = None)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlj35A2VrC1L",
        "colab_type": "text"
      },
      "source": [
        "### i. Using the Z-score Method with Outlier Data\n",
        "\n",
        "There are a number of ways in which to deal with outlier data, but one of the more common ones is utilizing a **z-score** which gives us the ability to exclude items based on standard deviation. This, in turn, will allow us to exclude data that falls outside of standard deviation boundaries of our choice (thus eliminating outliers). We will then look at our data again and see if this has helped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXRvD85LkVX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that we will use the following library to accomplish this task (imported in \"Setup Environment\")\n",
        "#  from scipy import stats\n",
        "\n",
        "# ------------------------------------- Calculate the z-score for all data -------------------------------------\n",
        "# Get the absolute z-score value for our dataset\n",
        "z = np.abs(stats.zscore(dfx))\n",
        "\n",
        "# Finally, we remove those values with standard deviations greater than 3\n",
        "dfz = dfx[(z < 3).all(axis = 1)]\n",
        "\n",
        "# Create a \"figure\" that can hold multiple plots\n",
        "fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(nrows = 2, \n",
        "                                                                ncols = 4, \n",
        "                                                                figsize = (16, 8)\n",
        "                                                                )\n",
        "\n",
        "# Show boxplots for all of our current non-categorical features\n",
        "sns.boxplot(x=dfz['longitude'], ax = ax1)\n",
        "sns.boxplot(x=dfz['latitude'], ax = ax2)\n",
        "sns.boxplot(x=dfz['housing_median_age'], ax = ax3)\n",
        "sns.boxplot(x=dfz['total_rooms'], ax = ax4)\n",
        "sns.boxplot(x=dfz['total_bedrooms'], ax = ax5)\n",
        "sns.boxplot(x=dfz['population'], ax = ax6)\n",
        "sns.boxplot(x=dfz['median_income'], ax = ax7)\n",
        "sns.boxplot(x=dfz['median_house_value'], ax = ax8)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Also show the data description to get accurate values for the quartiles\n",
        "print('Original Dataframe (dfx):\\n')\n",
        "display(dfx.describe())\n",
        "\n",
        "print('Z-score Dataframe (dfz):\\n')\n",
        "display(dfz.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Rw9CEqzlQy",
        "colab_type": "text"
      },
      "source": [
        "> **Questions:**\n",
        "- Were there any **substantial changes** to our data using the z-score method for removing outliers?\n",
        "- Did this **help our goal?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBDGPfEnlAmq",
        "colab_type": "text"
      },
      "source": [
        "### ii. Using the Interquartile Range Method (IQR) with Outlier Data\n",
        "\n",
        "Another way to work with outliers is to eliminate those values that fall outside of our IQR (Interquartile Range). This will work somewhat similarly to the z-score, but our threshold for eliminating outlier data will be based upon quartiles.\n",
        "\n",
        "> Websites referenced:\n",
        "- [Towards Data Science: Detect and Remove Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
        "- [Purple Math: How IQR Relates to Box-and-Whisker Graphs (great, simple explanation of IQR)](https://www.purplemath.com/modules/boxwhisk3.htm)\n",
        "\n",
        "After running the code below, we will have the IQR necessary to calculate our threshold for outliers. The formula is:\n",
        "\n",
        "- ```lower boundary = q1 - (1.5 * iqr)```\n",
        "- ```upper boundary = q3 + (1.5 * iqr)```\n",
        "\n",
        "The subsequent code will then look for and remove all samples that fall outside these boundaries and leave us (hopefully) with substantially fewer outliers. \n",
        "\n",
        "We will then compare both methods (z-score and IQR) against the original dataset to see which one gives us the most usable data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Za0cQflbfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove outliers with IQR method\n",
        "\n",
        "# Establish variables\n",
        "q1 = dfx.quantile(0.25)\n",
        "q3 = dfx.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "print('\\n\\nValues less than these will be removed:\\n', \n",
        "      '-'*30,\n",
        "      '\\n',\n",
        "      lower\n",
        "      )\n",
        "print('\\n\\nValues greater than these will be removed:\\n', \n",
        "      '-'*30,\n",
        "      '\\n', \n",
        "      upper\n",
        "      )\n",
        "\n",
        "# Remove samples outside of the upper and lower boundaries\n",
        "dfi = dfx[~((dfx < lower) | (dfx > upper)).any(axis = 1)]\n",
        "\n",
        "# Compare housing_median_age\n",
        "fig, (ax1a, ax1b, ax1c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['longitude'],  ax = ax1a, color='g')\n",
        "sns.boxplot(x=dfz['longitude'],  ax = ax1b)\n",
        "sns.boxplot(x=dfx['longitude'],  ax = ax1c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare total_rooms\n",
        "fig, (ax2a, ax2b, ax2c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['latitude'],  ax = ax2a, color='g')\n",
        "sns.boxplot(x=dfz['latitude'],  ax = ax2b)\n",
        "sns.boxplot(x=dfx['latitude'],  ax = ax2c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare total_bedrooms\n",
        "fig, (ax3a, ax3b, ax3c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['housing_median_age'], ax = ax3a, color='g')\n",
        "sns.boxplot(x=dfz['housing_median_age'], ax = ax3b)\n",
        "sns.boxplot(x=dfx['housing_median_age'], ax = ax3c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare population\n",
        "fig, (ax4a, ax4b, ax4c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['total_rooms'], ax = ax4a, color='g')\n",
        "sns.boxplot(x=dfz['total_rooms'], ax = ax4b)\n",
        "sns.boxplot(x=dfx['total_rooms'], ax = ax4c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare households\n",
        "fig, (ax5a, ax5b, ax5c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['total_bedrooms'], ax = ax5a, color='g')\n",
        "sns.boxplot(x=dfz['total_bedrooms'], ax = ax5b)\n",
        "sns.boxplot(x=dfx['total_bedrooms'], ax = ax5c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare median_income\n",
        "fig, (ax6a, ax6b, ax6c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['median_income'], ax = ax6a, color='g')\n",
        "sns.boxplot(x=dfz['median_income'], ax = ax6b)\n",
        "sns.boxplot(x=dfx['median_income'], ax = ax6c, color='grey')\n",
        "plt.show()\n",
        "plt.close()\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compare median_house_value\n",
        "fig, (ax7a, ax7b, ax7c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n",
        "sns.boxplot(x=dfi['median_house_value'], ax = ax7a, color='g')\n",
        "sns.boxplot(x=dfz['median_house_value'], ax = ax7b)\n",
        "sns.boxplot(x=dfx['median_house_value'], ax = ax7c, color='grey')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sygsyjg7K5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare data descriptions for all three dataframes\n",
        "print('Z-Score Dataframe (dfz):\\n')\n",
        "display(dfz.describe())\n",
        "\n",
        "print('Original Dataframe (dfx):\\n')\n",
        "display(dfx.describe())\n",
        "\n",
        "print('IQR Dataframe (dfi):\\n')\n",
        "display(dfi.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEceAPqlnBBo",
        "colab_type": "text"
      },
      "source": [
        "Everything appears to be better using the IQR method with the exception of **```median_house_value```**, which we might want to inspect a little more closely.Let's look at the new data in a distribution plot (histogram) to see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHu_s_ZnALc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare median_house_price via histograms\n",
        "fig, (ax8a, ax8b, ax8c) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 5), )\n",
        "ax8a = sns.distplot(dfi['median_house_value'],  \n",
        "                    kde = True,\n",
        "                    hist = True,\n",
        "                    bins = 50, \n",
        "                    color = 'green',\n",
        "                    ax = ax8a,\n",
        "                    hist_kws={'alpha':1}, \n",
        "                    ).set_title('dfi (IQR)')\n",
        "ax8b = sns.distplot(dfz['median_house_value'], \n",
        "                    kde = True,\n",
        "                    hist = True,\n",
        "                    bins = 50,\n",
        "                    ax = ax8b,\n",
        "                    hist_kws={'alpha':1}\n",
        "                    ).set_title('dfz (Z-score)')\n",
        "ax8c = sns.distplot(df['median_house_value'],  \n",
        "                    kde = True,\n",
        "                    hist = True,\n",
        "                    bins = 50, \n",
        "                    color = 'grey',\n",
        "                    ax = ax8c,\n",
        "                    hist_kws={'alpha':1}\n",
        "                    ).set_title('dfx (Original)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9RciXN22Z0L",
        "colab_type": "text"
      },
      "source": [
        "The IQR method appears to eliminate many of the outlier problems that exist while also handling the problem of the **```median_house_value```** \"wall\" problem at ~500,000. We will be capable of using any of the dataframes as we move forward, but the **IQR-based dataframe seems likely to produce the best model**. We can experiment with that hypothesis as we build out various models in the next setps."
      ]
    }
  ]
}
