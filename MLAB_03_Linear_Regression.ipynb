{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-03-Linear-Regression.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_03_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBVG5uaPJnDO",
        "colab_type": "text"
      },
      "source": [
        "# Build a Linear Regression Model\n",
        "\n",
        "At this point, we have now cleaned our dataset and are ready to build a machine learning model. We want to predict housing prices and we have labeled data, so we will build supervised learning models with **Linear Regression**, a **Decision Tree** and **Gradient Boosting** (which creates multiple decision trees). \n",
        "\n",
        "Before we begin building a model, **there are a few definitions that we should cover** so that you can better understand what is happening in the code below.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "## Key Definitions\n",
        "- **[Estimator / Model](https://scikit-learn.org/stable/user_guide.html)**\n",
        "  - A method that neatly packages up all of the low-level training code. \n",
        "    - This method will loop through a training set using a [score method](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation) to determine \"loss\" (deviation from the predicted value).\n",
        "    - A **linear regression estimator**, for example, will try to find a single line that has the least amount of deviation from all target values.\n",
        "  - Estimators allow us to **configure** training loops rather than coding everything from the ground up.\n",
        "  - [Examples](https://scikit-learn.org/stable/user_guide.html) include **linear regressors**, **decision trees**, **k-Means Clustering**, etc.\n",
        "  >**Note:** The estimators called by sklearn will calculate error/loss/cost exactly rather than using Gradient Descent to minimize it incrementally via \"epochs\". See the exact description and calculations at the following links:\n",
        "    - [Data Science Exchange: Epochs discussion](https://datascience.stackexchange.com/questions/29044/how-many-epochs-does-fit-method-run)\n",
        "    - [Machine Learning Mastery: Gradient Descent](https://machinelearningmastery.com/gradient-descent-for-machine-learning/).\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "- **[Scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)**\n",
        "  - A method that measures \"loss\" (a.k.a. \"cost\" or \"error\") when training an estimator\n",
        "  - Can be measured in various ways such as **mean squared error**, **median absolute error**, etc.\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "- **Splitting Data**\n",
        "  - Refers to the act of separating data into **training** and **testing** (and possibly **validation**) sets\n",
        "  - The parameter **```test_size```** dictates the proportion of data to be set aside as the test set (with the remaining being the training set)\n",
        "    - Splits are typically 70/30, 80/20 or somewhere between for training/test sets\n",
        "  - Accomplished with the **[train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** method which will return 4 arrays of data (X_train, X_test, y_train, y_test)\n",
        "  - These 4 arrays are then used when **fitting** the data to the esimator\n",
        "  - Data can (and should be) **shuffled** with this method (to reduce chances of bias)\n",
        "  > **Question**: Why do we need to split our data into training and testing sets?\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "- **Hyperparameters**\n",
        "  - These are values that we define **before** training a model. Regular parameters learn and change during the training process, but hyperparameters cannot \"learn\" and must be set beforehand.\n",
        "  - Includes things like **```n_estimators```**, **```max_depth```**, etc.\n",
        "  - We can \"tune\" hyperparameters in order to get better model performance.\n",
        "  - We can also employ a \"GridSearchCV\" function that will analyze all combinations of hyperparameters and reveal which combinations produced the best performance (shown at the end of the lesson). We use this function to reduce the work necessary to create a \"good\" model.\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "- **[Fitting](https://stackoverflow.com/questions/45704226/what-does-fit-method-in-scikit-learn-do) a model/estimator**\n",
        "  - This refers to the act of actually **training** the estimator on the training set of data (X_train, y_train)\n",
        "  - The estimator loops through the data (using the hyperparameter settings), measures loss with the scoring method and produces a **trained model**.\n",
        "  - The trained model is then run against the test set to evaluate the accuracy.\n",
        "  - The error rates returned can be used to determine if:\n",
        "    - The model needs to be further refined\n",
        "    - Data needs to be further engineered\n",
        "    - The model is ready to deploy.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Basic steps for Building a Machine Learning Model\n",
        "### 1. **Import necessary libraries**\n",
        "  - In order to train a model, we must identify the library that contains it and call the import function so that we can use it in our code (```from sklearn import linear_model```). For this lesson, we also need to import some other libraries such as ```preprossesing``` and ```Counter```. \n",
        "\n",
        "### 2. **Identify the Target Variable**\n",
        "  - We must then **identify our target variable**. Basically, we can choose any feature that we would like to predict as our target, and the remaining features will be used to try and determine an effective model that can predict any value of the target feature. \n",
        "  - For this lesson, we will use **```median_house_value```** as our target variable, meaning that we are trying to predict a house's value based upon the factors contained in the other features (```total_rooms, total_bedrooms, median_income```, etc). We can easily change this up later, however, if we want to try and predict another variable instead.\n",
        "\n",
        "### 3. **Split the Dataset into Training and Testing sets**\n",
        "  - In this step, we set our training and testing sets, determine the test set size and shuffle our data (to prevent bias).\n",
        "  - **NOTE:** It's vitally important to **shuffle** the dataset at this point in order to randomize the data and prevent bias from entering our model.\n",
        "\n",
        "### 4. **Fit (or \"train\") the Model**\n",
        "  - At this point, we call our estimator and \"fit\" it to our training arrays (X_train, y_train)\n",
        "    - ```estimator.fit(X_train, y_train)```\n",
        "  - Once trained, the **test data** is run through the model and **error metrics** are gathered.\n",
        "\n",
        "### 5. **Visualize the data**\n",
        "- Once everything is completed, we choose an appropriate way to visualize our model's performance.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> Websites Referenced:\n",
        "- [Towards Data Science: Introduction to Linear Regression in Python](https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0)\n",
        "- [Sprinboard Blog: Linear Regression in Python: A Tutorial](https://www.springboard.com/blog/linear-regression-in-python-a-tutorial/)\n",
        "- [Geeks for Geeks: Linear Regression (great, simple explanation of the math)](https://www.geeksforgeeks.org/ml-linear-regression/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu8TIv3rDvGk",
        "colab_type": "text"
      },
      "source": [
        "## A. Setup Environment\n",
        "- Import libraries\n",
        "- Prepare dataset(s)\n",
        "- Create functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaiRzqceGvsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n",
        "df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n",
        "df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n",
        "df.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "\n",
        "# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n",
        "df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# ------------------------------------- FIX MISSING DATA -------------------------------------\n",
        "tb_med = df['total_bedrooms'].median(axis=0)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n",
        "df.name = 'df'\n",
        "\n",
        "# ------------------------------------- Z-SCORE -------------------------------------\n",
        "z = np.abs(stats.zscore(df))\n",
        "dfz = df[(z < 3).all(axis = 1)]\n",
        "dfz.name = 'dfz'\n",
        "\n",
        "# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n",
        "q1 = df.quantile(0.25)\n",
        "q3 = df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n",
        "dfi.name = 'dfi'\n",
        "#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(df = 'df'):\n",
        "  corr = df.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def make_heading(heading = 'heading'):\n",
        "  print('{0}:\\n'.format(heading), '-' * 30)\n",
        "\n",
        "\n",
        "def show_coef(df_in, model):\n",
        "  coef_df = pd.DataFrame(model.coef_, df_in.columns, columns=['Coefficient'])\n",
        "  make_heading('\\n\\nCoefficients')\n",
        "  display(coef_df)\n",
        "  make_heading('\\n\\nIntercept')\n",
        "  display(model.intercept_)\n",
        "\n",
        "\n",
        "def drop_features(df_in):\n",
        "  df_in = df_in.drop([#'median_house_value',\n",
        "                      'longitude',\n",
        "                      'latitude',\n",
        "                      #'housing_median_age',\n",
        "                      'total_rooms',\n",
        "                      'total_bedrooms',\n",
        "                      #'median_income',\n",
        "                      'INLAND',\n",
        "                      #'NEAR BAY',\n",
        "                      #'NEAR OCEAN'\n",
        "                      ],\n",
        "                      axis = 1\n",
        "                      )\n",
        "  return features_dfx\n",
        "\n",
        "\n",
        "def plot_predictions(y_test, y_pred):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax = plt.subplot()\n",
        "  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  ax.set_title(\"Ground Truth vs Predicted\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def run_linear_model(model, X, y, test_size = '0.3'):\n",
        "  # Split the dataset into training and testing arrays\n",
        "  #   We take our X and y variables that we have just created and now split each one into a training set and testing\n",
        "  #   set: X_train, X_test, y_train, y_test)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                      y,\n",
        "                                                      test_size=test_size,    # reserve this percentage of our data for testing, use the rest for training\n",
        "                                                      shuffle = True,\n",
        "                                                      random_state = 21\n",
        "                                                      )\n",
        "\n",
        "  # Get the name of the model (for use in our display functions)\n",
        "  model_name = type(model).__name__\n",
        "\n",
        "  # Fit (or train) the model (have the model create coefficient multipliers which will be used to predict \"y\")\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Here we want to input our X_test array to make predictions based upon our trained model\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Gather metrics on model performance\n",
        "  model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "  model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "  mean_ae = metrics.mean_absolute_error(y_test, y_pred)  # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "  median_ae = metrics.median_absolute_error(y_test, y_pred)  # Median absolute error\n",
        "  rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "  ev = metrics.explained_variance_score(y_test, y_pred) # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "  predictive_accuracy = metrics.r2_score(y_test, y_pred)  # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "\n",
        "  # Display the shape of each new array:\n",
        "  print('\\n' * 5, '*' * 50,' ', model_name,' ', '*' * 50)\n",
        "  make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "  print('X_train shape: ', X_train.shape)\n",
        "  print('X_test shape:  ', X_test.shape)\n",
        "  print('y_train shape: ', y_train.shape)\n",
        "  print('y_test shape:  ', y_test.shape)\n",
        "  print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "  # Show the coefficients\n",
        "  show_coef(features_dfx, model)\n",
        "\n",
        "  # Show metrics on model performance (training and testing)\n",
        "  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "  print(\"Training Accuracy                 (X_train, y_train):  %.2f\" % model_train_score)\n",
        "  print(\"Test Accuracy                     (X_test, y_test):    %.2f\" % model_test_score)\n",
        "  print(\"Predictive Accuracy (R^2)         (y_test, y_pred):    %.2f\" % predictive_accuracy)\n",
        "  print(\"Explained Variance (1 is best)    (y_test, y_pred):    %.2f\" % ev)\n",
        "  print(\"Mean Absolute Error on Test Set   (y_test, y_pred):    %.2f\" % mean_ae)\n",
        "  print(\"Median Absolute Error on Test Set (y_test, y_pred):    %.2f\" % median_ae)\n",
        "  print(\"RMSE on Test Set                  (y_test, y_pred):    %.2f\" % rmse)\n",
        "  print('=' * 50, '\\n\\n\\n')\n",
        "\n",
        "  plot_predictions(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NoTOrvD-rq",
        "colab_type": "text"
      },
      "source": [
        "## B. Choose a dataset to use\n",
        "- Three datasets have been created:\n",
        "  1. **df** -- the **original dataset** ('NaN' values have been replaced with median, so there are no null values)\n",
        "  2. **dfz** -- a dataset that has used the **z-score method** for removing outlier data\n",
        "  3. **dfi** -- a dataset that has used the **IQR method** for removing outlier data\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "**NOTE:** When choosing your dataset, you will need to change two commands to reflect the dataset that you want to use. In the following example, the \n",
        "\n",
        "**Original code:** --------------------------------------->  **New code** (For example, to change **from** 'dfz' **to** 'dfi' dataset):\n",
        "```\n",
        "dfx = dfz.copy()      -->   dfx = dfi.copy()\n",
        "dfx.name = dfz.name   -->   dfx.name = dfi.name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  \n",
        "  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxObYDCISc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *************************** DETERMINE DATA FOR USE ***************************\n",
        "# Choose which dataframe you would like to use (dfi: IQR, dfz: z-score, df: original (with replaced NaN values))\n",
        "dfx = df.copy()\n",
        "dfx.name = df.name\n",
        "\n",
        "# Show a heatmap for the given dataframe you want to use\n",
        "make_heatmap(dfx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-FFTxqnN_Mf",
        "colab_type": "text"
      },
      "source": [
        "> Websites Referenced:\n",
        "- [Scikit-Learn Documentation: Model Evaluation](https://scikit-learn.org/0.15/modules/model_evaluation.html)\n",
        "- [Helpful breakdown of model metrics](https://datascience.stackexchange.com/questions/28426/train-accuracy-vs-test-accuracy-vs-confusion-matrix)\n",
        "- [Ridge and Lasso Regression: Regularization of Coefficients](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgVl5bOTRjMD",
        "colab_type": "text"
      },
      "source": [
        "## C. Step by step: Building a Linear Regression model\n",
        "- In this section, we walk through building a linear regression model using the dataframe chosen in the previous step.\n",
        "\n",
        "### The basic steps are:\n",
        "  1. Define the **features** and the **target variable**.\n",
        "  2. **Split** the dataset into **training** and **testing** partitions\n",
        "  3. **Define the model:**\n",
        "    - Determine which model to use (LinearRegression, Lasso, etc.)\n",
        "    - **Train the model** - run the model against our training set\n",
        "    - Use the trained model to **predict against the test set**\n",
        "  4. **Capture metrics** for the model\n",
        "    - Training accuracy\n",
        "    - Testing accuracy\n",
        "    - Prediction error\n",
        "    - Error values (Mean Absolute Error, Median Absolute Error, RMSE, etc.)\n",
        "  5. **Analyze metrics**\n",
        "    - Look at the varirous accuracy and error scores to **determine how well the model fits** (overfit, underfit, good, bad, etc.)\n",
        "  6. **Plot graphs** to visualize the various accuracy / error metrics\n",
        "  7. **Predict new values** to see if the model performs as expected (given the accuracy and coefficient values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1UmJXYNcy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> STEP BY STEP LINEAR MODEL CONSTRUCTION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "\n",
        "# ---------------------------- Identify Target Variable ----------------------------\n",
        "# Create a copy of the dataframe with only the desired features (dropping the target feature)\n",
        "features_dfx = dfx.copy()\n",
        "features_dfx = features_dfx.drop(['median_house_value', # You *MUST* drop 'median_house_value, as that is our target variable; Hashtag (#) means that you want to KEEP the value\n",
        "                                  'longitude',\n",
        "                                  'latitude',\n",
        "                                  #'housing_median_age',\n",
        "                                  #'total_rooms',\n",
        "                                  #'total_bedrooms',\n",
        "                                  #'median_income',\n",
        "                                  #'INLAND',\n",
        "                                  'NEAR BAY',\n",
        "                                  'NEAR OCEAN'\n",
        "                                  ],\n",
        "                                 axis = 1\n",
        "                                 )\n",
        "\n",
        "# ---------------------------- SPLIT THE DATASET ----------------------------\n",
        "# We extract the values of our two datasets into \"X\" and \"y\" variables for use in later functions\n",
        "# The X variables are the \"explanatory\" numbers (or images). They are the numbers that we use to try and predict\n",
        "#   what the \"y\" variable is.\n",
        "\n",
        "# \"X\" are the independent variables, the \"featues\" that we will use to try and predict \"y\"\n",
        "X = features_dfx.values\n",
        "\n",
        "# \"y\" is the dependent variable (the \"label\"); the actual value that we are trying to learn to predict. \n",
        "#   Each time we make a prediction, it is compared to the actual values, and the \"loss\" (the difference between\n",
        "#   the prediction and real number) is added to previous \"loss\". \n",
        "y = dfx['median_house_value'].values\n",
        "\n",
        "# Split the dataset into training and testing arrays\n",
        "#   We take our X and y variables that we have just created and now split each one into a training set and testing\n",
        "#   set: X_train, X_test, y_train, y_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.3,    # reserve this percentage of our data for testing, use the rest for training\n",
        "                                                    shuffle = True,\n",
        "                                                    random_state = 21\n",
        "                                                    )\n",
        "\n",
        "\n",
        "# ---------------------------- CREATE THE MODEL  ----------------------------\n",
        "# Here we define our model as a Linear Regression algorithm\n",
        "model = linear_model.LinearRegression(n_jobs=-1)\n",
        "model_name = type(model).__name__\n",
        "\n",
        "# Fit (or train) the model (have the model create coefficient multipliers which will be used to predict \"y\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Here we want to input our X_test array to make predictions based upon our trained model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# ----------------------------- METRICS --------------------------------------\n",
        "model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "mean_ae = metrics.mean_absolute_error(y_test, y_pred)  # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "median_ae = metrics.median_absolute_error(y_test, y_pred)  # Median absolute error\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "ev = metrics.explained_variance_score(y_test, y_pred) # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "predictive_accuracy = metrics.r2_score(y_test, y_pred)  # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "\n",
        "# Display the shape of each array:\n",
        "make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "print('X_train shape: ', X_train.shape)\n",
        "print('X_test shape:  ', X_test.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('y_test shape:  ', y_test.shape)\n",
        "print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "# Display the model coefficients and intercept (bias)\n",
        "coef = pd.DataFrame(model.coef_, features_dfx.columns, columns=['Coefficient'])\n",
        "make_heading('\\n\\nCoefficient(s)')\n",
        "display(coef)\n",
        "make_heading('\\n\\nIntercept')\n",
        "display(model.intercept_)\n",
        "\n",
        "# Display training set metrics\n",
        "make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "print(\"Training Accuracy (score)                 (X_train, y_train):  %.2f\" % model_train_score)\n",
        "print(\"Test Accuracy (score)                     (X_test, y_test):    %.2f\" % model_test_score)\n",
        "print(\"Predictive Accuracy (R^2 score)           (y_test, y_pred):    %.2f\" % predictive_accuracy)\n",
        "print(\"Explained Variance (1 is best) (loss)     (y_test, y_pred):    %.2f\" % ev)\n",
        "print(\"Mean Absolute Error on Test Set (loss)    (y_test, y_pred):    %.2f\" % mean_ae)\n",
        "print(\"Median Absolute Error on Test Set (loss)  (y_test, y_pred):    %.2f\" % median_ae)\n",
        "print(\"RMSE on Test Set (loss)                   (y_test, y_pred):    %.2f\" % rmse)\n",
        "\n",
        "\n",
        "# ----------------------------- PLOTTING --------------------------------------\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "ax.set_xlabel('Actual')\n",
        "ax.set_ylabel('Predicted')\n",
        "ax.set_title(\"Ground Truth vs Predicted\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3xz7p5TNUu",
        "colab_type": "text"
      },
      "source": [
        "Important Definitions:\n",
        "- **Accuracy:** The amount of correct classifications / the total amount of classifications.\n",
        "- **Train accuracy:** The accuracy of a model on examples it was constructed on.\n",
        "- **Test accuracy:** The accuracy of a model on examples it hasn't seen.\n",
        "- **Explained Variance Score:** Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set.\n",
        "\n",
        "> Websites referenced:\n",
        "- [MAE and RMSE — Which Metric is Better?](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)\n",
        "  - **Mean Absolute Error (MAE):** MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
        "  - **Root mean squared error (RMSE):** RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation. **RMSE tends to punish large errors more**.\n",
        "  - **NOTE:** Both values are *negatively oriented* meaning that lower is better. Also, **RMSE should be more useful when large errors are particularly undesirable.**\n",
        "- [Scikit-Learn Documentation: Model Evaluation](https://scikit-learn.org/0.15/modules/model_evaluation.html)\n",
        "- [**Excellent Tutorial** -- Ritchie Ng: Learning to Evaluate Linear Regression Models (single and multiple)](https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/)\n",
        "  - Definitely take some time to read through this tutorial. Ritchie provides an excellent breakdown of how to construct single and multiple linear regression, the underlying math and, critically, how one can interpret the results.\n",
        "- [Helpful breakdown of model metrics](https://datascience.stackexchange.com/questions/28426/train-accuracy-vs-test-accuracy-vs-confusion-matrix)\n",
        "- [Machine Learning Mastery: Evaluating Model Metrics (great examples!)](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)\n",
        "- [Effects of Alpha on Regression](https://chrisalbon.com/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkhhm8ZBl03a",
        "colab_type": "text"
      },
      "source": [
        "## D. Run Multiple Linear Regression Models Using Functions\n",
        "By moving our \"step-by-step\" code into a function, we can make running a model much easier and, therefore, compare multiple models against one another. \n",
        "\n",
        "This procedure abstracts the code in such a way as to make it reusable for different models. The function will:\n",
        "  1. Take (**```model, X, y, test_size```**) as parameters\n",
        "  2. Split the X and y arrays into training and testing sets according to the **```test_size```** (i.e. test_size = 0.2 means that we want 20% of the data to be set aside for testing)\n",
        "  3. Train the model\n",
        "  4. Gather metrics\n",
        "  5. Create a heatmap for the chosen dataframe\n",
        "  6. Print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pD6UvqX5kUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n",
        "df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n",
        "df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n",
        "df.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "\n",
        "# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n",
        "df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# ------------------------------------- FIX MISSING DATA -------------------------------------\n",
        "tb_med = df['total_bedrooms'].median(axis=0)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n",
        "df.name = 'df'\n",
        "\n",
        "# ------------------------------------- Z-SCORE -------------------------------------\n",
        "z = np.abs(stats.zscore(df))\n",
        "dfz = df[(z < 3).all(axis = 1)]\n",
        "dfz.name = 'dfz'\n",
        "\n",
        "# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n",
        "q1 = df.quantile(0.25)\n",
        "q3 = df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n",
        "dfi.name = 'dfi'\n",
        "#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(df = 'df'):\n",
        "  corr = df.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def make_heading(heading = 'heading'):\n",
        "  print('{0}:\\n'.format(heading), '-' * 30)\n",
        "\n",
        "\n",
        "def show_coef(df_in, model):\n",
        "  coef_df = pd.DataFrame(model.coef_, df_in.columns, columns=['Coefficient'])\n",
        "  make_heading('\\n\\nCoefficients')\n",
        "  display(coef_df)\n",
        "  make_heading('\\n\\nIntercept')\n",
        "  display(model.intercept_)\n",
        "\n",
        "\n",
        "def drop_features(df_in):\n",
        "  df_in = df_in.drop([#'median_house_value',\n",
        "                      'longitude',\n",
        "                      'latitude',\n",
        "                      #'housing_median_age',\n",
        "                      'total_rooms',\n",
        "                      'total_bedrooms',\n",
        "                      #'median_income',\n",
        "                      'INLAND',\n",
        "                      #'NEAR BAY',\n",
        "                      #'NEAR OCEAN'\n",
        "                      ],\n",
        "                      axis = 1\n",
        "                      )\n",
        "  return features_dfx\n",
        "\n",
        "\n",
        "def plot_predictions(y_test, y_pred):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax = plt.subplot()\n",
        "  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  ax.set_title(\"Ground Truth vs Predicted\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def run_linear_model(model, X, y, test_size = '0.3'):\n",
        "  # Split the dataset into training and testing arrays\n",
        "  #   We take our X and y variables that we have just created and now split each one into a training set and testing\n",
        "  #   set: X_train, X_test, y_train, y_test)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                      y,\n",
        "                                                      test_size=test_size,    # reserve this percentage of our data for testing, use the rest for training\n",
        "                                                      shuffle = True,\n",
        "                                                      random_state = 21\n",
        "                                                      )\n",
        "\n",
        "  # Get the name of the model (for use in our display functions)\n",
        "  model_name = type(model).__name__\n",
        "\n",
        "  # Fit (or train) the model (have the model create coefficient multipliers which will be used to predict \"y\")\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Here we want to input our X_test array to make predictions based upon our trained model\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Display the shape of each new array:\n",
        "  print('\\n' * 5, '*' * 50,' ', model_name,' ', '*' * 50)\n",
        "  make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "  print('X_train shape: ', X_train.shape)\n",
        "  print('X_test shape:  ', X_test.shape)\n",
        "  print('y_train shape: ', y_train.shape)\n",
        "  print('y_test shape:  ', y_test.shape)\n",
        "  print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "  # Gather metrics on model performance\n",
        "  model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "  model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "  mean_ae = metrics.mean_absolute_error(y_test, y_pred)  # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "  median_ae = metrics.median_absolute_error(y_test, y_pred)  # Median absolute error\n",
        "  rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "  ev = metrics.explained_variance_score(y_test, y_pred) # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "  predictive_accuracy = metrics.r2_score(y_test, y_pred)  # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "\n",
        "  # Show the coefficients\n",
        "  show_coef(features_dfx, model)\n",
        "\n",
        "  # Show metrics on model performance (training and testing)\n",
        "  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "  print(\"Training Accuracy                 (X_train, y_train):  %.2f\" % model_train_score)\n",
        "  print(\"Test Accuracy                     (X_test, y_test):    %.2f\" % model_test_score)\n",
        "  print(\"Predictive Accuracy (R^2)         (y_test, y_pred):    %.2f\" % predictive_accuracy)\n",
        "  print(\"Explained Variance (1 is best)    (y_test, y_pred):    %.2f\" % ev)\n",
        "  print(\"Mean Absolute Error on Test Set   (y_test, y_pred):    %.2f\" % mean_ae)\n",
        "  print(\"Median Absolute Error on Test Set (y_test, y_pred):    %.2f\" % median_ae)\n",
        "  print(\"RMSE on Test Set                  (y_test, y_pred):    %.2f\" % rmse)\n",
        "  print('=' * 50, '\\n\\n\\n')\n",
        "\n",
        "  plot_predictions(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQEWUEqiY6dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------- Identify Target Variable ----------------------------\n",
        "# Create a copy of the dataframe with only the desired features (dropping the target feature)\n",
        "features_dfx = dfx.copy()\n",
        "features_dfx = features_dfx.drop(['median_house_value'], axis = 1)  # We must drop this feature because it is our target \n",
        "\n",
        "# Choose additional features to add to the model (if you want to use the feature, place a '#' next to it)\n",
        "features_dfx = features_dfx.drop([#'longitude',\n",
        "                                  #'latitude',\n",
        "                                  #'housing_median_age',\n",
        "                                  #'total_rooms',\n",
        "                                  #'total_bedrooms',\n",
        "                                  #'median_income',\n",
        "                                  #'INLAND',\n",
        "                                  'NEAR BAY',\n",
        "                                  'NEAR OCEAN'\n",
        "                                  ],\n",
        "                                 axis = 1\n",
        "                                 )\n",
        "\n",
        "# ---------------------------- SPLIT THE DATASET ----------------------------\n",
        "# We extract the values of our two datasets into \"X\" and \"y\" variables for use in later functions\n",
        "# The X variables are the \"explanatory\" numbers (or images). They are the numbers that we use to try and predict\n",
        "#   what the \"y\" variable is.\n",
        "\n",
        "# \"X\" are the independent variables, the \"featues\" that we will use to try and predict \"y\"\n",
        "X = features_dfx.values\n",
        "\n",
        "# \"y\" is the dependent variable (the \"label\"); the actual value that we are trying to learn to predict. \n",
        "#   Each time we make a prediction, it is compared to the actual values, and the \"loss\" (the difference between\n",
        "#   the prediction and real number) is added to previous \"loss\". \n",
        "y = dfx['median_house_value'].values\n",
        "\n",
        "# Define your model(s)\n",
        "# There are multiple hyperparameters that we can set (change them to see effects)\n",
        "modela = linear_model.LinearRegression(n_jobs = -1, \n",
        "                                       fit_intercept=True, \n",
        "                                       normalize=True\n",
        "                                       )\n",
        "\n",
        "modelb = linear_model.Ridge(alpha = 0.05)  # The \"alpha\" parameter is related to normalization by reducing the effect of \n",
        "\n",
        "modelc = linear_model.LassoCV(fit_intercept = True,\n",
        "                              max_iter=1000,\n",
        "                              cv = 10,\n",
        "                              n_jobs=-1,\n",
        "                              random_state=1,\n",
        "                              #selection = 'random',  \n",
        "                              )\n",
        "\n",
        "modeld = linear_model.HuberRegressor(epsilon=1.7,\n",
        "                                     alpha = 0.01,\n",
        "                                     fit_intercept=True\n",
        "                                     )\n",
        "\n",
        "# Show a heatmap for reference\n",
        "make_heatmap(dfx)\n",
        "\n",
        "# Train and compare metrics on the models\n",
        "test_size = 0.33\n",
        "run_linear_model(modela, X, y, test_size = test_size)\n",
        "run_linear_model(modelb, X, y, test_size = test_size)\n",
        "run_linear_model(modelc, X, y, test_size = test_size)\n",
        "run_linear_model(modeld, X, y, test_size = test_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
