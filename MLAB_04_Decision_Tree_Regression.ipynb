{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-04-Decision-Tree-Regression.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_04_Decision_Tree_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAkWiEDA5QUv",
        "colab_type": "text"
      },
      "source": [
        "# Build a Decision Tree Model (Regression)\n",
        "\n",
        "> Websites Referenced:\n",
        "- [Scikit-Learn Documentation: Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
        "- [Excellent explanation of Decision Tree Regression with sample code](https://gdcoder.com/decision-tree-regressor-explained-in-depth/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9VVGiXJ7TWW",
        "colab_type": "text"
      },
      "source": [
        "## A. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEy-VYZn7lpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "import missingno as msno\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import ensemble\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import learning_curve,GridSearchCV\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn import preprocessing\n",
        "from collections import Counter\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n",
        "df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n",
        "df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n",
        "df.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "\n",
        "# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n",
        "df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# ------------------------------------- FIX MISSING DATA -------------------------------------\n",
        "tb_med = df['total_bedrooms'].median(axis=0)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n",
        "df.name = 'df'\n",
        "\n",
        "# ------------------------------------- Z-SCORE -------------------------------------\n",
        "z = np.abs(stats.zscore(df))\n",
        "dfz = df[(z < 3).all(axis = 1)]\n",
        "dfz.name = 'dfz'\n",
        "\n",
        "# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n",
        "q1 = df.quantile(0.25)\n",
        "q3 = df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n",
        "dfi.name = 'dfi'\n",
        "#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(df = 'df'):\n",
        "  corr = df.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def make_heading(heading = 'heading'):\n",
        "  print('{0}:\\n'.format(heading), '-' * 30)\n",
        "\n",
        "\n",
        "def show_coef(df_in, model):\n",
        "  coef_df = pd.DataFrame(model.coef_, df_in.columns, columns=['Coefficient'])\n",
        "  make_heading('\\n\\nCoefficients')\n",
        "  display(coef_df)\n",
        "  make_heading('\\n\\nIntercept')\n",
        "  display(model.intercept_)\n",
        "\n",
        "\n",
        "def drop_features(df_in):\n",
        "  df_in = df_in.drop([#'median_house_value',\n",
        "                      'longitude',\n",
        "                      'latitude',\n",
        "                      #'housing_median_age',\n",
        "                      'total_rooms',\n",
        "                      'total_bedrooms',\n",
        "                      #'median_income',\n",
        "                      'INLAND',\n",
        "                      #'NEAR BAY',\n",
        "                      #'NEAR OCEAN'\n",
        "                      ],\n",
        "                      axis = 1\n",
        "                      )\n",
        "  return features_dfx\n",
        "\n",
        "\n",
        "def plot_test_predictions(y_test, y_pred):\n",
        "  make_heading('Prediction Performance')  # Make a heading to separate output\n",
        "  fig, ax = plt.subplots()\n",
        "  ax = plt.subplot()\n",
        "  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  ax.set_title(\"Ground Truth vs Predicted\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def plot_feature_importance(feature_importance):\n",
        "  # Make importances relative to max importance\n",
        "  feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "  sorted_idx = np.argsort(feature_importance)\n",
        "  pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "  make_heading('Feature Importance')  # Make a heading to separate output\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "  plt.yticks(pos, features_dfx)\n",
        "  plt.xlabel('Relative Importance')\n",
        "  plt.title('Variable Importance')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "\n",
        "def show_metrics(X_train, X_test, y_train, y_test, y_pred):\n",
        "  # Display the shape of each array:\n",
        "  #make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "  #print('X_train shape: ', X_train.shape)\n",
        "  #print('X_test shape:  ', X_test.shape)\n",
        "  #print('y_train shape: ', y_train.shape)\n",
        "  #print('y_test shape:  ', y_test.shape)\n",
        "  #print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "  # Display training / testing set metrics\n",
        "  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "  print(\"Training Accuracy (score)                 (X_train, y_train):  {:.2f}\".format(model_train_score))\n",
        "  print(\"Test Accuracy (score)                     (X_test, y_test):    {:.2f}\".format(model_test_score))\n",
        "  print(\"Predictive Accuracy (R^2 score)           (y_test, y_pred):    {:.2f}\".format(predictive_accuracy))\n",
        "  print(\"Explained Variance (1 is best) (loss)     (y_test, y_pred):    {:.2f}\".format(ev))\n",
        "  print(\"Mean Absolute Error on Test Set (loss)    (y_test, y_pred):    {:.2f}\".format(mean_ae))\n",
        "  print(\"Median Absolute Error on Test Set (loss)  (y_test, y_pred):    {:.2f}\".format(median_ae))\n",
        "  print(\"RMSE on Test Set (loss)                   (y_test, y_pred):    {:.2f}\".format(rmse))\n",
        "  print(\"Model Depth                            (model.get_depth()):    {:d}\".format(model_depth))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NoTOrvD-rq",
        "colab_type": "text"
      },
      "source": [
        "## B. Choose a dataset to use\n",
        "- Three datasets have been created:\n",
        "  1. **df** -- the **original dataset** ('NaN' values have been replaced with median, so there are no null values)\n",
        "  2. **dfz** -- a dataset that has used the **z-score method** for removing outlier data\n",
        "  3. **dfi** -- a dataset that has used the **IQR method** for removing outlier data\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "**NOTE:** When choosing your dataset, you will need to change two commands to reflect the dataset that you want to use. In the following example, the \n",
        "\n",
        "**Original code:** --------------------------------------->  **New code** (For example, to change **from** 'dfz' **to** 'dfi' dataset):\n",
        "```\n",
        "dfx = dfz.copy()      -->   dfx = dfi.copy()\n",
        "dfx.name = dfz.name   -->   dfx.name = dfi.name\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mCB2AeT7cPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *************************** DETERMINE DATA FOR USE ***************************\n",
        "# Choose which dataframe you would like to use (dfi: IQR, dfz: z-score, df: original (with replaced NaN values))\n",
        "dfx = df.copy()\n",
        "dfx.name = df.name\n",
        "\n",
        "# Show a heatmap for the given dataframe you want to use\n",
        "make_heatmap(dfx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77jpQAgEHoDf",
        "colab_type": "text"
      },
      "source": [
        "## C. Step by Step: Building a Decision Tree Model\n",
        "\n",
        "- In this section, we walk through building a **Decision Tree Regression model** using the dataframe chosen in the previous step.\n",
        "\n",
        "### The basic steps are:\n",
        "  1. Define the **features** and the **target variable**.\n",
        "  2. **Split** the dataset into **training** and **testing** partitions\n",
        "  3. **Define the model:**\n",
        "    - **Select Hyperparameters**\n",
        "    - **Train the model** - run the model against our training set\n",
        "    - Use the trained model to **predict against the test set**\n",
        "  4. **Capture metrics** for the model (done during training, testing and prediction)\n",
        "    - Training accuracy\n",
        "    - Testing accuracy\n",
        "    - Prediction error\n",
        "    - Error values (Mean Absolute Error, Median Absolute Error, RMSE, etc.)\n",
        "  5. **Analyze metrics**\n",
        "    - Look at the varirous accuracy and error scores to **determine how well the model fits** (overfit, underfit, good, bad, etc.)\n",
        "  6. **Plot graphs** to visualize the various accuracy / error metrics\n",
        "  7. **Predict new values** to see if the model performs as expected (given the accuracy and coefficient values)\n",
        "\n",
        "---\n",
        "### Setting Hyperparameters\n",
        "Please see this link for more information: [Scikit-Learn Documentation: Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
        "- **```criterion```** : string, optional (default=”mse”)\n",
        "- The function to measure the quality of a split. Supported criteria are:\n",
        "  - ***mse*** for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node\n",
        "  - ***friedman_mse***, which uses mean squared error with Friedman’s improvement score for potential splits\n",
        "  - ***mae*** for the mean absolute error, which minimizes the L1 loss using the median of each terminal node.\n",
        "- **```min_samples_leaf```** : int, float, optional (default=1)\n",
        "  - It is the minimum number of samples for a terminal node.\n",
        "  - In other words, a final node must have this many samples in order to be established. Any less, and it will not be created.\n",
        "\n",
        "- **```min_samples_split```** : int, float, optional (default=2)\n",
        "  - It is the minimum samples for a node split that we discuss above.\n",
        "  - In other words, a node cannot be split unless it has at least this many samples. Any less, and it will become a terminal node.\n",
        "\n",
        "- **```max_features```** : int, float, string or None, optional (default=”auto”)\n",
        "  - The number of features to consider when looking for the best split\n",
        "  - In other words, this determines the number (precisely defined or given as a percentage) of features that the model will contemplate when deriving the best means by which to split a node.\n",
        "\n",
        "- **```max_depth```** : integer or None, optional (default=None)\n",
        "  - The maximum depth of the tree. \n",
        "  - In other words, what is the maximum \"depth\" that the tree should grow?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw2dwazM5ey2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> STEP BY STEP DECISION TREE CONSTRUCTION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# ---------------------------- Identify Target Variable ----------------------------\n",
        "# Create a copy of the dataframe with only the desired features (dropping the target feature)\n",
        "features_dfx = dfx.drop(['median_house_value'], axis = 1)\n",
        "\n",
        "# Determine additional features to keep/drop (# means that we want to keep that variable)\n",
        "features_dfx = features_dfx.drop([#'longitude',\n",
        "                                  #'latitude',\n",
        "                                  #'housing_median_age',\n",
        "                                  #'total_rooms',\n",
        "                                  #'total_bedrooms',\n",
        "                                  #'median_income',\n",
        "                                  #'INLAND',\n",
        "                                  'NEAR BAY',\n",
        "                                  'NEAR OCEAN'\n",
        "                                  ],\n",
        "                                 axis = 1\n",
        "                                 )\n",
        "\n",
        "\n",
        "errvals = np.array([])              # Create an array to hold our error values during training (used for plotting results)\n",
        "rs = 20                             # Set our \"Random State\" variable (ensuring that we always start at the same point in our dataset)\n",
        "\n",
        "\n",
        "# ---------------------------- SPLIT THE DATASET ----------------------------\n",
        "# We extract the values of our two datasets into \"X\" and \"y\" variables for use in later functions\n",
        "# The X variables are the \"explanatory\" numbers (or images). They are the numbers that we use to try and predict\n",
        "#   what the \"y\" variable is.\n",
        "\n",
        "# \"X\" are the independent variables, the \"featues\" that we will use to try and predict \"y\"\n",
        "X = features_dfx.values\n",
        "\n",
        "# \"y\" is the dependent variable (the \"label\"); the actual value that we are trying to learn to predict. \n",
        "#   Each time we make a prediction, it is compared to the actual values, and the \"loss\" (the difference between\n",
        "#   the prediction and real number) is added to previous \"loss\". \n",
        "y = dfx['median_house_value'].values\n",
        "\n",
        "# Split the dataset into training and testing arrays\n",
        "#   We take our X and y variables that we have just created and now split each one into a training set and testing\n",
        "#   set: X_train, X_test, y_train, y_test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.3,    # reserve this percentage of our data for testing, use the rest for training\n",
        "                                                    shuffle = True,\n",
        "                                                    random_state = rs\n",
        "                                                    )\n",
        "\n",
        "\n",
        "# ---------------------------- CREATE THE MODEL  ----------------------------\n",
        "# Define the desired algorithm and congigure the hyperparameters\n",
        "\"\"\"model = DecisionTreeRegressor(criterion='mae',  # can also be 'mse' or 'friedman_mse'\n",
        "                              max_depth = 500, \n",
        "                              min_samples_split = 4,\n",
        "                              min_samples_leaf = 4,\n",
        "                              max_features = 0.6,\n",
        "                              random_state = rs, \n",
        "                              )\n",
        "\"\"\"\n",
        "\n",
        "model = DecisionTreeRegressor(criterion='friedman_mse',  # can also be 'mse' or other values\n",
        "                              max_depth = None, \n",
        "                              min_samples_split = 64,\n",
        "                              min_samples_leaf = 32,\n",
        "                              max_features = 0.9,\n",
        "                              random_state = rs, \n",
        "                              )\n",
        "\n",
        "model_name = type(model).__name__   # Get the name of the model (for use in our display functions)\n",
        "\n",
        "\n",
        "# ---------------------------- Fit / TRAIN THE MODEL ----------------------------\n",
        "# Train the model using our training sets (X_train, y_train)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Here we want to input our X_test array to make predictions based upon our trained model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ---------------------------- GATHER METRICS ----------------------------\n",
        "model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "mean_ae = metrics.mean_absolute_error(y_test, y_pred)  # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "median_ae = metrics.median_absolute_error(y_test, y_pred)  # Median absolute error\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "ev = metrics.explained_variance_score(y_test, y_pred) # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "predictive_accuracy = metrics.r2_score(y_test, y_pred)  # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "model_depth = model.get_depth()\n",
        "\n",
        "\n",
        "# ---------------------------- ANALYSIS / VISUALIZATION ----------------------------\n",
        "show_metrics(X_train, X_test, y_train, y_test, y_pred)  # Dispay the scores and loss for the model\n",
        "plot_test_predictions(y_test, y_pred)                   # Create a scatterplot of real values against predicted ones for the test set\n",
        "feature_importance = model.feature_importances_         # Gather feature importance values\n",
        "plot_feature_importance(feature_importance)             # Plot feature importance\n",
        "joblib.dump(model, 'ca_housing_dt_model.pkl')           # Save the model so that we can use it later\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
