{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-06-Grid-Search.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_06_Grid_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdmXP7DVt2k",
        "colab_type": "text"
      },
      "source": [
        "# Build a Grid Search Model (w/ Cross-Validation)\n",
        "In the previous section (Build a Gradient Boosting Model), we learned how to build multiple decision trees that work sequentially to create a more effective model.\n",
        "\n",
        "One of the **problems** associated with this method, however, is the sheer **number of interconnected hyperparameters**. Because each of the hyperparameters play off of one another, **determining the optimal settings can prove difficult and time-consuming**.\n",
        "\n",
        "There are tools to help with finding optimal settings in a Gradient Boosting model, however. Using **GridSearchCV**, one can apply **multiple hyperparameter settings at once** to a Gradient Boosting model and methodically process all possible combinations automatically. At the end of the run, GridSearchCV will produce a set of **optimal hyperparameters for a Gradient Boosting model** (given the possible inputs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OygC_2iuXksz",
        "colab_type": "text"
      },
      "source": [
        "## A. Setup the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T6HWYyMVltU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import ensemble\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import learning_curve,GridSearchCV\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n",
        "df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n",
        "df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n",
        "df.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "\n",
        "# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n",
        "df.drop(['proximity_to_store'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# ------------------------------------- FIX MISSING DATA -------------------------------------\n",
        "tb_med = df['total_bedrooms'].median(axis=0)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n",
        "df.name = 'df'\n",
        "\n",
        "# ------------------------------------- Z-SCORE -------------------------------------\n",
        "z = np.abs(stats.zscore(df))\n",
        "dfz = df[(z < 3).all(axis = 1)]\n",
        "dfz.name = 'dfz'\n",
        "\n",
        "# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n",
        "q1 = df.quantile(0.25)\n",
        "q3 = df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n",
        "dfi.name = 'dfi'\n",
        "#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(df = 'df'):\n",
        "  corr = df.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def make_heading(heading = 'heading'):\n",
        "  print('\\n\\n{0}:\\n'.format(heading), '-' * 30)\n",
        "\n",
        "\n",
        "def drop_features(df_in):\n",
        "  df_in = df_in.drop([#'median_house_value',\n",
        "                      'population',\n",
        "                      'households',\n",
        "                      'longitude',\n",
        "                      'latitude',\n",
        "                      #'housing_median_age',\n",
        "                      'total_rooms',\n",
        "                      'total_bedrooms',\n",
        "                      #'median_income',\n",
        "                      'INLAND',\n",
        "                      'ISLAND',\n",
        "                      #'NEAR BAY',\n",
        "                      #'NEAR OCEAN'\n",
        "                      ],\n",
        "                      axis = 1\n",
        "                      )\n",
        "  return features_dfx\n",
        "\n",
        "\n",
        "def plot_test_predictions(y_test, y_pred):\n",
        "  make_heading('Prediction Performance')  # Make a heading to separate output\n",
        "  fig, ax = plt.subplots()\n",
        "  ax = plt.subplot()\n",
        "  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  ax.set_title(\"Ground Truth vs Predicted\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def plot_feature_importance(feature_importance):\n",
        "  # Make importances relative to max importance\n",
        "  feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "  sorted_idx = np.argsort(feature_importance)\n",
        "  pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "  make_heading('Feature Importance')  # Make a heading to separate output\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "  plt.yticks(pos, features_dfx)\n",
        "  plt.xlabel('Relative Importance')\n",
        "  plt.title('Variable Importance')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def plot_deviance():\n",
        "  # Plot the training/testing accuracy\n",
        "  n_est = gs_cv.best_estimator_.n_estimators_\n",
        "  y_pred = gs_cv.best_estimator_.predict(X_test)\n",
        "  y_staged_predicted_score = gs_cv.best_estimator_.staged_predict(X_test)\n",
        "  model_test_score = gs_cv.best_estimator_.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "  model_train_score = gs_cv.best_estimator_.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "  \n",
        "\n",
        "  # Create an empty array of zeros based on the value in 'n_estimators' key\n",
        "  test_score = np.zeros(shape = (n_est,),\n",
        "                        dtype=np.float64\n",
        "                        )\n",
        "\n",
        "  # Compute test set deviance (loss) at each stage and place it into the array\n",
        "  #  based on the predicted value (y_pred) against the actual value (y_test)\n",
        "  for i, y_pred in enumerate(y_staged_predicted_score):\n",
        "      test_score[i] = gs_cv.best_estimator_.loss_(y_test, y_pred)\n",
        "  \n",
        "  make_heading('Deviance Over Time')  # Make a heading to separate output\n",
        "  plt.figure(figsize=(15, 7))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('Deviance')\n",
        "  plt.plot(np.arange(n_est) + 1, \n",
        "           gs_cv.best_estimator_.train_score_,\n",
        "           'b-',\n",
        "           label='Training Set Deviance'\n",
        "           )\n",
        "  plt.plot(np.arange(n_est) + 1,\n",
        "           test_score,\n",
        "           'r-',\n",
        "           label='Test Set Deviance'\n",
        "           )\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('Boosting Iterations')\n",
        "  plt.ylabel('Deviance')\n",
        "  plt.show()\n",
        "  plt.close\n",
        "  \n",
        "\n",
        "def show_metrics(X_train, X_test, y_train, y_test, y_pred, cv_best_score_mean):\n",
        "  # Display the shape of each array:\n",
        "  #make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "  #print('X_train shape: ', X_train.shape)\n",
        "  #print('X_test shape:  ', X_test.shape)\n",
        "  #print('y_train shape: ', y_train.shape)\n",
        "  #print('y_test shape:  ', y_test.shape)\n",
        "  #print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "  # Display training / testing set metrics\n",
        "  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "  print(\"Training Accuracy (score)                 (X_train, y_train):  {:.2f}\".format(model_train_score))\n",
        "  print(\"Test Accuracy (score)                     (X_test, y_test):    {:.2f}\".format(model_test_score))\n",
        "  print(\"Predictive Accuracy (R^2 score)           (y_test, y_pred):    {:.2f}\".format(predictive_accuracy))\n",
        "  print(\"Explained Variance (1 is best) (loss)     (y_test, y_pred):    {:.2f}\".format(ev))\n",
        "  print(\"Mean Absolute Error on Test Set (loss)    (y_test, y_pred):    {:.2f}\".format(mean_ae))\n",
        "  print(\"Median Absolute Error on Test Set (loss)  (y_test, y_pred):    {:.2f}\".format(median_ae))\n",
        "  print(\"RMSE on Test Set (loss)                   (y_test, y_pred):    {:.2f}\".format(rmse))\n",
        "  print(\"Best Cross-Validated Score (mean)         gs_cv.best_score_:   {:.2f}\".format(cv_best_score_mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NoTOrvD-rq",
        "colab_type": "text"
      },
      "source": [
        "## B. Choose a dataset to use\n",
        "- Three datasets have been created:\n",
        "  1. **df** -- the **original dataset** ('NaN' values have been replaced with median, so there are no null values)\n",
        "  2. **dfz** -- a dataset that has used the **z-score method** for removing outlier data\n",
        "  3. **dfi** -- a dataset that has used the **IQR method** for removing outlier data\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "**NOTE:** When choosing your dataset, you will need to change two commands to reflect the dataset that you want to use. In the following example, the \n",
        "\n",
        "**Original code:** --------------------------------------->  **New code** (For example, to change **from** 'dfz' **to** 'dfi' dataset):\n",
        "```\n",
        "dfx = dfz.copy()      -->   dfx = dfi.copy()\n",
        "dfx.name = dfz.name   -->   dfx.name = dfi.name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  \n",
        "  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxObYDCISc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *************************** DETERMINE DATA FOR USE ***************************\n",
        "# Choose which dataframe you would like to use (dfi: IQR, dfz: z-score, df: original (with replaced NaN values))\n",
        "dfx = df.copy()\n",
        "dfx.name = df.name\n",
        "\n",
        "# Show a heatmap for the given dataframe you want to use\n",
        "make_heatmap(dfx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9fD53_uT_4",
        "colab_type": "text"
      },
      "source": [
        "## C. Step by Step: Building a Gradient Boosting Model\n",
        "\n",
        "### From *Machine Learning For Absolute Beginners* by Oliver Theobald\n",
        "```\n",
        "\n",
        "model = ensemble.GradientBoostingRegressor(\n",
        "                                           n_estimators = 150, \n",
        "                                           learning_rate = 0.1, \n",
        "                                           max_depth = 30, \n",
        "                                           min_samples_split = 4, \n",
        "                                           min_samples_leaf = 6, \n",
        "                                           max_features = 0.6, \n",
        "                                           loss = 'huber'\n",
        "                                          )\n",
        "```\n",
        "\n",
        "The first line is the algorithm itself (gradient boosting) and comprises just one line of code. The code below dictates the hyperparameters for this algorithm. \n",
        "- **n_estimators** represents how many decision trees to be used. Remember that a high number of trees generally improves accuracy (up to a certain point) but will extend the model’s processing time. Above, I have selected 150 decision trees as an initial starting point. \n",
        "- **learning_rate** controls the rate at which additional decision trees influence the overall prediction. This effectively shrinks the contribution of each tree by the set learning_rate. Inserting a low rate here, such as 0.1, should help to improve accuracy. \n",
        "- **max_depth** defines the maximum number of layers (depth) for each decision tree. If “None” is selected, then nodes expand until all leaves are pure or until all leaves contain less than min_samples_leaf. Here, I have chosen a high maximum number of layers (30), which will have a dramatic effect on the final result, as we’ll soon see. \n",
        "- [**min_samples_split**](https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre) defines the minimum number of samples required to execute a new binary split. For example, min_samples_split = 10 means there must be ten available samples in order to create a new branch.\n",
        "- [**min_samples_leaf**](**min_samples_split**) represents the minimum number of samples that must appear in each child node (leaf) before a new branch can be implemented. This helps to mitigate the impact of outliers and anomalies in the form of a low number of samples found in one leaf as a result of a binary split. For example, min_samples_leaf = 4 requires there to be at least four available samples within each leaf for a new branch to be created. \n",
        "- **max_features** is the total number of features presented to the model when determining the best split.\n",
        "- **loss** calculates the model's error rate. For this exercise, we are using huber which protects against outliers and anomalies. Alternative error rate options include ls (least squares regression), lad (least absolute deviations), and quantile (quantile regression). Huber is actually a combination of least squares regression and least absolute deviations.\n",
        "\n",
        "\n",
        "  >Theobald, Oliver. Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition) (Machine Learning For Beginners Book 1) (pp. 139-141). Scatterplot Press. Kindle Edition.\n",
        "\n",
        "> Websites Referenced:\n",
        "- [Scikit-Learn Documentation: Ensemble Gradient Boosting Visualization](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py)\n",
        "- [Sharp Sight Labs: Numpy Zeros Tutorial](https://www.sharpsightlabs.com/blog/numpy-zeros-python/)\n",
        "- [Towards Data Science: Beginner's Guide to Cross-Validation](https://towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd)\n",
        "- [Scikit-Learn Documentation: Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
        "- [Scikit-Learn Documentation: Tuning the hyper-parameters of an estimator (Grid Search)](https://scikit-learn.org/stable/modules/grid_search.html)\n",
        "- [GDCoder.com: Decision Tree Regressor explained in depth](https://gdcoder.com/decision-tree-regressor-explained-in-depth/)\n",
        "- [link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl_TUtTRhP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> STEP BY STEP DECISION TREE CONSTRUCTION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# ---------------------------- Identify Target Variable ----------------------------\n",
        "# Create a copy of the dataframe with only the desired features (dropping the target feature)\n",
        "features_dfx = dfx.copy()\n",
        "features_dfx = dfx.drop(['median_house_value'], axis = 1)  # We *MUST* drop this b/c it is the target\n",
        "\n",
        "# Determine additional features to keep/drop (# means that we want to keep that variable)\n",
        "features_dfx = features_dfx.drop([#'longitude',\n",
        "                                  #'latitude',\n",
        "                                  #'housing_median_age',\n",
        "                                  #'total_rooms',\n",
        "                                  #'total_bedrooms',\n",
        "                                  #'population',\n",
        "                                  #'households',\n",
        "                                  #'median_income',\n",
        "                                  #'median_house_value',\n",
        "                                  #'INLAND',\n",
        "                                  'ISLAND',\n",
        "                                  'NEAR BAY',\n",
        "                                  'NEAR OCEAN'\n",
        "                                  ],\n",
        "                                 axis = 1\n",
        "                                 )\n",
        "\n",
        "\n",
        "# ---------------------------- SPLIT THE DATASET ----------------------------\n",
        "X = features_dfx.values                 # Define the independent variable values to be used\n",
        "y = dfx['median_house_value'].values    # Define the dependent variable values to be used\n",
        "rs = 20                                 # Define the random state variable (ensuring continuity between runs)\n",
        "test_size = 0.2                         # Define the percentage of data to set aside for testing (usually b/w 0.2 - 0.3)\n",
        "\n",
        "# Split the dataset into training and testing arrays\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = test_size,\n",
        "                                                    shuffle = True,\n",
        "                                                    random_state = rs\n",
        "                                                    )\n",
        "\n",
        "\n",
        "# ---------------------------- CREATE THE MODEL  ----------------------------\n",
        "# -- Set the hyperparameters that we want to use in the model\n",
        "\"\"\"\n",
        "param_grid = {'n_estimators': [50, 100, 200],\n",
        "              'max_depth': [3, 5],\n",
        "              'min_samples_split': [2, 5, 8],\n",
        "              'min_samples_leaf': [2, 4],\n",
        "              'learning_rate': [0.1, 1.0],\n",
        "              'max_features': [0.6, 0.8],\n",
        "              'loss': ['huber']\n",
        "              }\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "param_grid = {'n_estimators': [400],\n",
        "              'max_depth': [7],\n",
        "              'min_samples_split': [72],\n",
        "              'min_samples_leaf': [20],\n",
        "              'learning_rate': [0.1],\n",
        "              'max_features': [0.8],\n",
        "              'loss': ['huber']\n",
        "              }\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "param_grid = {'n_estimators': [500],\n",
        "              'max_depth': [5],\n",
        "              'min_samples_split': [16],\n",
        "              'min_samples_leaf': [10],\n",
        "              'learning_rate': [0.1],\n",
        "              'max_features': [0.8],\n",
        "              'loss': ['huber']\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "# Define the desired algorithm and congigure the hyperparameters (supplied with '**params' arguement)\n",
        "base_model = ensemble.GradientBoostingRegressor()\n",
        "base_model_name = type(base_model).__name__                 # Get the name of the model (for use in our display functions)\n",
        "\n",
        "\n",
        "# ---------------------------- CREATE THE GRID SEARCH MODEL ----------------------------\n",
        "gs_cv = GridSearchCV(estimator = base_model,\n",
        "                     param_grid = param_grid,\n",
        "                     n_jobs = -1, \n",
        "                     cv = 3,\n",
        "                     return_train_score = True,\n",
        "                     verbose = 10,\n",
        "                     refit = True\n",
        "                     )\n",
        "\n",
        "\n",
        "# ---------------------------- FIT / TRAIN THE MODEL ----------------------------\n",
        "gs_cv.fit(X_train, y_train)                                 # Train the model using our training sets (X_train, y_train)\n",
        "y_pred = gs_cv.predict(X_test)                              # Make predictions based upon our trained model\n",
        "\n",
        "# ---------------------------- GATHER METRICS ----------------------------\n",
        "\n",
        "model_name = type(gs_cv).__name__                           # Get the name of our model (for use in displaying stats)\n",
        "model_test_score = gs_cv.score(X_test, y_test)              # Accuracy of our model on test data\n",
        "model_train_score = gs_cv.score(X_train, y_train)           # Accuracy of our model on training data\n",
        "mean_ae = metrics.mean_absolute_error(y_test, y_pred)       # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "median_ae = metrics.median_absolute_error(y_test, y_pred)   # Median absolute error\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "ev = metrics.explained_variance_score(y_test, y_pred)       # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "predictive_accuracy = metrics.r2_score(y_test, y_pred)      # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "cv_best_score_mean = gs_cv.best_score_                      # Get the mean cross-validated score of the best_esimator\n",
        "mae_train = metrics.mean_absolute_error(y_train, gs_cv.predict(X_train))  \n",
        "mae_test = metrics.mean_absolute_error(y_test, gs_cv.predict(X_test))  \n",
        "\n",
        "\n",
        "# ---------------------------- ANALYSIS / VISUALIZATION ----------------------------\n",
        "make_heading('Testing and Training Mean Absolute Error Data')\n",
        "print (\"Training Set Mean Absolute Error: %.2f\" % mae_train)  \n",
        "print (\"Test Set Mean Absolute Error: %.2f\" % mae_test)\n",
        "make_heading('Hyperparameters Explored in GridSearch')\n",
        "print(pd.Series(param_grid))\n",
        "make_heading('Best Hyperparameters')\n",
        "print(pd.Series(gs_cv.best_params_))\n",
        "show_metrics(X_train, X_test, y_train, y_test, y_pred, cv_best_score_mean)  # Dispay the scores and loss for the model\n",
        "plot_test_predictions(y_test, y_pred)                       # Create a scatterplot of real values against predicted ones for the test set\n",
        "feature_importance = gs_cv.best_estimator_.feature_importances_  # Gather feature importance values\n",
        "plot_feature_importance(feature_importance)                 # Plot feature importance\n",
        "plot_deviance()                                             # Plot training / testing accuracy\n",
        "\n",
        "# Save the trained model for future use\n",
        "joblib.dump(gs_cv.best_estimator_, 'ca_housing_gb_model.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
