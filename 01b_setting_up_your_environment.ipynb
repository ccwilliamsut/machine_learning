{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_for_Absolute_Beginners.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "KfxYhyyfaXwe",
        "hlqD0fOKvvJ3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/01b_setting_up_your_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V8ehJwDONYQc"
      },
      "source": [
        "# Build a Decision Tree Model with Python\n",
        "\n",
        "In this exercise, we will learn to build a simple model that uses a Decision Tree algorithm (with gradient boosting) to determine housing prices in the California market.\n",
        "\n",
        "## Basic Steps\n",
        "- **Step 1: Setup your environment**\n",
        "    - Import the appropraite libraries and setup software, if necessary (we will be using Google Colaboratory, so we do not need to setup any additional software)\n",
        "    - Import your data\n",
        "- **Step 2: Analyze your data**\n",
        "    - Become familiar with the data\n",
        "    - Look for possible relationships\n",
        "    - Determine which features should be used and which need to be removed\n",
        "- **Step 3: Scrub the data**\n",
        "    - Remove unnecessary features\n",
        "    - Perform *one-hot encoding* (convert text values to numerical values)\n",
        "    - Fix problematic data (outliers, null values, duplicate data, etc.)\n",
        "- **Step 4: Choose an algorithm**\n",
        "    - Determine which type of algorithm fits the data:\n",
        "        - **Supervised** - uses labeled data to predict values or classes (predictive model; regression and classification)\n",
        "        - **Unsupervised** - uses unlabeled data to create clusters of data (descriptive model; clustering)\n",
        "        - **Reinforcement Learning** - Creates a system of rewards and punishments to interactively learn from the environment (online model; classification and control)\n",
        "    - Determine which algorithm to use\n",
        "        - Linear regression, logistic regression, k-means clustering, k-nearest neighbor, etc.\n",
        "    - Implement the algorithm\n",
        "        - Build it out with estimated guesses for the hyperparameters (where applicable)\n",
        "- **Step 5: Test and fit the model**\n",
        "    - Run the model and analyze performance\n",
        "    - Look for overfitting (too honed on on training data)\n",
        "    - Look for underfitting (does not predict well on test data)\n",
        "    - Continue adjusting until a good fit is found (run, tweak, run again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfxYhyyfaXwe",
        "colab_type": "text"
      },
      "source": [
        "# Step 1. Setup your environment (importing libraries and data)\n",
        "\n",
        "For this project, we will draw on the following libraries (commonly used for machine learning projects):\n",
        "\n",
        "- pandas (for working easily with datasets)\n",
        "- sklearn (scikit-learn contains a number of useful machine learning tools)\n",
        "- IPython (for displaying results in Colaboratory)\n",
        "- seaborn (for displaying graphs)\n",
        "\n",
        "### The original dataset comes from Kaggle\n",
        "- [California Housing Data](https://www.kaggle.com/camnugent/california-housing-prices)\n",
        "\n",
        "### My modified dataset for this course is in my Github repository\n",
        "- [ccwilliamsut Github](https://github.com/ccwilliamsut/ml_beginners/blob/master/CaliforniaHousingData.csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6By4K_XtNw-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import ensemble \n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Import the data\n",
        "# Set the URL for the data file\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/ml_beginners/master/CaliforniaHousingData.csv'\n",
        "\n",
        "# Import the datafile from the provided url\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq3WxwHps7nF",
        "colab_type": "text"
      },
      "source": [
        "### Verify that the dataset imported correctly\n",
        "Quickly look to see that our data was imported and will be usable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js9h_N8gB4sA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test that the data was imported correctly (look at the first 10 rows of the file)\n",
        "display(df.head())\n",
        "\n",
        "# List the column headers to see what we are working with\n",
        "print(\"\\n\\nHere are all of our columns:\")\n",
        "display(df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpp0DDdWt3F4",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Analyze the data\n",
        "- Become familiar with the data\n",
        "- Look for possible relationships\n",
        "- Determine which features should be used and which need to be removed\n",
        "\n",
        "\n",
        "\n",
        "We want to get an idea of what we are working with (i.e. the \"shape\" of the data) so that we can begin to see what work is required to prepare the data for training. We also want to begin thinking about what kind of algorithm we will be using (supervised or unsupervised, typically).\n",
        "\n",
        "> **Key questions to consider at this point:**\n",
        "1. Does my data have **labels**? (supervised or unsupervised algorithms)\n",
        "2. Are there major **issues** with the data (lots of null values, small number of samples, misspellings, derived data, unknown scales or values, etc.)?\n",
        "3. What is my **goal**? Will I be able to accomplish that goal with this dataset?\n",
        "    - If not, can I employ feature engineering to create the data I need?\n",
        "    - If so, which features will contribute to that goal and which are unnecessary?\n",
        "4. Can I see any **relationships** in the data that might serve as a good foundation for my model?\n",
        "\n",
        "\n",
        "Let's begin by looking at a summary of the dataset using the **```dataframe.describe()```** and **```list(dataset.columns)```** functions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvoeT8tzMh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze the \"shape\" of the data\n",
        "display(df.describe())\n",
        "\n",
        "# List all the columns in the dataset\n",
        "list(df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuoKogADhnmD",
        "colab_type": "text"
      },
      "source": [
        "### Analyze at the data above\n",
        "- Notice that all columns were not shown in the **```dataset.describe()```** function... Why not?\n",
        "- Do you see any immediate problems?\n",
        "- Is our data labeled or unlabeled?\n",
        "- Can you spot any possible relationships that we might explore?\n",
        "- Can I accomplish my goal with this dataset?\n",
        "\n",
        "### Use graphs to explore the data\n",
        "You can also use graphs to look at the distributions of data as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5FqNq4XkYD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List the first 5 items in 'ocean_proximity' to discover why it is not shown in the describe function\n",
        "display(df['ocean_proximity'].head(5))\n",
        "\n",
        "# Use histograms to look at the distribution of the data\n",
        "df.hist('median_house_value')\n",
        "df.hist('housing_median_age')\n",
        "df.hist('t_rooms')\n",
        "df.hist('total_bedrooms')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yhLuFWuD7WK",
        "colab_type": "text"
      },
      "source": [
        "### Problems\n",
        "Looking at the data in the above few cells (including the column list and data description), there are at least a few problems we can see:\n",
        "1. The **``` median_house_value```** has a lot of values stacked up in the $500,000 range\n",
        "2. **```t_rooms```** and **```total_bedrooms```** have very *long tails* which can cause problems with accuracy\n",
        "3. We have **text-based data** in ```ocean_proximity```...  What should we do with this?\n",
        "4. There is a **spelling error**... Do we need to worry about this?\n",
        "5. We have some **missing data** in one of our columns... Can you spot which one?\n",
        "---\n",
        "Take some time to think about the above questions. Discuss them with your group or partner if applicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-lRpBewGSs0",
        "colab_type": "text"
      },
      "source": [
        "### Other types of visualizations\n",
        "The *seaborn* library gives us access to a number of different visualizations. We will cover a few more here, but for further information, see the following links:\n",
        "- [Seaborn official documentation](https://seaborn.pydata.org/)\n",
        "- [Specific info on distributions](https://seaborn.pydata.org/tutorial/distributions.html)\n",
        "\n",
        "Before we can perform more advanced visualizations, however, we must first deal with some of the issues listed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC7Dle91B6rm",
        "colab_type": "text"
      },
      "source": [
        "#**Step 3: Scrub the data**\n",
        "### Typical actions taken at this stage are:\n",
        "\n",
        "1.   Modifying or removing data (incomplete, irrelevant or duplicate)\n",
        "2.   *One-hot encoding* (converting text-based values to numerical values)\n",
        "3.   Binning (using numerical values to categorize data, such as longitude, price range, etc.)\n",
        "4.   Feature engineering (in order to optimize our model)\n",
        "       * feature selection (determining which features will be useful)\n",
        "       * combining features (to save processing time)\n",
        "       * removing unnecessary features\n",
        "       \n",
        "Next we need to methodically look through our dataset for errors, problems and missing values. This is known as \"scrubbing\" the data, and it is a **vital part of any machine learning project**. In fact, this stage represents most of the time that a data scientist spends on a project, as the data usually needs to be cleaned and changed significantly in order to maximize efficiency and create a viable model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBjeHIfb2gLR",
        "colab_type": "text"
      },
      "source": [
        "##Pinpoint problems\n",
        "We need to look at the data in multiple ways to identify all the problems that might exist. \n",
        "\n",
        "###1. Identify null values: \n",
        "- We can get a count of \"NaN\" values in the dataset with ```dataframe.isnull().sum(axis=0)```\n",
        "- Another way to do this is to use the using ```dataframe.count(axis=0)``` to get a count of all non-null values\n",
        "\n",
        "Both methods are shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jff7NOA81sm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a count of \"NaN\" or missing values by feature\n",
        "print('Counting only \"NaN\" values:\\n')\n",
        "display(df.isnull().sum(axis=0))\n",
        "\n",
        "# Count the records in each column\n",
        "print('\\n\\nCounting all non-null values:\\n')\n",
        "display(df.count(axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UUx4mu13KRC",
        "colab_type": "text"
      },
      "source": [
        "We can see in the above output that we are missing 207 records in ```total_bedrooms```. How can we deal with these missing values?\n",
        ">The first questions to ask are: *Do we need this column? Will the information help us?*\n",
        ">If the answer is \"yes\", then we should fix the data. If not, then we can move on. In this case, however, it's likely that ```total_bedrooms``` will be useful for our model, so we will keep it.\n",
        "\n",
        "We have the following choices when dealing with null data:\n",
        "1. Use the **mode** of the column\n",
        "    - Useful for binary and categorical data (yes/no, divorce status, etc.)\n",
        "2. Use the **median** value of the column\n",
        "    - Useful for continuous variables in which there is an infinite number of possibilities\n",
        "3. **Delete** the samples with missing data\n",
        "    - Used as a last resort, as it reduces the data available for training/testing\n",
        "    \n",
        ">Because the value in question here is continuous (though not extremely fine-grained), we will use the mode to fill in the missing data.\n",
        "\n",
        "data['Native Country'].fillna(data['Native Country'].mode()[0], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQqHT066n8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the original data for comparison\n",
        "display(df.describe())\n",
        "\n",
        "# Replace the null values with mode values\n",
        "df_mode = df\n",
        "df_mode['total_bedrooms'].fillna(df_mode['total_bedrooms'].mode()[0], inplace=True)\n",
        "display(df_mode.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlqD0fOKvvJ3",
        "colab_type": "text"
      },
      "source": [
        "## Pairplots using Seaborn\n",
        "Seaborn is a visualization library that gives us access to a number of different easy graphing options. One of those options is a pairplot in which each featuer is compared to another in a series of graphs so that we might identify any possible relationships. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHbjRLkBs7X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4UImfowKck",
        "colab_type": "text"
      },
      "source": [
        "## Analysis of the data above\n",
        "Looking at the graphs above:\n",
        "\n",
        "*   Do you see any possible relationships?\n",
        "*   Are there any features that we can eliminate to make things more optimized?\n",
        "*   Are there any other issues that might be causing a problem?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6sKyjSOUUm0",
        "colab_type": "text"
      },
      "source": [
        "## Remove unneccessary features\n",
        "If a feature will not help the model, we should remove it before attempting any feature engineering or before training our model. With larger datasets, this can significantly reduce compute time, and it saves us from having to fix any errors in the data.\n",
        "\n",
        "*    It appears that the **```proximity_to_store```** is pretty much useless. It does not have any immediately recognizable scale, is likely unaffiliated with the price of a house and seems to be uniformly distributed. We can get rid of it.\n",
        "*    Latitude and Longitude also seem unlikely to be useful. It might be able to identify specific cities or regions, but it's not going to help us nearly as much as other features in the set.\n",
        "    *    Notice also that this feature is misspelled (should be \"latitude\" instead of \"lattitude\"). If we were going to keep this feature then we would fix the spelling in order to help any downstream applications or developers, as they might not notice\n",
        "*   We should also get rid of **```median_income```** since we are not sure what the scale represents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLMEce1NUTz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove unneccessary features\n",
        "del df['proximity_to_store']\n",
        "del df['lattitude']\n",
        "del df['longitude']\n",
        "del df['median_income']\n",
        "\n",
        "# Ensure that changes have been made\n",
        "display(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daRe8W8kXSiY",
        "colab_type": "text"
      },
      "source": [
        "## Fix feature names if necessary\n",
        "Looking at the feature names, **```t_rooms```**  stands out a bit. What is \"t_rooms\" measuring? Total rooms? Tea rooms? \n",
        "\n",
        "Looking at the feature next to it (**```total_bedrooms```**), it seems likely that this is supposed to be **```total_rooms```**, as the numbers seem consistently larger than the total bedrooms. After speaking with the team that gathered the data, we confirm that this is, in fact, a measure of total rooms per block, so we need to change the feature name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhvYcUiqYGPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure that feature names are descriptive of content and change them if necessary\n",
        "df.rename(columns={'t_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "# Check that changes are applied and correct\n",
        "display(df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efTqXQk0yEb",
        "colab_type": "text"
      },
      "source": [
        "## Find and count the missing values\n",
        "If a value is null when the data is imported to Pandas, then the value \"NaN\" is assigned. We could search through the entire dataset to manually locate the problems, but Pandas provides a much easier solution:\n",
        "\n",
        "\n",
        "\n",
        "**```dataframe.isnull()```** will help us to identify where the missing values lie, and **```sum()```** will count them for us within each feature, as seen below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql_Vdnfzyqo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a count of \"NaN\" or missing values by feature\n",
        "df.isnull().sum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm9PFeWSPX1M",
        "colab_type": "text"
      },
      "source": [
        "We see in the output above that 207 of our records are missing values. At this point, we need to deal with these missing values using one of the following methods:\n",
        "*    Assign a mean value to each (can create bias if outliers exist)\n",
        "*    Assign a median value\n",
        "*    Delete the samples with missing values\n",
        "\n",
        "In truth, 207 samples out of our 20,000+ is not really a big number, so we could probably delete the records without much impact to the accuracy of our model. For the purposes of this lesson, however, we will look through our options to see if we can save those records (and so that you can learn how to do this when a larger percentage of your data is affected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Tr45JBQqrr",
        "colab_type": "text"
      },
      "source": [
        "## Fill the values with the median of the feature\n",
        "Below, we copy the original dataframe (df) into a new dataframe (df2) to analyze the impact of using the median value. We need to see if this drastically changes any of the dataset's statistical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqEDxlfD3Qis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the dataframe to a new dataframe for comparison\n",
        "df_median = df.fillna(df.median())\n",
        "\n",
        "# Check that all null values have been filled in the new dataframe\n",
        "df_median.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDQWb3wX3dBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze the mean for the affected feature (total_bedrooms) against the other dataframe's value ()\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXVOg-Mugir0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_median.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxuAbsf_hP1f",
        "colab_type": "text"
      },
      "source": [
        "Next, let's compare the above results to what happens if we drop the null values instead of using the median."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3N_fl8Qhccz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dropped = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
        "df_dropped.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-TSTl6zi4vQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_median.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYKIZf8Pil5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dropped.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NseYAntndKT",
        "colab_type": "text"
      },
      "source": [
        "We can see from the above analysis that filling the null records with the median value in ```total_bedrooms``` does not drastically change the values as compared to the dropped version, and we get the added benefit of more data for all the other columns that are not null in those records, so we will keep them and move on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRfZYhiSn3Yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.fillna(df.median())\n",
        "display(df.describe())\n",
        "print('\\n\\nCount of NaN values in each category:')\n",
        "display(df.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjYXRAFdpRzn",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with outliers\n",
        "The next problem that we have to fix in our data are the outliers. \n",
        "\n",
        "If we look again at our data description (**```df.describe()```**), we can see that the max value is 500001, which indicates that the data does not go beyond that number. To confirm this suspicion, we can simply get a sum for all values over $500,000 by using the following command: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13qSg2n85HPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count the number of samples with values over 500000\n",
        "count1 = (df['median_house_value'] == 500001).sum()\n",
        "count2 = (df['median_house_value'] > 500000).sum()\n",
        "print(\"Count of houses valued at $500,001: {0}\".format(count1))\n",
        "print(\"Count of houses valued over $500,000: {0}\".format(count2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjtruGR6vbCe",
        "colab_type": "text"
      },
      "source": [
        "We can see that there are 965 samples in the dataset that have the value 500001. Since this represents almost 5% of our data, it stands a good chance of introducing bias into our model (skewing the mean for **```median_house_value```**). Furthermore, it is preventing us from being able to accurately analyze the various relationships between the data in our **```sns.pairplot(df)```** graphs. \n",
        "\n",
        "We could do something like we did before and replace the outlier values with a median value to preserve the other datapoints, but this would likely result in more problems. For example, these higher value homes likely have other figures associated with them such as **```total_rooms```** and **```total_bedrooms```** that will not be accurately represented by median house value. We will likely cause more problems in our model than we will fix, so it is probably best to just eliminate the outlier samples instead. We will lose 5% of our data, but the remaining data will likely be more accurate.\n",
        "\n",
        "Before we get rid of all of our outliers, however, let's first change all our text-based values to numerical ones. Currently **```ocean_proximity```** has values like ```<1H OCEAN``` and ```INLAND``` rather than numerical values. We can use ***one hot encoding*** to fix that problem. Basically, it will create new features for each possibility and then assign a ```0``` if false and ```1``` if true for each sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRk_ZwOS3mtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a new dataframe with boolean (0 or 1) values for 'ocean_proximity'\n",
        "features_df = pd.get_dummies(df, columns=['ocean_proximity'])\n",
        "\n",
        "# Print the results to ensure that we have numerical values for each feature\n",
        "display(features_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXjZIcRM69ls",
        "colab_type": "text"
      },
      "source": [
        "### Next we can get rid of the outliers\n",
        "Now we can deal with all possible outliers in our code by using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9mHCHWp1zos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary libraries\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Use the following code to eliminate all outliers from every column in our new feature set\n",
        "# This code is referenced here: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n",
        "features_df = features_df[(np.abs(stats.zscore(features_df)) < 3).all(axis=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3v-2mBjd-vL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze the median house value \n",
        "print('Basic distribution:')\n",
        "sns.pairplot(features_df)\n",
        "#sns.distplot(features_df['median_house_value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCbaWW5HtkdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze the median house value with specified number of bins\n",
        "print('Binned distribution(15 bins)')\n",
        "sns.distplot(df['median_house_value'], bins=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvmgE_Ryyqud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(df['total_rooms'], bins=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3MY17z12Zk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display((df['total_rooms'] > 6000).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}