{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine-Learning-for-Absolute-Beginners_v1.2.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZdBTVdyx6XLO","colab_type":"text"},"source":["# I. Introduction\n","This beginner's project is based on many ideas found in *Machine Learning for Absolute Beginners: A Plain English Introduction* by Oliver Theobald. It has been heavily augmented with references found around the web (linked when appropriate).\n","\n","---\n","\n","![Machine Learning for Absolute Beginners](https://images-na.ssl-images-amazon.com/images/I/413%2BI3pEaXL.jpg)\n","\n","---\n","\n","\n","This book is an excellent introduction for those just beginning their machine learning journey. Though this class introduces a number of principles found in that text, I highly recommend buying the book yourself and proceed through it after your experience here. He walks the reader through a number of important concepts that are too extensive for this course, but his writing is clear and he does a spectacular job of explaining difficult topics to ensure understanding. Additionally, he provides a number examples which the reader can tackle after getting down the basics. \n","\n","The book can be found here if you are interested: [Machine Learning for Absolute Beginners](https://www.amazon.com/Machine-Learning-Absolute-Beginners-Introduction/dp/1549617214/ref=sr_1_1?crid=1AF1PFSE85G4F&keywords=machine+learning+for+absolute+beginners+a+plain+english+introduction&qid=1563399014&s=gateway&sprefix=machine+learning+for+absolute+beginners+%2Caps%2C326&sr=8-1)"]},{"cell_type":"markdown","metadata":{"id":"iSOFzt3G6vkv","colab_type":"text"},"source":["# II. Obtain and Work with the Data"]},{"cell_type":"markdown","metadata":{"id":"0HupfNor71lI","colab_type":"text"},"source":["## A. Get the dataset\n","1. Press \"Connect\" in the upper right corner of this page (on Colab).\n","\n","2. The dataset is available through Kaggle.com which requires a login to access. However, I have made the dataset available on my Github page, but you should be able to access and import it using the following commands. \n","\n","\n","- If the following command fails, please go to [Github page](https://github.com/ccwilliamsut/machine_learning/tree/master/absolute_beginners/data_files/modified), download \"CaliforniaHousingDataModified.csv\" to your downloads folder and uncomment the alternative commands.\n","\n","- If the above does not work and you are still having trouble, consult this [link](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92) to learn how to work with data files in Colaboratory."]},{"cell_type":"code","metadata":{"id":"br8B_N6pRQnf","colab_type":"code","colab":{}},"source":["# ------------------------------------------ Setup the Environment ------------------------------------------\n","# Import libraries\n","import pandas as pd\n","import seaborn as sns; sns.set()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","import missingno as msno\n","from IPython.display import display\n","from sklearn.model_selection import train_test_split \n","from sklearn import ensemble\n","from scipy import stats\n","from sklearn import linear_model\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import learning_curve,GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn import preprocessing\n","from collections import Counter\n","from sklearn.externals import joblib\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# Set the URL for the data file\n","url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n","\n","# Import the datafile from the provided url and run the cell\n","df = pd.read_csv(url)\n","\n","# ------- ALTERNATIVE COMMANDS (if above commands do not work-------\n","#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXeyg7UMXReV","colab_type":"text"},"source":["## B. Learn how to initially explore datasets\n"]},{"cell_type":"markdown","metadata":{"id":"tb-nqdN4X4o0","colab_type":"text"},"source":["### (1). Basic functions for analyzing datasets\n","If we want to see the names of our **features** (i.e. column headings), we simply issue the first of the following commands. We can also neatly list them out with the second function."]},{"cell_type":"code","metadata":{"id":"4KUCFFiZZH_X","colab_type":"code","colab":{}},"source":["# Look at the first 5 rows of the file\n","print('The \"head\" of our data:')\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5IrKqXA5y3_","colab_type":"code","colab":{}},"source":["# Get a random sample of the data\n","print('\\n\\nRandom sample of the data:')\n","df.sample(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5dpBjJNHGGZ","colab_type":"code","colab":{}},"source":["# Get a statistical description of the data\n","print('\\n\\nStatistical description of the data:')\n","df.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9XX2OfADona","colab_type":"code","colab":{}},"source":["# List the column headers to see what we are working with\n","print(\"\\n\\nHere are all of our columns:\\n\")\n","df.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCq3jZPJDssa","colab_type":"code","colab":{}},"source":["# List all the columns in the dataset in an ordered way\n","print('\\n\\nAll of our columns in a list form (much easier to read):\\n')\n","list(df.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbSMCUGIDwNa","colab_type":"code","colab":{}},"source":["# Look at a specific sample (record) in the dataset\n","print('\\n\\nLook at one specific record in a dataset:\\n')\n","print(df.iloc[27])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNIjsKX_Dy6A","colab_type":"code","colab":{}},"source":["# Look at the \"shape\" of the dataset\n","print('\\n\\nLook at the shape a dataset (rows / columns):\\n')\n","df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGeJj3qkEYfE","colab_type":"code","colab":{}},"source":["# Look at some information about the dataset (feature name, total non-null record count,\n","#   wheter the feature is empty, datatype for each feature)\n","print('\\n\\nLook at the info about a dataset (feature name, record count, records exist, datatype):\\n')\n","df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqwz_zJlbGzG","colab_type":"text"},"source":["### (2). Analyze the dataset for potential problems\n","\n","**Finding problems**\n","- Look at the above data closely. **Talk** about it with your **group/partner** or think about it on your own.\n","- Can you spot the potential issues that we might have with this dataset?\n","\n","```\n","\n","\n","```\n","\n","When you are ready, unhide the cells below to see some of the potential issues with this dataset."]},{"cell_type":"markdown","metadata":{"id":"GlOBbJ1QzBLB","colab_type":"text"},"source":["#### i. Looking for missing data\n","One thing that we need to do before moving on is to see if there is any missing data. If a value is null when the data is imported to Pandas, then the value \"NaN\" is assigned. Some of our data might have null (```NaN```) values that will cause problems with the performance of our model. \n","\n","Missing data can cause a number of problems:\n","- Further analysis with statistical measures and graphs can **distort our understanding**\n","- Missing data can **distort our model** if those values affect predictive or clustering calculations\n","- Some **functions can fail** if they encouter null values\n","\n","Let's quickly look over our dataset for any missing values."]},{"cell_type":"code","metadata":{"id":"viAG3Pwj1NON","colab_type":"code","colab":{}},"source":["# Use the MissingNo library to visualize where any missing data is\n","msno.matrix(df)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SedV62RY4msA","colab_type":"text"},"source":["It appears that our **```total_bedrooms```** feature has some missing data. We can investigate further to count the number of records with missing values."]},{"cell_type":"code","metadata":{"id":"qLCtiWsNAt99","colab_type":"code","colab":{}},"source":["# Get a count of \"NaN\" or missing values by feature\n","print('Count of missing values by feature:\\n')\n","display(df.isnull().sum(axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWfvD3cseciV","colab_type":"text"},"source":["### (3). Discussion / Reflection\n"]},{"cell_type":"markdown","metadata":{"id":"yFrBAnnWfQFk","colab_type":"text"},"source":["#### Quality issues\n","The primary problems at this point are:\n","- The spelling of **```lattitude```**\n","- The meaning of **```t_rooms```**\n","- The scale used for **```median_income```**\n","- The scale used for **```proximity_to_store```**\n","\n","\n","#### Analyzing the problems\n","- **Spelling** might not seem important at first (after all, it doesn't directly affect our model's performance), but the spelling is actually a fairly big issue. This is not because of any immediate coding problems, as we could easily write our code and accommodate the misspelling. But when our model moves downstream in other workflows, we will have to make special note of the misspelling to others, lest we make their debugging more problematic and time-comsuming. Worse yet, it might not get caught at all and be shown to leadership as is, possibly leaving a bad impression. **If we think that we will possibly use this feature, then it is better to fix it now than wait for later**. \n","\n","- **```t_rooms```** presents another issue, as we cannot be sure exactly what it means at this point. We can guess based on the other data that it likely means \"total rooms\", but we need to be sure. To get the answer, we (fictionally, in this case) contact the team that collected the data and confirm that **```t_rooms```** should be **```total_rooms```**. Again, we want to think about if we actually need this feature before making the change. It seems likely that the total rooms in a house will have some kind of correlation to the overall price, so let's keep this and change it.\n","\n","- **```median_income```** and **```proximity_to_store```** present different problems. We do not know the scale being used in either case, and they appear to be different from one another. We will have to analyze these features further in the next section to see if we (1) can use them as is, (2) have to drop one or both of them, or (3) have to engineer them to create something useful."]},{"cell_type":"code","metadata":{"id":"90Nk40d7bTRF","colab_type":"code","colab":{}},"source":["# Change the spelling of a feature name\n","df.rename(columns = {'lattitude':'latitude',\n","                     't_rooms':'total_rooms'\n","                     },\n","          inplace=True\n","          )\n","\n","# Verify that the changes have been made as intended\n","list(df.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwZ2QxLrF5u-","colab_type":"text"},"source":["## C. Analyze the data for viability\n","- Become more familiar with the data\n","- Look for possible relationships\n","- Determine which features should be used and which need to be removed\n","\n","```\n","\n","```\n","\n","**Key questions to consider at this point:**\n","1. Does my data have **labels**? (dependent variables; determines whether we will use supervised or unsupervised algorithms)\n","2. Are there major **issues** with the data (lots of null values, small number of samples, misspellings, derived data, unknown scales or values, etc.)?\n","3. What is my **goal**? (For this course, our goal is to create a model that can predict housing prices based upon the given set of features.)\n","    - Will I be able to accomplish that goal with this dataset?\n","    - If not, can I employ feature engineering to create the data I need?\n","    - If so, which features will contribute to that goal and which are unnecessary?\n","4. Can I see any **relationships** in the data that might serve as a good foundation for my model?"]},{"cell_type":"code","metadata":{"id":"LgkcdwEoGwze","colab_type":"code","colab":{}},"source":["# Look at the data once again to get an idea of variance, outliers, problem features, relationships, etc.\n","df.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tuoKogADhnmD","colab_type":"text"},"source":["### (1). Identify patterns, problems and usability\n","- Notice that **not all columns are shown** in the **```dataset.describe()```** function... Why do you think this might be?\n","- Do you see any **new problems**?\n","- Is our data **labeled or unlabeled**?\n","- Can you spot **any possible relationships** that we might explore (using any of the data above)?\n","- Can we **accomplish our goal** with this dataset?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"COjpfuwULV19","colab_type":"text"},"source":["### (2). Preliminary inquiries about \"problem\" data\n","Let's look at a few features that we have identified as problems above:\n","1. Why is **```ocean_proximity```** not included in the **```dataset.describe()```** function?\n","2. What do the distributions of **```median_income```** and **```ocean_proximity```** tell us? Are they useful?"]},{"cell_type":"code","metadata":{"id":"xqHI5K4QFdFA","colab_type":"code","colab":{}},"source":["# List 15 random samples in 'ocean_proximity' to discover why it is not shown in the describe function\n","print('Here are some of the contents of the \"ocean_proximity\" column:')\n","df['ocean_proximity'].sample(15)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYfcKL21yTUK","colab_type":"text"},"source":["#### i. Question: Ocean Proximity\n","- Why do you think that **```ocean_proximity```** is not included in the **```dataset.describe()```** function?\n","- Do you think we can use this data? How?"]},{"cell_type":"code","metadata":{"id":"P4GlFYC23kmv","colab_type":"code","colab":{}},"source":["# Look at distributions of median_income and ocean_proximity\n","fig, (ax0, ax1) = plt.subplots(nrows = 1,\n","                               ncols = 2,\n","                               figsize = (15, 5),\n","                               sharey = False\n","                               )\n","df.hist('median_income', ax = ax0)\n","sns.countplot(x = 'ocean_proximity',\n","              data = df,\n","              orient = 'h',\n","              ax = ax1\n","              )\n","\n","\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6DJRKyyu4I0P","colab_type":"text"},"source":["#### ii. Questions: Usability\n","- Do you think that the above data will help our model to be more accurate?\n","- Should we keep one, both or none of these features?\n","\n","We will not any changes to these columns just yet, but it is worth noting the potential problems that they exhibit (unknown scale and non-numerical data)."]},{"cell_type":"markdown","metadata":{"id":"qJwZFAZhFV8X","colab_type":"text"},"source":["### (3). Use graphs to explore the data further\n","Using a variety of graphs is a great way to explore data. \n","\n","> Websites referenced:\n","- [Overview of graph types and purposes](https://towardsdatascience.com/a-step-by-step-guide-for-creating-advanced-python-data-visualizations-with-seaborn-matplotlib-1579d6a1a7d0)"]},{"cell_type":"markdown","metadata":{"id":"SiGojgqR4ws5","colab_type":"text"},"source":["#### i. Univariate Data Analysis\n","We can use some graphs to analyze one feature at a time. A couple of the more useful in this regard are:\n","- Histograms\n","- KDE (Kernel Density Estimation) - helps us to understand the distribution of data\n","\n","> Websites referenced:\n","[Analyze the data through data visualization using Seaborn (Toward Data Science)](https://towardsdatascience.com/analyze-the-data-through-data-visualization-using-seaborn-255e1cd3948e)"]},{"cell_type":"markdown","metadata":{"id":"gWtlVk955w7U","colab_type":"text"},"source":["##### a. Histogram Plots\n","\n","> Websites referenced:\n","- [Tutorial on visualizing distributions](https://www.machinelearningplus.com/plots/matplotlib-histogram-python-examples/)\n","- [Creating multi-dimensional subplots](https://matplotlib.org/3.1.1/gallery/subplots_axes_and_figures/subplots_demo.html)\n","- [Great demo of the differences between histograms and KDE](https://mglerner.github.io/posts/histograms-and-kernel-density-estimation-kde-2.html)\n","- [Explanation of KDE](http://www.mvstat.net/tduong/research/seminars/seminar-2001-05/)"]},{"cell_type":"code","metadata":{"id":"9ROhYEAU7aDH","colab_type":"code","colab":{}},"source":["# List the features in the dataset\n","list(df.columns)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mKHFErB6Wm_","colab_type":"code","colab":{}},"source":["# Use histograms to explore features in the dataset\n","\n","# Create a \"figure\" that can hold multiple plots\n","fig, (ax0, ax1, ax2) = plt.subplots(nrows = 1,              # create a grid with a specified number of rows\n","                                    ncols = 3,              # specify the number of columns\n","                                    figsize = (15, 4),      # specify the size of each subplot\n","                                    sharey = False          # specify if all plots will share the y-axis values\n","                                    )\n","\n","# ------------------ Histograms ---------------------\n","# Histogram 1 (in slot 'ax1', the first container)\n","sns.distplot(df['housing_median_age'], \n","             kde = False,\n","             bins = 20, \n","             color = 'magenta',\n","             ax = ax0\n","             )\n","\n","# Histogram 2 (in slot 'ax2', the second container)\n","sns.distplot(df['total_rooms'], \n","             kde = False,\n","             bins = 30, \n","             color = 'green', \n","             ax = ax1\n","             )\n","\n","# Histogram 3 (in slot 'ax3', the third container)\n","sns.distplot(df['median_house_value'], \n","             kde = False,\n","             bins = 50, \n","             color = 'blue',\n","             ax = ax2\n","             )\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlYwzsStTg6A","colab_type":"text"},"source":["##### b. KDE (Kernel Density Estimation) Plots"]},{"cell_type":"code","metadata":{"id":"CCpHTii8SB72","colab_type":"code","colab":{}},"source":["# Plot Kernel Density Estimates (KDE) for some features\n","fig, (ax0, ax1, ax2) = plt.subplots(nrows = 1,              # create a grid with a specified number of rows\n","                                    ncols = 3,              # specify the number of columns\n","                                    figsize = (15, 4),      # specify the size of each subplot\n","                                    sharey = False          # specify if all plots will share the y-axis values\n","                                    )\n","# KDE plot 1 (in slot 'ax4', the fourth container)\n","sns.distplot(df['housing_median_age'], \n","             hist = False,\n","             color = 'magenta',\n","             ax = ax0,\n","             kde_kws = {'shade': True}\n","             )\n","\n","# KDE plot 2 (in slot 'ax5', the fifth container)\n","sns.distplot(df['total_rooms'], \n","             hist = False,\n","             color = 'green', \n","             ax = ax1,\n","             kde_kws = {'shade': True}\n","             )\n","\n","# KDE plot 3 (in slot 'ax6', the sixth container)\n","sns.distplot(df['median_house_value'], \n","             hist = False,\n","             color = 'blue',\n","             ax = ax2, \n","             kde_kws = {'shade': True}\n","             )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NF19UgoPI-wb","colab_type":"text"},"source":["#### ii. Bivariate Data Analysis\n","We can use a multitude of graphs to analyze features against each other in varying combinations.\n","\n","Datasets generally have two primary data types:\n","- Statistical Data (integers, continuous variables)\n","- Categorical Data (boolean values, text-based values, etc.)\n","\n","We can explore both of these types with the following types of graphs (note that this is not an exhaustive list):\n","- Pairplots\n","- Scatterplots\n","- Lineplots\n","- Surface plots\n","- Correlation heatmaps\n","- Category plots\n","- Boxplots\n","- Pointplots\n","\n","> Websites referenced: \n","- [Analyze the data through data visualization using Seaborn (Toward Data Science)](https://towardsdatascience.com/analyze-the-data-through-data-visualization-using-seaborn-255e1cd3948e)"]},{"cell_type":"markdown","metadata":{"id":"-LzECFVYLY-n","colab_type":"text"},"source":["##### a. Statistical Data\n","- Scatterplots\n","- Lineplots\n","- Pairplots\n","- Surface Plots\n","\n","> Websites referenced:\n","- [Stackoverflow Help](https://stackoverflow.com/questions/25212986/how-to-set-some-xlim-and-ylim-in-seaborn-lmplot-facetgrid)\n","\n","Let's begin by analyzing a few features using these types of graphs:"]},{"cell_type":"markdown","metadata":{"id":"kGqfEpbGBf5p","colab_type":"text"},"source":["1. **Scatterplots**"]},{"cell_type":"code","metadata":{"id":"UNg9N88HMV8b","colab_type":"code","colab":{}},"source":["# Use scatterplots to compare features in the dataset\n","\n","# Create a \"figure\" that can hold multiple plots (in this case, 1 row and 3 columns)\n","fig, ((ax1, ax2, ax3)) = plt.subplots(nrows = 1,                      # create a grid with a specified number of rows\n","                                      ncols = 3,                      # specify the number of columns\n","                                      figsize = (20, 5),              # specify the size of each subplot\n","                                      sharey = False,                 # specify if all plots will share the y-axis values\n","                                      sharex = False                  # specify if all plots will share the x-axis values\n","                                      )\n","\n","# Plot scatterplots on the top row and lineplots on the bottom row\n","# Scatterplots 1 to 3\n","ax1 = sns.regplot(x = df['median_income'],\n","                  y = df['median_house_value'],\n","                  data = df,\n","                  ax = ax1, \n","                  dropna = True,\n","                  order = 2,                                          # Note here that we are using polynomial regression\n","                  line_kws = {'color': 'darkorange'},\n","                  fit_reg = True\n","                  )\n","\n","ax2 = sns.regplot(x = df['total_bedrooms'],\n","                  y = df['median_house_value'],\n","                  data = df,\n","                  ax = ax2, \n","                  dropna = True,\n","                  order = 2,\n","                  line_kws = {'color': 'purple'},\n","                  fit_reg = True\n","                  )\n","\n","ax3 = sns.regplot(x = df['population'],\n","                  y = df['median_house_value'],\n","                  data = df,\n","                  ax = ax3, \n","                  dropna = True,\n","                  order = 2,\n","                  line_kws = {'color': 'blue'},\n","                  fit_reg = True\n","                  )\n","\n","\n","# Change the range of a few axes variables to make the graphs more useful\n","ax1.set(ylim = (0, 500000))\n","\n","#ax2.set(ylim = (0, 500000))\n","#ax2.set(xlim = (0, 2))\n","\n","ax3.set(ylim = (0, 500000))\n","ax3.set(xlim = (0, 15000))\n","\n","#ax6.set(xlim = (0, 15000))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9tCbTs1BqMp","colab_type":"text"},"source":["###### 2. Lineplots\n",">**Note:** Lineplots can be very compute-intensive b/c they are first sorted and then plotted.  We will therefore create a sample set of data for use with those plots."]},{"cell_type":"code","metadata":{"id":"yKL8hoq4BvIQ","colab_type":"code","colab":{}},"source":["# Create a \"figure\" that can hold multiple plots\n","fig, ((ax1, ax2, ax3)) = plt.subplots(nrows = 1,                    # create a grid with a specified number of rows\n","                                    ncols = 3,                      # specify the number of columns\n","                                    figsize = (20, 5),             # specify the size of each subplot\n","                                    sharey = True,                 # specify if all plots will share the y-axis values\n","                                    sharex = False                  # specify if all plots will share the x-axis values\n","                                    )\n","# Create a sample dataset\n","df_sample = df.sample(frac = 0.005, random_state = 5)\n","\n","# Ensure that our sample dataset does not have any missing values\n","df_sample = df_sample.fillna(df_sample.median())\n","\n","# Plot the data for each figure container\n","ax1 = sns.lineplot(x = df_sample['median_income'],\n","                   y = df_sample['median_house_value'],\n","                   data = df_sample,\n","                   ax = ax1,\n","                   style = 'ocean_proximity',\n","                   hue = 'ocean_proximity'\n","                   )\n","\n","ax2 = sns.lineplot(x = df_sample['total_bedrooms'],\n","                   y = df_sample['median_house_value'],\n","                   data = df_sample,\n","                   ax = ax2,\n","                   hue = 'ocean_proximity'\n","                   )\n","\n","ax3 = sns.lineplot(x = df_sample['housing_median_age'],\n","                   y = df_sample['median_house_value'],\n","                   data = df_sample,\n","                   ax = ax3,\n","                   style = 'ocean_proximity'\n","                   )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5FdXd6poWg9","colab_type":"text"},"source":["###### 3. Pairplots\n","With this simple function, we can quickly analyze most of our features against one another to:\n","- Seek out possible relationships\n","- Identify possible issues with data\n","```\n","```\n","\n","***Websites referenced:***\n","- [Seaborn Documentation: Pairplots](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n","- [Short Tutorial](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n","- [Pairplot Code Examples](https://jovianlin.io/data-visualization-seaborn-part-2/)"]},{"cell_type":"code","metadata":{"id":"B0iJFEpmNrDg","colab_type":"code","colab":{}},"source":["# Use seaboarn.pairplot() to quickly look at possible relationships and visually analyze the data\n","\n","# Use this to analyze each feature against the others while also segmenting out the proximity to the ocean\n","#sns.pairplot(data = df,\n","#              hue='ocean_proximity', \n","#              dropna=True, \n","#             )\n","\n","# Use this function to analyze each feature against the others with an added regression line to help identify relationships\n","sns.pairplot(data = df, \n","             dropna = True\n","             )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xrG-R9o7vSG","colab_type":"text"},"source":["###### 4. Surface Plots\n","The immediate goal for us is to determine which features will help us make a better model and which ones will not. For those that will not help us, we want to drop them in order to reduce compute needs and processing time for our final model. \n","\n","Let's look at some different types of graphs and how they can help us analyze our dataset for this purpose.\n","\n","> Websites referenced:\n","> [Stackoverflow: Limiting x- and y-axis values](https://stackoverflow.com/questions/54951362/seaborn-jointplot-with-defined-axes-limits)\n","\n","```\n","\n","```\n","Let's first look at a basic contoured graph (using KDE: Kernel Density Estimation):"]},{"cell_type":"code","metadata":{"id":"yqULlQ_hQw8l","colab_type":"code","colab":{}},"source":["# Use contoured plots with KDE (Kernel Density Estimation) and targeted ranges\n","\n","# Create a surface plot with KDE\n","plot1 = sns.jointplot(x = df['total_bedrooms'],       # set the x-axis variable\n","                      y = df['median_house_value'],   # set the y-axis variable\n","                      data=df,                        # define the dataset being used\n","                      kind='kde',                     # define the graph type to be used\n","                      dropna=True                     # define what happens to 'NaN' values\n","                      )\n","\n","# Adjust the top of the subplot\n","plt.subplots_adjust(top=0.9)\n","\n","# Add a title to a graph\n","plot1.fig.suptitle('Plot 1: Original Plot')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSeyGsVCtZZ6","colab_type":"text"},"source":["Looking at the data above, we can see that the vast majority of **```total_bedrooms```** values are less than 1000. It is likely that we are seeing the effect of some outlier data above 1000, so let's concetrate on that portion of the data to see if there is something interesting to learn.\n","```\n","\n","```\n","We will construct the same graph, but we will limit the x-axis and y-axis outputs to specified ranges (x <= 1000 and y <= 400,000)."]},{"cell_type":"code","metadata":{"id":"HWUMn7SxrqJD","colab_type":"code","colab":{}},"source":["# Plot the same type of graph (contour KDE) but with restricted x- and y-axis values\n","plot2 = sns.jointplot(x = df['total_bedrooms'],\n","                      y = df['median_house_value'], \n","                      data=df,\n","                      kind='kde',\n","                      dropna=True\n","                      )\n","\n","# Limit the x-axis plots to a specified degree\n","plot2.ax_marg_x.set_xlim(0, 1000)\n","plot2.ax_marg_y.set_ylim(0, 400000)\n","\n","# Add a title to a second graph\n","plot2.fig.suptitle('Plot 2: Limited x-axis & y-axis values')\n","plt.subplots_adjust(top=0.9)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlBQQmjtbPja","colab_type":"text"},"source":["###### 5. Correlation Heatmaps\n","We can also look at how well data is correlated via the **```seaboarn.heatmap()```** function. This will help to give us an idea of which features are well or poorly correlated to each other.\n","\n","> *Websites referenced:*\n","- [Seaborn Documentation: Heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n","- [Excellent tuturial on modifying heatmaps](https://heartbeat.fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations-f1c49c816f07)\n","- [Short Tutorial about correlation and heatmaps](https://jovianlin.io/data-visualization-seaborn-part-2/)"]},{"cell_type":"code","metadata":{"id":"cIPm62PDDUPO","colab_type":"code","colab":{}},"source":["# -- Analyze the data with a heatmap --\n","# 1. Only keep the columns and data that we want to see correlated\n","cols = df.drop(['latitude', 'longitude', 'ocean_proximity'],\n","               axis=1\n","               )\n","\n","# 2. Fill in missing values with the median value of the feature\n","#     - You can also use the mean (or even the mode if dealing with categorical data)\n","cols.fillna(cols.median(),\n","            inplace = True\n","            )\n","\n","# 2. Calculate the correlations\n","corr = cols.corr()\n","\n","# 3. Set the size you want\n","plt.figure(figsize=(9 ,9))\n","\n","# 4. Display the heatmap\n","sns.heatmap(corr,\n","            annot=True,\n","            vmin = -1,\n","            vmax = 1,\n","            center = 0\n","            )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2s6-IaSGG8D","colab_type":"text"},"source":["##### b. Categorical Data\n","Categorical data is comprised of:\n","- qualitative measurements (good / bad / okay, 5-stars / 3-stars, etc.)\n","- discrete measurements ('near ocean', 'on time', etc.)\n","\n","```\n","\n","```\n","\n","Categorical data can also be qualified by the following parameters:\n","- nominal (no intrinsic order to the categories)\n","- ordinal (has an intrinsic order such as a 5-star ratings system)\n","- dichotomous (can be only one of two possible values such as true/false, off/on, etc.)\n","\n","```\n","\n","```\n","\n","We can analyze categorical data using many types of graphs with varying techniques. One of these techniquest is known as a **facetgrid**, and it allows us to break out statistical data according to categorical measures."]},{"cell_type":"markdown","metadata":{"id":"H6pDV2UHWbFl","colab_type":"text"},"source":["###### 1. Countplots\n"]},{"cell_type":"code","metadata":{"id":"SFZvVOrXWnOX","colab_type":"code","colab":{}},"source":["# Create a countplot with a categorical variable\n","sns.countplot(x = df['ocean_proximity'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSFdnoVJE26l","colab_type":"text"},"source":["###### 2. Histograms (broken down by category, using Facetgrid)\n","**Facetgrid plots** allow one to break out data by category while analyzing either univariate or bivariate data."]},{"cell_type":"code","metadata":{"id":"K5FqNq4XkYD9","colab_type":"code","colab":{}},"source":["# Analyze graphs side-by-side (using seaborn Facetgrid)\n","ghist = sns.FacetGrid(df, \n","                      col='ocean_proximity', \n","                      hue = 'ocean_proximity', \n","                      dropna = True, \n","                      legend_out=True, \n","                      despine=True\n","                      )\n","ghist.map(plt.hist,\n","          'median_house_value',\n","          alpha=1,\n","          bins = 20\n","          )\n","\n","# Create another facetgrid with scatterplots\n","gscat = sns.FacetGrid(df,\n","                      col='ocean_proximity',\n","                      hue = 'ocean_proximity',\n","                      dropna = True,\n","                      despine=True\n","                      )\n","gscat.map(plt.scatter,\n","          'median_income',\n","          'median_house_value',\n","          alpha=.3\n","          )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcnJDWILYUjg","colab_type":"text"},"source":["###### 3. Category Plots (Catplots)"]},{"cell_type":"code","metadata":{"id":"nE_dNcIGZFQm","colab_type":"code","colab":{}},"source":["# Create a category plot using 'ocean_proximity'\n","g = sns.catplot(x = 'ocean_proximity',\n","                y = 'median_house_value',\n","                hue = 'ocean_proximity',\n","                data = df,\n","                height = 5,\n","                aspect = 1.5,\n","                kind = 'box',          # Change this to: 'box', 'violin', 'boxen', 'point', or 'bar'\n","                order = ['NEAR OCEAN',\n","                         'NEAR BAY',\n","                         '<1H OCEAN',\n","                        'ISLAND',\n","                        'INLAND']\n","                )\n","\n","g.set_xticklabels(rotation = 45)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHlyUfU2f2fH","colab_type":"text"},"source":["#### (4). Binning / Bucketing Data\n","Using this method, we can analyze continuous variables as categorical ones (though the actual value might remain continuous). THis allows us to explore our dataset in different ways than would be natively available.\n","\n","In the following example, we will create \"bins\" or \"buckets\" of categories for **```median_house_value```** and then filter our data into them (using a new feature called **```median_house_value_bins```**."]},{"cell_type":"code","metadata":{"id":"4FOBvLIkgt3n","colab_type":"code","colab":{}},"source":["# Use binned / bucketed data to create discrete groups from statistical (continuous) data\n","\n","# Create bin / bucket edges (left is inclusive, right is excusive), then create labels to match those bins\n","bins = [0, 100000, 200000, 300000, 400000, 500000, 600000]\n","labels = ['< 100k', '100-199', '200-299', '300-399', '400-499', '500-599']\n","\n","# Create a new feature in our dataset to hold the new categorical data and perform the pandas.cut() operation\n","df['median_house_value_bins'] = pd.cut(df['median_house_value'], \n","                                       bins = bins,\n","                                       labels = labels)\n","\n","# Create a variable to hold the name of our new category and operators (so changing them later is easy)\n","yval = 'avg_people_per_household'\n","vnum = 'population'\n","vdem = 'households'\n","\n","# Here we create another feature based on calculations of existing features\n","df[yval] = (df[vnum] / df[vdem])\n","\n","# Create a catplot which will break out our categorical data in multiple ways\n","g = sns.catplot(x = 'ocean_proximity',                # set the x-axis categorical variable (will create ticks for each category)\n","                y = yval,                             # set the y-axis variable (the dependent variable)\n","                col = 'median_house_value_bins',      # set the column category with which you want to break up the data\n","                data = df,                            # set the data source\n","                height = 4,                           # set the height of the graph\n","                aspect = .8,                          # set the aspect ratio to determine the graph shape (height * aspect)\n","                order = ['NEAR OCEAN',                # set the order in which you would like to lay out the categories on the x-axis\n","                         'NEAR BAY',\n","                         '<1H OCEAN',\n","                         'ISLAND',\n","                         'INLAND'\n","                         ]\n","                )\n","\n","g.set_axis_labels(x_var = '', y_var = yval)           # set the variables to use on the x- and y-axes\n","g.set_xticklabels(rotation = 45)                      # rotate the x-axis labels (if running into each other)\n","g.set(ylim = (0,20))                                   # set the y-axis limits to measure"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWHoKoZRc7lQ","colab_type":"text"},"source":["#### (5). FInal Analysis and Planning\n","Now that we have explored our data in multiple ways and visualized it, we can begin to transform it in ways that will help us accomplish our goal (to predict the **```median_house_value```** based on the feature set that we choose.\n"]},{"cell_type":"markdown","metadata":{"id":"g2_TB9_ljfDZ","colab_type":"text"},"source":["\n","##### a. Questions:\n","- How do we read this data? What does it mean?\n","- Which features, if any, will help us build a model that can accurately predict the **```median_house_value```**? Do any of them appear to correlate?\n","- Which features do we need to **drop** or **change**?"]},{"cell_type":"markdown","metadata":{"id":"OFojxRFfjnuD","colab_type":"text"},"source":["##### b. New problems:\n","\n","When looking at all the data analysis above, it becomes easier to identify at least a few new problems:\n","1. The **``` median_house_value```** has a lot of values stacked up in the $500,000 range\n","2. **```total_rooms```** and **```total_bedrooms```** have very *long tails* which can cause problems with accuracy (changes the mean, distorts graphs, etc.)\n","3. We have **categorical data** in ```ocean_proximity```...  Can we use this in a machine learning model?\n","4. We have some **missing data** in one of our columns... Can you spot which one?\n","\n","```\n","```\n","\n","Take some time to think about the above questions. Discuss them with your group or partner if applicable."]},{"cell_type":"markdown","metadata":{"id":"d_dmE0ovyZ96","colab_type":"text"},"source":["# III. Scrubbing Data\n","Most of a data scientist's time is spent on working with data. This includes performing all of the tasks that we have done previously as well as the ones which we will now cover:\n","- One-hot encoding\n","- Deleting (and sometimes creating new) features\n","- Fixing missing data\n","- Dealing with outlier data\n","\n","\n","---\n","\n","At the beginning of this section, we will re-import all of our libraries and perform all of the small changes that we have made thus far. The reason I re-introduce this code here is to make it easier and faster for you to make changes to your code and see the results. It keeps you from having to wait for all of the above code to execute before seeing any of your changes. **Please note that any changes you make above will not be reflected below, as the dataset is essentially \"reset\" with the code cell that follows**.\n","\n","> **NOTE:** Because we will be looking at a heatmap a couple of times, I have created a function call that will make it much easier. By calling this function and supplying it with a version of the dataframe (optional), we do not have to re-create the code each time. To call it from here on out, we simply issue the command **```make_heatmap(df)```**."]},{"cell_type":"code","metadata":{"id":"xNizcb4hteO8","colab_type":"code","colab":{}},"source":["# ------------------------------------------ Setup the Environment ------------------------------------------\n","# Import libraries\n","import pandas as pd\n","import seaborn as sns; sns.set()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","import missingno as msno\n","from IPython.display import display\n","from sklearn.model_selection import train_test_split \n","from sklearn import ensemble\n","from scipy import stats\n","from sklearn import linear_model\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import learning_curve,GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn import preprocessing\n","from collections import Counter\n","from sklearn.externals import joblib\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# Set the URL for the data file\n","url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n","\n","# Import the datafile from the provided url and run the cell\n","df = pd.read_csv(url)\n","\n","# ------- ALTERNATIVE COMMANDS (if above commands do not work-------\n","#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n","\n","# Change the spelling of a feature name\n","df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n","\n","\n","# --------------------------------------------- FUNCTIONS ---------------------------------------------\n","def make_heatmap(dataframe = df):\n","  # Show a heatmap to identify any possible correlations\n","  # - Calculate the correlation matrix\n","  corr = df.corr()\n","\n","  # Set the size you want\n","  plt.figure(figsize=(15 ,9))\n","\n","  # Display the heatmap\n","  sns.heatmap(corr,\n","              annot=True,\n","              vmin = -1,\n","              vmax = 1,\n","              center = 0,\n","              fmt = '.1g',\n","              cmap = 'coolwarm'\n","              )\n","\n","  plt.show()\n","  plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OG1V5YxlHRrs","colab_type":"text"},"source":["## A. One-Hot Encoding\n","One of our features, **```ocean_proximity```**, is categorical. Because we believe this feature to be useful in helping us predict housing prices, we want to keep it, but we must change the categorical values into numerical ones for modeling purposes. To do this, we employ a procedure known as **one-hot encoding** which creates a new feature for each of the known values and then assigns a **```1``` or ```0```** depending upon whether or not each row contains that value."]},{"cell_type":"code","metadata":{"id":"fibhmpJAIAZ8","colab_type":"code","colab":{}},"source":["# Perform one-hot encoding on 'ocean_proximity'\n","# Show the initial state \n","print('The original state of our dataset:')\n","display(df.sample(5))\n","\n","# 1. First, cast our target feature into a categorical data type\n","df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n","\n","# 2. Create a temp dataframe to hold our new dummy values\n","df_dummies = pd.get_dummies(df['ocean_proximity'], \n","                            drop_first = True\n","                            )\n","\n","# 3. Drop the target feature from the original dataframe\n","df.drop(['ocean_proximity'], \n","        axis = 1, \n","        inplace = True\n","        )\n","\n","# 4. join our temp dataframe with our original to create a single dataframe\n","df = pd.concat([df, df_dummies], \n","               axis=1\n","               )\n","\n","# Show the new state after one-hot encoding\n","print('\\n\\nThe state of our dataset after one-hot encoding:')\n","display(df.sample(5))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5an4RoLEN0P","colab_type":"text"},"source":["## B. Removing Unwanted Features\n","When working with many datasets, there are likely to be features that are not useful for the goal at hand. In our case, we are trying to predict the median house value, so we must look at the features we have and decide if there are any that will not help us in that goal.\n","\n","Let's first take a look at the correlations between our target variable and the other variables to get an idea of which features will be useful in predicting a price. We will use our brand new **```make_heatmap```** function to build it."]},{"cell_type":"code","metadata":{"id":"lP-jbjuqDvfG","colab_type":"code","colab":{}},"source":["# Call on your new function to create a heatmap of the current dataframe\n","make_heatmap(df)\n","\n","# Get a list of column names and decide which ones we can eliminate (if any)\n","display(list(df.columns))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwZ7ZaDIE57x","colab_type":"text"},"source":["Based upon our goal, the following features can probably be dropped:\n","- **```population```** - Appears to have almost no correlation, and it appears to be well represented in other features (such as total rooms, total bedrooms, etc.)\n","- **```households```** - Perfect correlation with total bedrooms, and very correlated with population, so this appears to be well-represented elsewhere\n","- **```proximity_to_store```** - This appears to be uniformly distributed and, thus, of little use for predictive purposes\n","- **```ISLAND```** - Almost no correlation with our target variable (little better than guessing)\n","\n","> **NOTE:** If you would like to change which features are dropped and/or kept, simply modify the code below by adding or removing the hash in front of the desired feature. The features with hash marks are kept and those without them are dropped. **Remember** that you will also need to run the **environment setup commands in section III** (and all cells below it) if you modify these parameters."]},{"cell_type":"code","metadata":{"id":"8qcZkojVsPsV","colab_type":"code","colab":{}},"source":["# Drop those columns that we will not use for training / testing\n","df.drop([#'longitude',\n","         #'latitude',\n","         #'housing_median_age',\n","         #'total_rooms',\n","         #'total_bedrooms',\n","         'population',\n","         'households',\n","         #'median_income',\n","         #'median_house_value',\n","         'proximity_to_store',\n","         #'INLAND',\n","         'ISLAND',\n","         #'NEAR BAY',\n","         #'NEAR OCEAN'\n","         ],\n","        axis = 1,\n","        inplace = True\n","        )\n","\n","# List the current features in our dataset\n","list(df.columns)\n","print('\\n\\n')\n","\n","# List the data types for each column\n","df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dW1MbVVU_9eL","colab_type":"text"},"source":["## C. Missing Data\n","When first analyzing our data, we discovered that there were some \"NaN\" values (null data) in the **```total_bedrooms```** feature. Before proceeding to build our model, we must first figure out what to do with the records that contain those missing values.\n","\n","Let's quickly count the missing values again"]},{"cell_type":"code","metadata":{"id":"C14Kman52-GA","colab_type":"code","colab":{}},"source":["# Get a count of \"NaN\" or missing values by feature\n","print('Count of missing values by feature:\\n')\n","display(df.isnull().sum(axis=0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nnl_YzCr5tvc","colab_type":"text"},"source":["\n","Now that we have located missing data, we have *at least* four choices:\n","1. Fill in missing values with the **mode**\n","2. Fill in missing values with the **median**\n","3. **Delete** samples with missing values\n","4. **Delete** the feature(s) containing the missing data\n","\n","**Options 1 & 2** will depend upon the type of data being manipulated. \n","- For **categorical data**, it is generally okay to use the **mode** as this represents the most frequently encountered value.\n","- For **continuous data**, it is generally okay to use the **median** so as to avoid the influence of any outlier data (using the mean will allow this influence)\n","\n","**Option 3** (deleting samples) is considered the **last resort** as it will cause us to lose valuable data that can help our model perform better. If the number of samples to be deleted are insignificant compared to the overall size of the dataset, then this might not be a problem. Often, however, our data is limited and every sample counts, so we will want to keep every bit of it that we can.\n","\n","**Option 4** (deleting features with missing data) is also worth considering **if and only if the feature is not consequential for the model**. In other words, if we do not feel that the data in this feature will help us to train a more accurate prediction model, then we can just delete the feature altogether and be done with it.\n","\n","> **NOTE:** There are actually a host of different methods for dealing with missing data, and they range greatly in their complixity. **The method one chooses will ultimately depend upon the type of data being used and the goal(s) of the model.** For the purposes of this lesson, however, we have covered 4 possibilities simply to demonstrate that there are easy ways to manage datasets with missing values. For more information on dealing with missing data, please see the following links:\n","- [Towards Data Science: Compensating for missing values](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)\n","- [Geeks for Geeks: Excellent yet simple tutorial for working with missing data](https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/)\n","- [Pandas Documentation: Working with missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n","- [Machine Learning Mastery: Detailed options for dealing with missing data](https://machinelearningmastery.com/handle-missing-data-python/)\n","\n","```\n","```\n","In this case, we have 207 values missing out of 20,640 which represents about 1% of our data. For the purposes of this exercise, we will elect to keep that data and fill those missing values.\n","\n","> **Question:** Which method should we use to fill in the missing data?"]},{"cell_type":"markdown","metadata":{"id":"2zmMWnAn6P_Y","colab_type":"text"},"source":["#### (1). Working with missing data"]},{"cell_type":"code","metadata":{"id":"MsLMUaAB6Yvh","colab_type":"code","colab":{}},"source":["# Let's look at the current state of our data prior to changes\n","print('-------------------- BEFORE CHANGES --------------------')\n","print('\\nData types by column:')\n","print(df.dtypes)\n","print('\\nCount of null values per feature:')\n","print(df.isnull().sum(axis=0))\n","print('\\nMedian value for total_bedrooms: ', df['total_bedrooms'].median())\n","print('\\nMean value for total_bedrooms: ', df['total_bedrooms'].mean(), '\\n\\n')\n","\n","# --------------- Now lets apply the changes we want to see ------------------\n","# First, create a variable to hold the median value\n","tb_med = df['total_bedrooms'].median(axis=0)\n","\n","# Next, we want to fill the \"NaN\" values with the median value of the column (with current 'NaN' columns skipped in the calculation)\n","df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n","\n","# Change the datatype of 'total_bedrooms' to 'int' (since we cannot really have partial bedrooms)\n","df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n","\n","# Next, we want to verify that our columns are all of the correct length and data type (ensuring that we can perform calculations on them later)\n","print('-------------------- AFTER CHANGES --------------------')\n","print('\\nData types by column:')\n","print(df.dtypes)\n","print('\\nCount of null values per feature:')\n","print(df.isnull().sum(axis=0))\n","\n","# Finally, we want to ensure that our median value is still accurate (should be the same as above including the replaced 'NaN' values)\n","print('\\nMedian value for total_bedrooms: ', df['total_bedrooms'].median())\n","print('\\nMean value for total_bedrooms: ', df['total_bedrooms'].mean())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxD9xTIGDNft","colab_type":"text"},"source":["## D. Working with Outlier Data\n","\n","Now that we have filled in our missing data, let's take a look at our remaining features' distributions. This will help us to get an idea of other changes that we might need to make.\n","\n","First, let's look at some graphs (histograms and boxplots).\n","\n",">Websites Referenced:\n","- [Towards Data Science: Working with Outlier Data](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n","- [Medium.com: Standardize and Normalize Data](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)"]},{"cell_type":"code","metadata":{"id":"Ir9wN-IaJQTV","colab_type":"code","colab":{}},"source":["# Look at distributions of median_income and proximity_to_store\n","\n","# Create a \"figure\" that can hold multiple plots\n","fig, ((ax1, ax2, ax3, ax4, ax5), (ax6, ax7, ax8, ax9, ax10)) = plt.subplots(nrows = 2, ncols = 5, figsize = (20, 5), sharey = False)\n","\n","df.hist('longitude', ax = ax1)\n","df.hist('latitude', ax = ax2)\n","df.hist('housing_median_age', ax = ax3)\n","df.hist('total_rooms', ax = ax4)\n","df.hist('total_bedrooms', ax = ax5)\n","df.hist('median_income', ax = ax6)\n","df.hist('median_house_value', ax = ax7)\n","sns.countplot(x = df['INLAND'], ax = ax8)\n","sns.countplot(x = df['NEAR BAY'], ax = ax9)\n","sns.countplot(x = df['NEAR OCEAN'], ax = ax10)\n","\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDmCawcbNpsY","colab_type":"code","colab":{}},"source":["# Create a \"figure\" that can hold multiple plots\n","fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(nrows = 1, ncols = 7, figsize = (25, 5), sharey = False)\n","\n","sns.boxplot(x=df['longitude'], ax = ax1)\n","sns.boxplot(x=df['latitude'], ax = ax2)\n","sns.boxplot(x=df['housing_median_age'], ax = ax3)\n","sns.boxplot(x=df['total_rooms'], ax = ax4)\n","sns.boxplot(x=df['total_bedrooms'], ax = ax5)\n","sns.boxplot(x=df['median_income'], ax = ax6)\n","sns.boxplot(x=df['median_house_value'], ax = ax7)\n","sns.lmplot(x = 'median_house_value', y = 'INLAND', logistic = True, data = df, ci = None)\n","sns.lmplot(x = 'median_house_value', y = 'NEAR BAY', logistic = True, data = df, ci = None)\n","sns.lmplot(x = 'median_house_value', y = 'NEAR OCEAN', logistic = True, data = df, ci = None)\n","\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GFkN0CU7kGMv","colab_type":"text"},"source":["### (1). Eliminating outlier data with Z-score\n","\n","There are a number of ways in which to deal with outlier data, but one of the more common ones is utilizing a **z-score** which gives us the ability to exclude items based on standard deviation. This, in turn, will allow us to exclude data that falls outside of standard deviation boundaries of our choice (thus eliminating outliers). We will then look at our data again and see if this has helped."]},{"cell_type":"code","metadata":{"id":"PXRvD85LkVX5","colab_type":"code","colab":{}},"source":["# ------------------------------------- Calculate the z-score for all data -------------------------------------\n","# Get the absolute z-score value for our dataset\n","z = np.abs(stats.zscore(df))\n","\n","# Finally, we remove those values with standard deviations greater than 3\n","dfz = df[(z < 3).all(axis = 1)]\n","\n","# Create a \"figure\" that can hold multiple plots\n","fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(nrows = 1, ncols = 7, figsize = (25, 5), sharey = False)\n","\n","sns.boxplot(x=dfz['longitude'], ax = ax1)\n","sns.boxplot(x=dfz['latitude'], ax = ax2)\n","sns.boxplot(x=dfz['housing_median_age'], ax = ax3)\n","sns.boxplot(x=dfz['total_rooms'], ax = ax4)\n","sns.boxplot(x=dfz['total_bedrooms'], ax = ax5)\n","sns.boxplot(x=dfz['median_income'], ax = ax6)\n","sns.boxplot(x=dfz['median_house_value'], ax = ax7)\n","\n","# Also show the data description to get accurate values for the quartiles\n","display(dfz.describe())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBDGPfEnlAmq","colab_type":"text"},"source":["### (2). Eliminating outlier data with Interquartile Range\n","\n","Another way to work with outliers is to eliminate those values that fall outside of our IQR (Interquartile Range). This will work somewhat similarly to the z-score, but our threshold for eliminating outlier data will be based upon quartiles.\n","\n","> Websites referenced:\n","- [Towards Data Science: Detect and Remove Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n","\n","After running the code below, we will have the IQR necessary to calculate our threshold for outliers. The formula is:\n","\n","- ```lower boundary = q1 - (1.5 * iqr)```\n","- ```upper boundary = q3 + (1.5 * iqr)```\n","\n","The subsequent code will then look for and remove all samples that fall outside these boundaries and leave us (hopefully) with substantially fewer outliers. \n","\n","We will then compare both methods (z-score and IQR) against the original dataset to see which one gives us the most usable data."]},{"cell_type":"code","metadata":{"id":"S2Za0cQflbfb","colab_type":"code","colab":{}},"source":["# Create a dataset with outliers removed via IQR method\n","\n","# First, establish quantile variables\n","q1 = df.quantile(0.25)\n","q3 = df.quantile(0.75)\n","iqr = q3 - q1\n","\n","# Display the interquartile range for our features\n","print(iqr)\n","\n","# Remove outliers with IQR method\n","\n","# Establish variables\n","q1 = df.quantile(0.25)\n","q3 = df.quantile(0.75)\n","iqr = q3 - q1\n","lower = q1 - (1.5 * iqr)\n","upper = q3 + (1.5 * iqr)\n","\n","# Remove samples outside of the upper and lower boundaries\n","dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n","\n","# Compare housing_median_age\n","fig, (ax1a, ax1b, ax1c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['longitude'],  ax = ax1a, color='g')\n","sns.boxplot(x=dfz['longitude'],  ax = ax1b)\n","sns.boxplot(x=df['longitude'],  ax = ax1c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare total_rooms\n","fig, (ax2a, ax2b, ax2c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['latitude'],  ax = ax2a, color='g')\n","sns.boxplot(x=dfz['latitude'],  ax = ax2b)\n","sns.boxplot(x=df['latitude'],  ax = ax2c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare total_bedrooms\n","fig, (ax3a, ax3b, ax3c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['housing_median_age'], ax = ax3a, color='g')\n","sns.boxplot(x=dfz['housing_median_age'], ax = ax3b)\n","sns.boxplot(x=df['housing_median_age'], ax = ax3c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare population\n","fig, (ax4a, ax4b, ax4c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['total_rooms'], ax = ax4a, color='g')\n","sns.boxplot(x=dfz['total_rooms'], ax = ax4b)\n","sns.boxplot(x=df['total_rooms'], ax = ax4c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare households\n","fig, (ax5a, ax5b, ax5c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['total_bedrooms'], ax = ax5a, color='g')\n","sns.boxplot(x=dfz['total_bedrooms'], ax = ax5b)\n","sns.boxplot(x=df['total_bedrooms'], ax = ax5c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare median_income\n","fig, (ax6a, ax6b, ax6c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['median_income'], ax = ax6a, color='g')\n","sns.boxplot(x=dfz['median_income'], ax = ax6b)\n","sns.boxplot(x=df['median_income'], ax = ax6c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')\n","\n","\n","# Compare median_house_value\n","fig, (ax7a, ax7b, ax7c) = plt.subplots(nrows = 3, ncols = 1, figsize = (10, 5), sharex = True)\n","sns.boxplot(x=dfi['median_house_value'], ax = ax7a, color='g')\n","sns.boxplot(x=dfz['median_house_value'], ax = ax7b)\n","sns.boxplot(x=df['median_house_value'], ax = ax7c, color='grey')\n","plt.show()\n","plt.close()\n","print('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEceAPqlnBBo","colab_type":"text"},"source":["Everything appears to be better using the IQR method with the exception of **```median_house_value```**, which we might want to inspect a little more closely.Let's look at the new data in a distribution plot (histogram) to see what it looks like."]},{"cell_type":"code","metadata":{"id":"CGHu_s_ZnALc","colab_type":"code","colab":{}},"source":["# Compare median_house_price via histograms\n","fig, (ax8a, ax8b, ax8c) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 5), )\n","ax8a = sns.distplot(dfi['median_house_value'],  \n","                    kde = True,\n","                    hist = True,\n","                    bins = 50, \n","                    color = 'green',\n","                    ax = ax8a,\n","                    hist_kws={'alpha':1}, \n","                    )\n","ax8b = sns.distplot(dfz['median_house_value'], \n","                    kde = True,\n","                    hist = True,\n","                    bins = 50,\n","                    ax = ax8b,\n","                    hist_kws={'alpha':1}\n","                    )\n","ax8c = sns.distplot(df['median_house_value'],  \n","                    kde = True,\n","                    hist = True,\n","                    bins = 50, \n","                    color = 'grey',\n","                    ax = ax8c,\n","                    hist_kws={'alpha':1}\n","                    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p9RciXN22Z0L","colab_type":"text"},"source":["Upon further investigation, it appears that the IQR method gives us the best resulting dataset with which to move forward and build our model. It eliminates many of the outlier problems that exist while also handling the problem of the **```median_house_value```** \"wall\" problem at ~500,000. \n","\n","Let's compare the shape and descriptions of our datasets one last time before moving onto building our model:"]},{"cell_type":"code","metadata":{"id":"q4ummlhKfklT","colab_type":"code","colab":{}},"source":["# Compare the results of each outlier removal method\n","print('IQR Dataframe shape (rows, columns):')\n","display(dfi.shape)\n","print('\\nZ-Score Dataframe shape (rows, columns):')\n","display(dfz.shape)\n","print('\\nOriginal Dataframe shape (rows, columns):')\n","display(df.shape)\n","print('\\nIQR Dataframe description:')\n","display(dfi.describe())\n","print('\\nZ-Score Dataframe description:')\n","display(dfz.describe())\n","print('\\nOriginal Dataframe description:')\n","display(df.describe())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M3RdmZN-r-CT","colab_type":"text"},"source":["# IV. Building Machine Learning Models\n","At this point, we have now cleaned our dataset and are ready to build a machine learning model. We want to predict housing prices and we have labeled data, so we will build a supervised learning model with both **linear regression** and a **regression tree** (which is a form of decision tree). \n","\n","Before we begin building a model, **there are a few definitions that we should cover** so that you can better understand what is happening in the code below.\n","\n","```\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"Jz-1cwkbsPV1","colab_type":"text"},"source":["## A. Key Definitions\n","- **[Estimator / Model](https://scikit-learn.org/stable/user_guide.html)**\n","  - A method that neatly packages up all of the low-level training code. \n","    - This method will loop through a training set using a [score method](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation) to determine \"loss\" (deviation from the predicted value).\n","    - A **linear regression estimator**, for example, will try to find a single line that has the least amount of deviation from all target values.\n","  - Estimators allow us to **configure** training loops rather than coding everything from the ground up.\n","  - [Examples](https://scikit-learn.org/stable/user_guide.html) include **linear regressors**, **decision trees**, **k-Means Clustering**, etc.\n","  >**Note:** The estimators called by sklearn will calculate error/loss/cost exactly rather than using Gradient Descent to minimize it incrementally via \"epochs\". See the exact description and calculations at the following links:\n","    - [Data Science Exchange: Epochs discussion](https://datascience.stackexchange.com/questions/29044/how-many-epochs-does-fit-method-run)\n","    - [Machine Learning Mastery: Gradient Descent](https://machinelearningmastery.com/gradient-descent-for-machine-learning/).\n","\n","\n","```\n","\n","```\n","\n","\n","- **[Scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation)**\n","  - A method that measures \"loss\" (a.k.a. \"cost\" or \"error\") when training an estimator\n","  - Can be measured in various ways such as **mean squared error**, **median absolute error**, etc.\n","\n","```\n","\n","```\n","\n","\n","- **Splitting Data**\n","  - Refers to the act of separating data into **training** and **testing** (and possibly **validation**) sets\n","  - The parameter **```test_size```** dictates the proportion of data to be set aside as the test set (with the remaining being the training set)\n","    - Splits are typically 70/30, 80/20 or somewhere between for training/test sets\n","  - Accomplished with the **[train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** method which will return 4 arrays of data (X_train, X_test, y_train, y_test)\n","  - These 4 arrays are then used when **fitting** the data to the esimator\n","  - Data can (and should be) **shuffled** with this method (to reduce chances of bias)\n","  > **Question**: Why do we need to split our data into training and testing sets?\n","\n","```\n","\n","```\n","\n","\n","- **Hyperparameters**\n","  - These are values that we define **before** training a model. Regular parameters learn and change during the training process, but hyperparameters cannot \"learn\" and must be set beforehand.\n","  - Includes things like **```n_estimators```**, **```max_depth```**, etc.\n","  - We can \"tune\" hyperparameters in order to get better model performance.\n","  - We can also employ a \"GridSearchCV\" function that will analyze all combinations of hyperparameters and reveal which combinations produced the best performance (shown at the end of the lesson). We use this function to reduce the work necessary to create a \"good\" model.\n","\n","```\n","\n","```\n","\n","- **[Fitting](https://stackoverflow.com/questions/45704226/what-does-fit-method-in-scikit-learn-do) a model/estimator**\n","  - This refers to the act of actually **training** the estimator on the training set of data (X_train, y_train)\n","  - The estimator loops through the data (using the hyperparameter settings), measures loss with the scoring method and produces a **trained model**.\n","  - The trained model is then run against the test set to evaluate the accuracy.\n","  - The error rates returned can be used to determine if:\n","    - The model needs to be further refined\n","    - Data needs to be further engineered\n","    - The model is ready to deploy.\n"]},{"cell_type":"markdown","metadata":{"id":"Jg6wcluVtSsy","colab_type":"text"},"source":["## B. Basic steps for Building a Machine Learning Model\n","1. **Import necessary libraries**\n","  - In order to train a model, we must identify the library that contains it and call the import function so that we can use it in our code (```from sklearn import linear_model```). For this lesson, we also need to import some other libraries such as ```preprossesing``` and ```Counter```. \n","---\n","2. **Identify the Target Variable**\n","  - We must then **identify our target variable**. Basically, we can choose any feature that we would like to predict as our target, and the remaining features will be used to try and determine an effective model that can predict any value of the target feature. \n","  - For this lesson, we will use **```median_house_value```** as our target variable, meaning that we are trying to predict a house's value based upon the factors contained in the other features (```total_rooms, total_bedrooms, median_income```, etc). We can easily change this up later, however, if we want to try and predict another variable instead.\n","---\n","3. **Split the Dataset into Training and Testing sets**\n","  - In this step, we set our training and testing sets, determine the test set size and shuffle our data (to prevent bias).\n","  - **NOTE:** It's vitally important to **shuffle** the dataset at this point in order to randomize the data and prevent bias from entering our model.\n","---\n","4. **Fit (or \"train\") the Model**\n","  - At this point, we call our estimator and \"fit\" it to our training arrays (X_train, y_train)\n","    - ```estimator.fit(X_train, y_train)```\n","  - Once trained, we then run the test data through the model while also gathering the error data through the sklearn.metrics library\n","    - ```this_err = metrics.median_absolute_error(y_test, e.predict(X_test))```\n","  - As those errors are gathered, we append them to the error array that we initialized in step C.\n","    - ```errvals = np.append(errvals, this_err)```\n","---\n","5. **Visualize the data**\n","- Once everything is completed, we then create a simple vertical bar graph to show how our model(s) performed."]},{"cell_type":"markdown","metadata":{"id":"_SDIfBkLvyXr","colab_type":"text"},"source":["## C. Models\n","We can now begin building our models (linear regression, regression tree and gradient boost models).\n","\n","We can setup our environment with the code below. This code will create **three datasets** with which you can work:\n","- **df:** This is our original data (though the 'NaN' values have been filled with median values\n","- **dfz:** This is a dataset created using the z-score method\n","- **dfi:** This is a dataset created using the IQR method"]},{"cell_type":"code","metadata":{"id":"CkgJsk0u-YCu","colab_type":"code","colab":{}},"source":["# ------------------------------------------ SETUP THE ENVIRONMENT ------------------------------------------\n","# Import libraries\n","import pandas as pd\n","import seaborn as sns; sns.set()\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","import missingno as msno\n","from IPython.display import display\n","from sklearn.model_selection import train_test_split \n","from sklearn import ensemble\n","from scipy import stats\n","from sklearn import linear_model\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import learning_curve,GridSearchCV\n","from sklearn.model_selection import cross_val_predict\n","from sklearn import preprocessing\n","from collections import Counter\n","from sklearn.externals import joblib\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n","\n","df = pd.read_csv(url)\n","#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n","\n","\n","# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n","df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n","\n","\n","# --------------------------------------------- FUNCTIONS ---------------------------------------------\n","def make_heatmap(df = 'df'):\n","  corr = df.corr()\n","  plt.figure(figsize=(15 ,9))\n","  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n","  plt.show()\n","  plt.close()\n","\n","\n","# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n","df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n","df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n","df.drop(['ocean_proximity'], axis = 1, inplace = True)\n","df = pd.concat([df, df_dummies], axis=1)\n","\n","\n","# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n","df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n","\n","\n","# ------------------------------------- FIX MISSING DATA -------------------------------------\n","tb_med = df['total_bedrooms'].median(axis=0)\n","df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n","df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n","\n","\n","# ------------------------------------- Z-SCORE -------------------------------------\n","z = np.abs(stats.zscore(df))\n","dfz = df[(z < 3).all(axis = 1)]\n","\n","# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n","q1 = df.quantile(0.25)\n","q3 = df.quantile(0.75)\n","iqr = q3 - q1\n","lower = q1 - (1.5 * iqr)\n","upper = q3 + (1.5 * iqr)\n","dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n","dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1, inplace = True)  # After applying IQR, the following features are now empty and can be dropped\n","\n","print('Original Heatmap')\n","make_heatmap(df)\n","\n","print('Z-Score Heatmap')\n","make_heatmap(dfz)\n","\n","print('IQR Heatmap')\n","make_heatmap(dfi)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaytmQ1it_aQ","colab_type":"text"},"source":["### (1). Linear Models"]},{"cell_type":"code","metadata":{"id":"_-UHhJZLLdJ6","colab_type":"code","colab":{}},"source":["# ---------------------------- A: Import Libraries ----------------------------\n","# NOTE: We have imported all necessary libraries in the code cell above\n","\n","# ---------------------------- B: Set Variables ----------------------------\n","# Set the Random State variable (for use in the splitting function)\n","dfx = dfi.copy()\n","rs = 20\n","\n","# Create an array to hold our error values during training\n","errvals = np.array([])\n","\n","# Here we will try a number of different linear regression models and then\n","#   compare their performance against one another.\n","estimators = [linear_model.LinearRegression(), \n","              linear_model.Ridge(),\n","              linear_model.Lasso(),\n","              linear_model.ElasticNet(),\n","              linear_model.BayesianRidge(),\n","              linear_model.OrthogonalMatchingPursuit()\n","              ]\n","\n","# Create labels that match the linear regressors for use in the graph\n","estimators_labels = np.array(['Linear',\n","                              'Ridge', \n","                              'Lasso', \n","                              'ElasticNet', \n","                              'BayesRidge', \n","                              'OMP'\n","                              ]\n","                             )\n","\n","\n","# ---------------------------- C: Identify Target Variable ----------------------------\n","# Create a copy of the dataframe with only the desired features (dropping the target feature)\n","features_dfx = dfx.copy()\n","features_dfx.drop(['median_house_value'], \n","                 axis = 1,\n","                 inplace = True\n","                 )\n","\n","# Create X and y arrays to hold the independent (X) and dependent (y) variables\n","X = features_dfx.values\n","y = dfx['median_house_value'].values\n","\n","# ---------------------------- D: Split Dataset ----------------------------\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.3,\n","                                                    shuffle = True,\n","                                                    random_state = rs\n","                                                    )\n","\n","\n","# ---------------------------- E: Fit (i.e. 'train') the model ----------------------------\n","# Create a temporary variable for use in the following loop\n","i = 0\n","\n","# Loop through each linear regression estimator and: \n","for estimator in estimators:\n","    estimator.fit(X_train, y_train)                                       # (1) train the model (fit the model to the training data)\n","    training_score = estimator.score(X_train, y_train)                    # (2) determine the R^2 score (measure of variance of prediction from the mean) for training and test sets\n","    testing_score = estimator.score(X_test, y_test)                   \n","    y_predicted_score = estimator.predict(X_test)                         # (3) record the loss of the trained estimator by applying the scoring mechanism to the test set                                       \n","    this_err = metrics.median_absolute_error(y_test, y_predicted_score)   # (4) append the error value to the 'errvals' array\n","    print(estimators_labels[i],                                           # (5) print the relevant metrics for analysis\n","          'Median Absolute Error on Test Set: %0.2f' % this_err,                                \n","          '\\n       Mean Absolute Error on Test Set: %.2f' % metrics.mean_absolute_error(y_test, y_predicted_score),  \n","          '\\n       Training set accuracy (R^2): ', training_score,\n","          '\\n       Testing set accuracy (R^2): ', testing_score\n","          ),                    \n","    errvals = np.append(errvals, this_err)                                # (6) append this_err to the errvals[] array (for use in plotting)\n","    print('-' * 80)                                        \n","    i += 1                                                                # (7) add 1 to the variable 'i' iterate through the estimator_labels list\n","\n","# Plot a bar chart with sorted values from errvals (mean_absolute_error for each linear regression estimator)\n","pos = np.arange(errvals.shape[0])\n","srt = np.argsort(errvals)\n","plt.figure(figsize=(8,4))\n","plt.bar(pos, errvals[srt], align='center')\n","plt.xticks(pos, estimators_labels[srt])\n","plt.xlabel('Estimator')\n","plt.ylabel('Median Absolute Error')\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rXelFmJuKez","colab_type":"text"},"source":["### (2). Single Regression Tree"]},{"cell_type":"code","metadata":{"id":"FO67e2W7MNSB","colab_type":"code","colab":{}},"source":["# ---------------------------- A: Import Libraries ----------------------------\n","# NOTE: We have imported all necessary libraries in the code cell above\n","\n","# ---------------------------- B: Set Variables ----------------------------\n","# Define the random state variable (which influences the splitting function to ensure that\n","#   we are splitting our dataset with the same value over multiple runs)\n","dfx = dfi.copy()\n","rs = 20\n","\n","# Define the desired algorithm and congigure the hyperparameters\n","estimator = DecisionTreeRegressor(criterion='mae',  # can also be 'mse' or other values\n","                                  max_depth = None, \n","                                  min_samples_split = 4,\n","                                  min_samples_leaf = 4,\n","                                  max_features = 1.0,\n","                                  random_state = rs\n","                                  )\n","\n","\n","# ---------------------------- C: Identify Target Variable ----------------------------\n","# Create a copy of the dataframe with only the desired features (dropping the target feature)\n","features_dfx = dfx.copy()\n","features_dfx.drop(['median_house_value'], \n","                 axis = 1,\n","                 inplace = True\n","                 )\n","\n","# Create X and y arrays to hold the independent (X) and dependent (y) variables\n","X = features_dfx.values\n","y = dfx['median_house_value'].values\n","\n","\n","# ---------------------------- D: Split Dataset ----------------------------\n","# Split the dataset (we can change the test set size here)\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size = 0.3,\n","                                                    shuffle = True\n","                                                    )\n","\n","\n","# ---------------------------- E: Fit (i.e. 'train') the model ----------------------------\n","# Train the model using our training sets (X_train, y_train)\n","estimator.fit(X_train, y_train)\n","\n","\n","# ---------------------------- Analysis / Visualization ----------------------------\n","# Analyze the results (mean/median absolute error in the model)\n","training_score = estimator.score(X_train, y_train)\n","testing_score = estimator.score(X_test, y_test)\n","y_predicted_score = estimator.predict(X_test)\n","\n","# Have a look at R sq to give an idea of the fit ,\n","# Explained variance score: 1 is perfect prediction\n","print('Training set mean accuracy (R^2): ',training_score)\n","# Explained variance score: 1 is perfect prediction\n","print('Test set mean accuracy (R^2): %.2f' % testing_score)\n","# The mean absolute error\n","print(\"Mean Absolute Error on Test Set: %.2f\" % metrics.mean_absolute_error(y_test, y_predicted_score))\n","\n","\n","# Run the model against the test data to produce a graph\n","fig, ax = plt.subplots()\n","ax.scatter(y_test, y_predicted_score, edgecolors=(0, 0, 0), alpha=0.5)\n","ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n","ax.set_xlabel('Actual')\n","ax.set_ylabel('Predicted')\n","ax.set_title(\"Ground Truth vs Predicted\")\n","plt.show()\n","\n","# Plot feature importance\n","feature_importance = estimator.feature_importances_\n","\n","# Make importances relative to max importance\n","feature_importance = 100.0 * (feature_importance / feature_importance.max())\n","sorted_idx = np.argsort(feature_importance)\n","pos = np.arange(sorted_idx.shape[0]) + .5\n","plt.subplot(1, 2, 2)\n","plt.barh(pos, feature_importance[sorted_idx], align='center')\n","plt.yticks(pos, features_dfx)\n","plt.xlabel('Relative Importance')\n","plt.title('Variable Importance')\n","plt.show()\n","plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3b9fD53_uT_4","colab_type":"text"},"source":["### (3). Gradient Boosting\n","### From *Machine Learning For Absolute Beginners* by Oliver Theobald\n","```\n","\n","model = ensemble.GradientBoostingRegressor(\n","                                           n_estimators = 150, \n","                                           learning_rate = 0.1, \n","                                           max_depth = 30, \n","                                           min_samples_split = 4, \n","                                           min_samples_leaf = 6, \n","                                           max_features = 0.6, \n","                                           loss = 'huber'\n","                                          )\n","```\n","\n","The first line is the algorithm itself (gradient boosting) and comprises just one line of code. The code below dictates the hyperparameters for this algorithm. \n","- **n_estimators** represents how many decision trees to be used. Remember that a high number of trees generally improves accuracy (up to a certain point) but will extend the model’s processing time. Above, I have selected 150 decision trees as an initial starting point. \n","- **learning_rate** controls the rate at which additional decision trees influence the overall prediction. This effectively shrinks the contribution of each tree by the set learning_rate. Inserting a low rate here, such as 0.1, should help to improve accuracy. \n","- **max_depth** defines the maximum number of layers (depth) for each decision tree. If “None” is selected, then nodes expand until all leaves are pure or until all leaves contain less than min_samples_leaf. Here, I have chosen a high maximum number of layers (30), which will have a dramatic effect on the final result, as we’ll soon see. \n","- [**min_samples_split**](https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre) defines the minimum number of samples required to execute a new binary split. For example, min_samples_split = 10 means there must be ten available samples in order to create a new branch.\n","- [**min_samples_leaf**](**min_samples_split**) represents the minimum number of samples that must appear in each child node (leaf) before a new branch can be implemented. This helps to mitigate the impact of outliers and anomalies in the form of a low number of samples found in one leaf as a result of a binary split. For example, min_samples_leaf = 4 requires there to be at least four available samples within each leaf for a new branch to be created. \n","- **max_features** is the total number of features presented to the model when determining the best split.\n","- **loss** calculates the model's error rate. For this exercise, we are using huber which protects against outliers and anomalies. Alternative error rate options include ls (least squares regression), lad (least absolute deviations), and quantile (quantile regression). Huber is actually a combination of least squares regression and least absolute deviations.\n","\n","\n","  >Theobald, Oliver. Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition) (Machine Learning For Beginners Book 1) (pp. 139-141). Scatterplot Press. Kindle Edition.\n","\n","I also referenced the following website(s) and adapted code in order to graph this model:\n","- [Scikit-Learn Documentation: Ensemble Gradient Boosting Visualization](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py)\n","- [Sharp Sight Labs: Numpy Zeros Tutorial](https://www.sharpsightlabs.com/blog/numpy-zeros-python/)"]},{"cell_type":"code","metadata":{"id":"777Iex2pM7lh","colab_type":"code","colab":{}},"source":["# ---------------------------- A: Import Libraries ----------------------------\n","# NOTE: We have imported all necessary libraries in the code cell above\n","\n","# ---------------------------- B: Set Variables ----------------------------\n","dfx = dfi.copy()\n","rs = 20\n","\n","# -- Create a dictionary with key/value pairs that we can use in our function calls\n","params = {'n_estimators': 500,\n","          'learning_rate': .01,\n","          'max_depth': 5,\n","          'min_samples_split': 6,\n","          'min_samples_leaf': 4,\n","          'max_features': 0.6,\n","          # Options for 'loss': huber, ls (least squares), lad (least absolute deviations) and quantile (quantile regression)\n","          'loss': 'huber'\n","          }\n","\n","## Use the params defined above in the GradientBoostingRegressor estimator call\n","estimator = ensemble.GradientBoostingRegressor(**params)\n","\n","\n","# ---------------------------- C: Identify Target Variable ----------------------------\n","# Create a copy of the dataframe with only the desired features (dropping the target feature)\n","features_dfx = dfx.copy()\n","features_dfx.drop(['median_house_value'], \n","                  axis = 1,\n","                  inplace = True\n","                  )\n","\n","\n","# ---------------------------- D: Split Dataset ----------------------------\n","# Create X and y arrays to hold the independent (X) and dependent (y) variables\n","X = features_dfx.values\n","y = dfx['median_house_value'].values\n","\n","\n","# Split the dataset (we can change the test set size here)\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size = 0.3,\n","                                                    random_state = rs,\n","                                                    shuffle = True\n","                                                    )\n","\n","\n","# ---------------------------- E: Fit (i.e. 'train') the model ----------------------------\n","estimator.fit(X_train, y_train) \n","\n","\n","# ---------------------------- Analysis / Visualization ----------------------------\n","training_score = estimator.score(X_train, y_train)\n","testing_score = estimator.score(X_test, y_test)\n","y_predicted_score = estimator.predict(X_test)\n","y_staged_predicted_score = estimator.staged_predict(X_test)\n","\n","# Print out the various metrics that we have collected on our test sets\n","print('Training set mean accuracy (R^2): ',training_score)\n","print('Test set mean accuracy (R^2): %.2f' % testing_score)\n","print(\"Mean Absolute error: %.2f\" % metrics.mean_absolute_error(y_test, y_predicted_score))\n","\n","\n","# Plot the training/testing accuracy\n","# Create an empty array of zeros based on the value in 'n_estimators' key\n","test_score = np.zeros(shape = (params['n_estimators'],),\n","                      dtype=np.float64\n","                      )\n","\n","# Compute test set deviance (loss) at each stage and place it into the array\n","#  based on the predicted value (y_pred) against the actual value (y_test)\n","for i, y_pred in enumerate(y_staged_predicted_score):\n","    test_score[i] = estimator.loss_(y_test, y_pred)\n","\n","plt.figure(figsize=(20, 10))\n","plt.subplot(1, 2, 1)\n","plt.title('Deviance')\n","plt.plot(np.arange(params['n_estimators']) + 1, \n","         estimator.train_score_, 'b-',\n","         label='Training Set Deviance'\n","         )\n","plt.plot(np.arange(params['n_estimators']) + 1,\n","         test_score, 'r-',\n","         label='Test Set Deviance'\n","         )\n","plt.legend(loc='upper right')\n","plt.xlabel('Boosting Iterations')\n","plt.ylabel('Deviance')\n","\n","\n","# Plot feature importance\n","feature_importance = estimator.feature_importances_\n","\n","# Make importances relative to max importance\n","feature_importance = 100.0 * (feature_importance / feature_importance.max())\n","sorted_idx = np.argsort(feature_importance)\n","pos = np.arange(sorted_idx.shape[0]) + .5\n","plt.subplot(1, 2, 2)\n","plt.barh(pos, feature_importance[sorted_idx], align='center')\n","plt.yticks(pos, features_dfx)\n","plt.xlabel('Relative Importance')\n","plt.title('Variable Importance')\n","plt.show()\n","\n","# Save the model so that we can use it later\n","joblib.dump(estimator, 'ca_housing_trained_model.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BG5ALz-uO4L3","colab_type":"text"},"source":["# V. Moving Forward"]},{"cell_type":"markdown","metadata":{"id":"mnVi64CgPAnM","colab_type":"text"},"source":["## A. Additional challenges\n","If you would like to continue experimenting with this dataset, you can try some of the following activities:\n","- Try using the z-score and original datasets instead of the IQR one shown in class.\n","- Try to get the best scores possible on the Gradient Boosing section.\n","- Try to import a different, though similar, dataset that uses Australian housing data\n","  - That dataset can be found here: [Australian Housing Data FULL](https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/original/Melbourne_housing_FULL.csv)\n","  - Simply copy the link into the \"url\" variable in the code and then begin exploring it. You will have to start out with new code, as our model has different variables, but the process will be similar."]},{"cell_type":"markdown","metadata":{"id":"kXwFOsxeQZdN","colab_type":"text"},"source":["## B. Additional Reading and Tools\n","There are a number of different paths forward, but here are a few of the most useful links and options that I have found during my learning:\n","\n","\n","### Books\n","1. Machine Learning With Random Forests And Decision Trees: A Visual Guide For Beginners  \n","  - Format: E-book \n","  - Author: Scott Hartshorn \n","  - Suggested Audience: Established beginners \n","  - A short, affordable ($3.20 USD), and engaging read on decision trees and random forests with detailed visual examples, useful practical tips, and clear instructions.\n","\n","2. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems \n","  - Format: E-Book\n","  - Book Author: Aurélien Géron \n","  - Suggested Audience: All (with an interest in programming in Python, Scikit-Learn, and TensorFlow) As a popular O’Reilly Media book written by machine learning consultant Aurélien Géron, this is an excellent advanced resource for anyone with a solid foundation of machine learning and computer programming.\n","\n","### Websites\n","1. [Google AI](https://experiments.withgoogle.com/collection/ai)\n","  - Excellent resource to experiment with AI/ML concepts and experiments\n","\n","\n","### Datasets\n"]}]}