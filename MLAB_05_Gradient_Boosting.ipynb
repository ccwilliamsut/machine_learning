{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLAB-05-Gradient-Boosting.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccwilliamsut/machine_learning/blob/master/MLAB_05_Gradient_Boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy-DfSMPMWJ8",
        "colab_type": "text"
      },
      "source": [
        "# Build a Gradient Boosting Model\n",
        "\n",
        "### Basic Decision Trees\n",
        "In a previous example (Build a Decision Tree Model), we learned how to build a single decision tree. One of the primary **problems** associated with decision trees, however, is that they are **prone to overfitting / high variance** (see this [link](https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991) for further information).\n",
        "\n",
        "The basic **reason for this** is has to do with the **depth of the tree** and how the increasing specificity of splits leads to a reliance on the training data that does not reflect the real world. The primary means of combating this is to reduce the depth (i.e. ```max_depth```), but this can, in turn, introduce bias problems that will be too generalized when confronted with unseen data.\n",
        "\n",
        "### Ensemble Methods\n",
        "One means of combating the issues facing decision trees are to employ methods of ***building multiple trees** that can utilize information from each other to derive better models.\n",
        "\n",
        "### Random Forests vs. Gradient Boosting\n",
        "Both methods create multiple trees, but they take different approaches to combining that data. \n",
        "- **Random forests** use random samples of the data to create each new tree independent of any other tree. The results are then averaged to produce a final model.\n",
        "- **Gradient boosting** uses errors from previous trees to inform how each new tree is created in order to produce a \"best\" model.\n",
        "\n",
        ">Websites Refereced:\n",
        "- [Towards Data Science: Decision Trees and Random Forests](https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991)\n",
        "- [Medium: Gradient Boosting vs Random Forest](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlehaOZpFKOw",
        "colab_type": "text"
      },
      "source": [
        "## A. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moe9-fhMEMtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP ENVIRONMENT <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import ensemble\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import learning_curve,GridSearchCV\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import preprocessing\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/ccwilliamsut/machine_learning/master/absolute_beginners/data_files/modified/CaliforniaHousingDataModified.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "#df = pd.read_csv(~/Downloads/CaliforniaHousingDataModified.csv)\n",
        "\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SETUP DATA <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# --------------------------------------------- RENAME FEATURES ---------------------------------------------\n",
        "df.rename(columns = {'lattitude':'latitude', 't_rooms':'total_rooms'}, inplace=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------- ONE-HOT ENCODING ---------------------------------------------\n",
        "df['ocean_proximity'] = pd.Categorical(df['ocean_proximity'])\n",
        "df_dummies = pd.get_dummies(df['ocean_proximity'], drop_first = True)\n",
        "df.drop(['ocean_proximity'], axis = 1, inplace = True)\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "\n",
        "\n",
        "# --------------------------------------------- DROP UNWANTED FEATURES ---------------------------------------------\n",
        "df.drop(['population', 'households', 'proximity_to_store', 'ISLAND'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "# ------------------------------------- FIX MISSING DATA -------------------------------------\n",
        "tb_med = df['total_bedrooms'].median(axis=0)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].fillna(value = tb_med)\n",
        "df['total_bedrooms'] = df['total_bedrooms'].astype(int)\n",
        "df.name = 'df'\n",
        "\n",
        "# ------------------------------------- Z-SCORE -------------------------------------\n",
        "z = np.abs(stats.zscore(df))\n",
        "dfz = df[(z < 3).all(axis = 1)]\n",
        "dfz.name = 'dfz'\n",
        "\n",
        "# ------------------------------------- INTERQUARTILE RANGE -------------------------------------\n",
        "q1 = df.quantile(0.25)\n",
        "q3 = df.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower = q1 - (1.5 * iqr)\n",
        "upper = q3 + (1.5 * iqr)\n",
        "dfi = df[~((df < lower) | (df > upper)).any(axis = 1)]\n",
        "dfi.name = 'dfi'\n",
        "#dfi = dfi.drop(['NEAR BAY', 'NEAR OCEAN'], axis = 1)  # After applying IQR, the following features are now empty and can be dropped\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FUNCTIONS <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "def make_heatmap(df = 'df'):\n",
        "  corr = df.corr()\n",
        "  plt.figure(figsize=(15 ,9))\n",
        "  sns.heatmap(corr, annot=True, vmin = -1, vmax = 1, center = 0, fmt = '.1g', cmap = 'coolwarm')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def make_heading(heading = 'heading'):\n",
        "  print('\\n\\n{0}:\\n'.format(heading), '-' * 30)\n",
        "\n",
        "\n",
        "def drop_features(df_in):\n",
        "  df_in = df_in.drop([#'median_house_value',\n",
        "                      'longitude',\n",
        "                      'latitude',\n",
        "                      #'housing_median_age',\n",
        "                      'total_rooms',\n",
        "                      'total_bedrooms',\n",
        "                      #'median_income',\n",
        "                      'INLAND',\n",
        "                      #'NEAR BAY',\n",
        "                      #'NEAR OCEAN'\n",
        "                      ],\n",
        "                      axis = 1\n",
        "                      )\n",
        "  return features_dfx\n",
        "\n",
        "\n",
        "def plot_test_predictions(y_test, y_pred):\n",
        "  make_heading('Prediction Performance')  # Make a heading to separate output\n",
        "  fig, ax = plt.subplots()\n",
        "  ax = plt.subplot()\n",
        "  ax.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha=0.5)\n",
        "  ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--', lw=4)\n",
        "  ax.set_xlabel('Actual')\n",
        "  ax.set_ylabel('Predicted')\n",
        "  ax.set_title(\"Ground Truth vs Predicted\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def plot_feature_importance(feature_importance):\n",
        "  # Make importances relative to max importance\n",
        "  feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
        "  sorted_idx = np.argsort(feature_importance)\n",
        "  pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "  make_heading('Feature Importance')  # Make a heading to separate output\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "  plt.yticks(pos, features_dfx)\n",
        "  plt.xlabel('Relative Importance')\n",
        "  plt.title('Variable Importance')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def plot_deviance():\n",
        "  # Plot the training/testing accuracy\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_staged_predicted_score = model.staged_predict(X_test)\n",
        "  model_test_score = model.score(X_test, y_test)  # Accuracy of our model on test data\n",
        "  model_train_score = model.score(X_train, y_train)  # Accuracy of our model on training data\n",
        "  \n",
        "\n",
        "  # Create an empty array of zeros based on the value in 'n_estimators' key\n",
        "  test_score = np.zeros(shape = (params['n_estimators'],),\n",
        "                        dtype=np.float64\n",
        "                        )\n",
        "\n",
        "  # Compute test set deviance (loss) at each stage and place it into the array\n",
        "  #  based on the predicted value (y_pred) against the actual value (y_test)\n",
        "  for i, y_pred in enumerate(y_staged_predicted_score):\n",
        "      test_score[i] = model.loss_(y_test, y_pred)\n",
        "  \n",
        "  make_heading('Deviance Over Time')  # Make a heading to separate output\n",
        "  plt.figure(figsize=(15, 7))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('Deviance')\n",
        "  plt.plot(np.arange(params['n_estimators']) + 1, \n",
        "           model.train_score_,\n",
        "           'b-',\n",
        "           label='Training Set Deviance'\n",
        "           )\n",
        "  plt.plot(np.arange(params['n_estimators']) + 1,\n",
        "           test_score,\n",
        "           'r-',\n",
        "           label='Test Set Deviance'\n",
        "           )\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('Boosting Iterations')\n",
        "  plt.ylabel('Deviance')\n",
        "  plt.show()\n",
        "  plt.close\n",
        "  \n",
        "\n",
        "def show_metrics(X_train, X_test, y_train, y_test, y_pred):\n",
        "  # Display the shape of each array:\n",
        "  #make_heading('The shape of each of our arrays after splitting into training and testing sets for dataframe \\\"{0}\\\"'.format(dfx.name))\n",
        "  #print('X_train shape: ', X_train.shape)\n",
        "  #print('X_test shape:  ', X_test.shape)\n",
        "  #print('y_train shape: ', y_train.shape)\n",
        "  #print('y_test shape:  ', y_test.shape)\n",
        "  #print('y_pred shape:  ', y_pred.shape)\n",
        "\n",
        "  # Display training / testing set metrics\n",
        "  make_heading('\\n\\nAccuracy and Error for training/testing on the {0} model for dataframe \\\"{1}\\\"'.format(model_name, dfx.name))\n",
        "  print(\"Training Accuracy (score)                 (X_train, y_train):  {:.2f}\".format(model_train_score))\n",
        "  print(\"Test Accuracy (score)                     (X_test, y_test):    {:.2f}\".format(model_test_score))\n",
        "  print(\"Predictive Accuracy (R^2 score)           (y_test, y_pred):    {:.2f}\".format(predictive_accuracy))\n",
        "  print(\"Explained Variance (1 is best) (loss)     (y_test, y_pred):    {:.2f}\".format(ev))\n",
        "  print(\"Mean Absolute Error on Test Set (loss)    (y_test, y_pred):    {:.2f}\".format(mean_ae))\n",
        "  print(\"Median Absolute Error on Test Set (loss)  (y_test, y_pred):    {:.2f}\".format(median_ae))\n",
        "  print(\"RMSE on Test Set (loss)                   (y_test, y_pred):    {:.2f}\".format(rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NoTOrvD-rq",
        "colab_type": "text"
      },
      "source": [
        "## B. Choose a dataset to use\n",
        "- Three datasets have been created:\n",
        "  1. **df** -- the **original dataset** ('NaN' values have been replaced with median, so there are no null values)\n",
        "  2. **dfz** -- a dataset that has used the **z-score method** for removing outlier data\n",
        "  3. **dfi** -- a dataset that has used the **IQR method** for removing outlier data\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "**NOTE:** When choosing your dataset, you will need to change two commands to reflect the dataset that you want to use. In the following example, the \n",
        "\n",
        "**Original code:** --------------------------------------->  **New code** (For example, to change **from** 'dfz' **to** 'dfi' dataset):\n",
        "```\n",
        "dfx = dfz.copy()      -->   dfx = dfi.copy()\n",
        "dfx.name = dfz.name   -->   dfx.name = dfi.name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  \n",
        "  \n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxObYDCISc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *************************** DETERMINE DATA FOR USE ***************************\n",
        "# Choose which dataframe you would like to use (dfi: IQR, dfz: z-score, df: original (with replaced NaN values))\n",
        "dfx = df.copy()\n",
        "dfx.name = df.name\n",
        "\n",
        "# Show a heatmap for the given dataframe you want to use\n",
        "make_heatmap(dfx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9fD53_uT_4",
        "colab_type": "text"
      },
      "source": [
        "## C. Step by Step: Building a Gradient Boosting Model\n",
        "\n",
        "### From *Machine Learning For Absolute Beginners* by Oliver Theobald\n",
        "```\n",
        "\n",
        "model = ensemble.GradientBoostingRegressor(\n",
        "                                           n_estimators = 150, \n",
        "                                           learning_rate = 0.1, \n",
        "                                           max_depth = 30, \n",
        "                                           min_samples_split = 4, \n",
        "                                           min_samples_leaf = 6, \n",
        "                                           max_features = 0.6, \n",
        "                                           loss = 'huber'\n",
        "                                          )\n",
        "```\n",
        "\n",
        "The first line is the algorithm itself (gradient boosting) and comprises just one line of code. The code below dictates the hyperparameters for this algorithm. \n",
        "- **n_estimators** represents how many decision trees to be used. Remember that a high number of trees generally improves accuracy (up to a certain point) but will extend the model’s processing time. Above, I have selected 150 decision trees as an initial starting point. \n",
        "- **learning_rate** controls the rate at which additional decision trees influence the overall prediction. This effectively shrinks the contribution of each tree by the set learning_rate. Inserting a low rate here, such as 0.1, should help to improve accuracy. \n",
        "- **max_depth** defines the maximum number of layers (depth) for each decision tree. If “None” is selected, then nodes expand until all leaves are pure or until all leaves contain less than min_samples_leaf. Here, I have chosen a high maximum number of layers (30), which will have a dramatic effect on the final result, as we’ll soon see. \n",
        "- [**min_samples_split**](https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre) defines the minimum number of samples required to execute a new binary split. For example, min_samples_split = 10 means there must be ten available samples in order to create a new branch.\n",
        "- [**min_samples_leaf**](**min_samples_split**) represents the minimum number of samples that must appear in each child node (leaf) before a new branch can be implemented. This helps to mitigate the impact of outliers and anomalies in the form of a low number of samples found in one leaf as a result of a binary split. For example, min_samples_leaf = 4 requires there to be at least four available samples within each leaf for a new branch to be created. \n",
        "- **max_features** is the total number of features presented to the model when determining the best split.\n",
        "- **loss** calculates the model's error rate. For this exercise, we are using huber which protects against outliers and anomalies. Alternative error rate options include ls (least squares regression), lad (least absolute deviations), and quantile (quantile regression). Huber is actually a combination of least squares regression and least absolute deviations.\n",
        "\n",
        "\n",
        "  >Theobald, Oliver. Machine Learning For Absolute Beginners: A Plain English Introduction (Second Edition) (Machine Learning For Beginners Book 1) (pp. 139-141). Scatterplot Press. Kindle Edition.\n",
        "\n",
        "I also referenced the following website(s) and adapted code in order to graph this model:\n",
        "- [Scikit-Learn Documentation: Ensemble Gradient Boosting Visualization](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py)\n",
        "- [Sharp Sight Labs: Numpy Zeros Tutorial](https://www.sharpsightlabs.com/blog/numpy-zeros-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYl_TUtTRhP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> STEP BY STEP DECISION TREE CONSTRUCTION <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# ---------------------------- Identify Target Variable ----------------------------\n",
        "# Create a copy of the dataframe with only the desired features (dropping the target feature)\n",
        "features_dfx = dfx.copy()\n",
        "features_dfx = dfx.drop(['median_house_value'], axis = 1)  # We *MUST* drop this b/c it is the target\n",
        "\n",
        "# Determine additional features to keep/drop (# means that we want to keep that variable)\n",
        "features_dfx = features_dfx.drop([#'longitude',\n",
        "                                  #'latitude',\n",
        "                                  #'housing_median_age',\n",
        "                                  #'total_rooms',\n",
        "                                  #'total_bedrooms',\n",
        "                                  #'median_income',\n",
        "                                  #'INLAND',\n",
        "                                  'NEAR BAY',\n",
        "                                  'NEAR OCEAN'\n",
        "                                  ],\n",
        "                                 axis = 1\n",
        "                                 )\n",
        "\n",
        "\n",
        "# ---------------------------- SPLIT THE DATASET ----------------------------\n",
        "X = features_dfx.values                 # Define the independent variable values to be used\n",
        "y = dfx['median_house_value'].values    # Define the dependent variable values to be used\n",
        "rs = 20                                 # Define the random state variable (ensuring continuity between runs)\n",
        "test_size = 0.3                         # Define the percentage of data to set aside for testing (usually b/w 0.2 - 0.3)\n",
        "\n",
        "# Split the dataset into training and testing arrays\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = test_size,\n",
        "                                                    shuffle = True,\n",
        "                                                    random_state = rs\n",
        "                                                    )\n",
        "\n",
        "\n",
        "# ---------------------------- CREATE THE MODEL  ----------------------------\n",
        "# -- Set the hyperparameters that we want to use in the model\n",
        "params = {'n_estimators': 1000,\n",
        "          'learning_rate': .01,\n",
        "          'max_depth': 5,\n",
        "          'min_samples_split': 10,\n",
        "          'min_samples_leaf': 5,\n",
        "          'max_features': None,       # Can be float, int, string or None; max_features < n_features = reduced variance and increased bias\n",
        "          'subsample': 1.0,\n",
        "          'random_state': rs,\n",
        "          'verbose': 1,\n",
        "          # Options for 'loss': huber, ls (least squares), lad (least absolute deviations) and quantile (quantile regression)\n",
        "          'loss': 'huber'\n",
        "          }\n",
        "\n",
        "# Define the desired algorithm and congigure the hyperparameters (supplied with '**params' arguement)\n",
        "model = ensemble.GradientBoostingRegressor(**params)\n",
        "model_name = type(model).__name__   # Get the name of the model (for use in our display functions)\n",
        "\n",
        "\n",
        "# ---------------------------- Fit / TRAIN THE MODEL ----------------------------\n",
        "# Train the model using our training sets (X_train, y_train)\n",
        "model.fit(X_train, y_train) \n",
        "\n",
        "# Here we want to input our X_test array to make predictions based upon our trained model\n",
        "y_pred = model.predict(X_test)\n",
        "y_staged_predicted_score = model.staged_predict(X_test)\n",
        "\n",
        "\n",
        "# ---------------------------- GATHER METRICS ----------------------------\n",
        "model_test_score = model.score(X_test, y_test)              # Accuracy of our model on test data\n",
        "model_train_score = model.score(X_train, y_train)           # Accuracy of our model on training data\n",
        "mean_ae = metrics.mean_absolute_error(y_test, y_pred)       # Mean absolute error (find mae on the test set (to see how well the trained model performs on new data(y_test against y_pred))\n",
        "median_ae = metrics.median_absolute_error(y_test, y_pred)   # Median absolute error\n",
        "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))  # Root mean squared error\n",
        "ev = metrics.explained_variance_score(y_test, y_pred)       # Explained Variance Score (y_test, y_pred): Measures the proportion to which a mathematical model accounts for the variation (dispersion) of a given data set\n",
        "predictive_accuracy = metrics.r2_score(y_test, y_pred)      # Determine the predictive accuracy of the model by getting the R^2 score (how well future samples are likely to be predicted by the model)\n",
        "\n",
        "# ---------------------------- ANALYSIS / VISUALIZATION ----------------------------\n",
        "show_metrics(X_train, X_test, y_train, y_test, y_pred)  # Dispay the scores and loss for the model\n",
        "plot_test_predictions(y_test, y_pred)                   # Create a scatterplot of real values against predicted ones for the test set\n",
        "feature_importance = model.feature_importances_         # Gather feature importance values\n",
        "plot_feature_importance(feature_importance)             # Plot feature importance\n",
        "plot_deviance()                                         # Plot training / testing accuracy\n",
        "joblib.dump(model, 'ca_housing_gb_model.pkl')           # Save the model so that we can use it later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3EcUkNUxkE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
